{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-negative matrix factorization (NMF) based topic modeling\n",
    "This notebook presents the NMF approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "\n",
    "# step 1: import\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils import *\n",
    "from estimators import *\n",
    "\n",
    "### Gensim\n",
    "#import gensim\n",
    "#import gensim.corpora as corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "### load NMF utility functions\n",
    "from nmf_util import *\n",
    "### load coherence score\n",
    "#import gensim.downloader as api\n",
    "from coherence_score import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.626891130072679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([[1,2,3,-2],[1,-11,3,4]])\n",
    "b=(a>0)*a\n",
    "c=np.linalg.norm(b,2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stella/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "/Users/stella/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "glove_file = './glove.6B/glove.6B.100d.txt'\n",
    "tmp_file = \"./test_word2vec.txt\"\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "# 加载转化后的文件\n",
    "model_glove = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### json load the dataset\n",
    "with open('../cleaned_data/Spam_Ham.json', 'r') as jf:\n",
    "    cleaned_data = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split data into 'sentence' and 'label'\n",
    "sentences = [it['sentence'] for it in cleaned_data]\n",
    "labels = [it['label'] for it in cleaned_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ham', 'spam'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model_glove = api.load(\"./glove.6B/glove.6B.100d.txt\")   ## load pretrained glove embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Count Vectors as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the corpora to Count vectors\n",
    "count = CountVectorizer(max_df=.95, min_df=10, max_features=5000)\n",
    "x_count = count.fit_transform(sentences)\n",
    "## convert to matrix --- feature-document matrix\n",
    "count_mat = x_count.toarray().T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869, 5572)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## features\n",
    "features = count.get_feature_names()\n",
    "len(list(features)),len(list(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. gassian_method L2 loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NMF methods for topic modeling\n",
    "k = 100   ## the number of topics -- tune it for better result\n",
    "W0,H0,err0=gaussian_method(count_mat, k, max_iter=4)  ## will return factor matrices: W, H and root mean squared error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22905.426368242854"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(count_mat - W0@H0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic0 = top_keywords(W0, features, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute the coherence score for each topic\n",
    "coherence_vec = []\n",
    "for i in range(W0.shape[1]):  \n",
    "    coherence_vec.append(coherence(dic0[i], model_glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35705584"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(coherence_vec)   ## the mean coherence score of all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5572)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.981     0.978     0.980       966\n",
      "           1      0.862     0.879     0.870       149\n",
      "\n",
      "    accuracy                          0.965      1115\n",
      "   macro avg      0.922     0.929     0.925      1115\n",
      "weighted avg      0.965     0.965     0.965      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "indices = list(range(len(labels)))   ## indices of documents\n",
    "\n",
    "## split data into train and test\n",
    "ind_train, ind_test, y_train, y_test = train_test_split(\n",
    "    indices, labels, test_size=0.2, random_state=2021, stratify=labels)\n",
    "\n",
    "## train/test datasets\n",
    "\n",
    "#H0 = H0.detach().numpy()\n",
    "print(H0.shape)\n",
    "x_train, x_test = H0[:, ind_train],H0[:, ind_test]\n",
    "\n",
    "## encode labels to integers\n",
    "Encoder = LabelEncoder()\n",
    "Y_train = Encoder.fit_transform(y_train)\n",
    "Y_test = Encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "# Classifier - Algorithm - SVM -- linear kernel\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1., kernel='linear', degree=3, gamma='auto', random_state=82, class_weight='balanced')\n",
    "SVM.fit(x_train.T, Y_train)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(x_test.T) # make predictions\n",
    "print(classification_report(Y_test, predictions_SVM, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. SGD without MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([869, 5572])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.FloatTensor(count_mat)\n",
    "#A. type\n",
    "print(A.shape)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeparableCritic(nn.Module):\n",
    "    \"\"\"Separable critic. where the output value is g(x) h(y). \"\"\"\n",
    "\n",
    "    def __init__(self, dim1, dim2, hidden_dim, embed_dim, layers, activation, **extra_kwargs):\n",
    "        super(SeparableCritic, self).__init__()\n",
    "        self._g = mlp(dim1, hidden_dim, embed_dim, layers, activation)\n",
    "        self._h = mlp(dim2, hidden_dim, embed_dim, layers, activation)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        scores = torch.matmul(self._h(y), self._g(x).t())\n",
    "        return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step 2: create model Class\n",
    "class GaussianNMF(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    centralized model training with stochastic gradient descent\n",
    "    class for Non-Negetive Matrix Multiplication using Gaussian Method\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, A, k):\n",
    "        \"\"\" initialization \"\"\"\n",
    "        super(GaussianNMF, self).__init__()\n",
    "        self.rows = A.size(0)\n",
    "        self.cols = A.size(1)\n",
    "        self.A = A\n",
    "        \n",
    "        self.W = torch.nn.Parameter(torch.rand(self.rows, k), requires_grad=True) \n",
    "        self.H = torch.nn.Parameter(torch.rand(k, self.cols), requires_grad=True)\n",
    "        #print(self.H)\n",
    "        self.num_topics = k\n",
    "        \n",
    "  \n",
    "    def forward(self):\n",
    "        return self.W.matmul(self.H) \n",
    "  \n",
    "    def batch_gd_train(self, epochs, batch_size, lr):\n",
    "        \"\"\"\n",
    "        train with full batch gradient descent, i.e., all data in a batch for each iteration\n",
    "        :params[in]: epochs,\n",
    "        \n",
    "        :params[out]: W, H\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)\n",
    "        for i in range(epochs):\n",
    "            pred = self.forward()\n",
    "            loss = (self.A - pred).pow(2).sum()\n",
    "            ## backward\n",
    "            optimizer.zero_grad()   ## zero all gradients\n",
    "            loss.backward()   ## find derivatives\n",
    "            optimizer.step()  \n",
    "            self.W.data[self.W.data < 0] = 0. \n",
    "            self.H.data[self.H.data < 0] = 0. \n",
    "            print('loss at Epoch ',i, ' ',loss.item())\n",
    "        ## return\n",
    "        return self.W, self.H\n",
    "        \n",
    "    ## split an iterable of items into batches \n",
    "    def chunks(self, ls, batch_size): \n",
    "        \"\"\" \n",
    "        Yield successive n-sized chunks from ls, an iterable. \n",
    "        :params[in]: ls, an iterable of items \n",
    "        :params[in]: batch_size, an integer, batch size \n",
    "        returns a generator \n",
    "        \"\"\" \n",
    "        for i in range(0, len(ls), batch_size): \n",
    "            yield ls[i:i + batch_size]\n",
    "        \n",
    "    '''def estimate_mutual_information(estimator, x, y, critic_fn,\n",
    "                                baseline_fn=None, alpha_logit=None, **kwargs):\n",
    "        \n",
    "        if estimator == 'smile':\n",
    "            mi = smile_lower_bound(scores, **kwargs)\n",
    "    '''\n",
    "\n",
    "    def sgd_train(self, epochs, batch_size, lr):\n",
    "        \"\"\"\n",
    "        train with stochastic gradient descent\n",
    "        :params[in]: epochs,\n",
    "        \n",
    "        :params[out]: W, H\n",
    "        \n",
    "        ** x = mini_data.T\n",
    "        y = mini_datah.T\n",
    "        \"\"\"\n",
    "        \n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr)                           \n",
    "        scheduler= StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "        data_index = list(range(self.cols))   ## all column indices\n",
    "        for i in range(epochs):\n",
    "            mini_batches = self.chunks(data_index, batch_size)\n",
    "            for it in mini_batches:\n",
    "                mini_data = A[:, it]\n",
    "                \n",
    "                mini_datah= self.H[:, it]\n",
    "               \n",
    "                ## data in a minibatch\n",
    "                pred = self.forward()[:, it]  ## prediction\n",
    "                loss = (mini_data - pred).pow(2).sum()\n",
    "                ## backward\n",
    "                optimizer.zero_grad()   ## zero all gradients\n",
    "                loss.backward()   ## find derivatives\n",
    "                optimizer.step()  \n",
    "                self.W.data[self.W.data < 0] = 0. \n",
    "                self.H.data[self.H.data < 0] = 0.\n",
    "               \n",
    "            ## shuffle indices\n",
    "            scheduler.step()\n",
    "            np.random.shuffle(data_index) \n",
    "            \n",
    "            ## current loss\n",
    "            cur_loss = (self.A - self.W@self.H).pow(2).sum()\n",
    "            print('loss at Epoch ',i, ' ',cur_loss.item())\n",
    "        ## return             \n",
    "        return self.W, self.H\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  0   30499714.0\n",
      "loss at Epoch  1   391396.78125\n",
      "loss at Epoch  2   53144.7578125\n",
      "loss at Epoch  3   41797.0703125\n",
      "loss at Epoch  4   40642.203125\n",
      "loss at Epoch  5   40160.7734375\n",
      "loss at Epoch  6   39861.109375\n",
      "loss at Epoch  7   39676.34375\n",
      "loss at Epoch  8   39336.4140625\n",
      "loss at Epoch  9   39010.14453125\n",
      "loss at Epoch  10   39182.234375\n",
      "loss at Epoch  11   37402.390625\n",
      "loss at Epoch  12   36640.6953125\n",
      "loss at Epoch  13   35908.578125\n",
      "loss at Epoch  14   35099.38671875\n",
      "loss at Epoch  15   34248.765625\n",
      "loss at Epoch  16   33346.484375\n",
      "loss at Epoch  17   32452.044921875\n",
      "loss at Epoch  18   31507.541015625\n",
      "loss at Epoch  19   30594.625\n",
      "loss at Epoch  20   29881.5546875\n",
      "loss at Epoch  21   29211.921875\n",
      "loss at Epoch  22   28565.31640625\n",
      "loss at Epoch  23   27974.58984375\n",
      "loss at Epoch  24   27439.55859375\n",
      "loss at Epoch  25   26937.6484375\n",
      "loss at Epoch  26   26472.546875\n",
      "loss at Epoch  27   26042.58984375\n",
      "loss at Epoch  28   25652.1484375\n",
      "loss at Epoch  29   25293.74609375\n",
      "loss at Epoch  30   25011.765625\n",
      "loss at Epoch  31   24748.048828125\n",
      "loss at Epoch  32   24500.1328125\n",
      "loss at Epoch  33   24259.5546875\n",
      "loss at Epoch  34   24030.3203125\n",
      "loss at Epoch  35   23813.9765625\n",
      "loss at Epoch  36   23603.3515625\n",
      "loss at Epoch  37   23405.5234375\n",
      "loss at Epoch  38   23216.548828125\n",
      "loss at Epoch  39   23033.5703125\n"
     ]
    }
   ],
   "source": [
    "nmf_method1 = GaussianNMF(A, 100) ## matrix factorization\n",
    "W1, H1 = nmf_method1.sgd_train(epochs=40, batch_size=1024, lr=2.e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0024, 0.0000],\n",
       "         [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0321, 0.0160],\n",
       "         [0.0000, 0.0000, 0.0065,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0143, 0.0034, 0.0000,  ..., 0.0000, 0.0000, 0.0263],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[0.0000e+00, 0.0000e+00, 2.7522e-01,  ..., 0.0000e+00, 2.6952e-01,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 4.1900e-04,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 2.9258e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 0.0000e+00, 7.5245e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [1.5017e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          5.2186e-05],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          1.7770e-04]], requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nmf_method1.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23033.5703, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A-W1@H1).pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.987     0.962     0.974       966\n",
      "           1      0.787     0.919     0.848       149\n",
      "\n",
      "    accuracy                          0.956      1115\n",
      "   macro avg      0.887     0.941     0.911      1115\n",
      "weighted avg      0.961     0.956     0.957      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "\n",
    "indices = list(range(len(labels)))   ## indices of documents\n",
    "\n",
    "## split data into train and test\n",
    "ind_train, ind_test, y_train, y_test = train_test_split(\n",
    "    indices, labels, test_size=0.2, random_state=2021, stratify=labels)\n",
    "## train/test datasets\n",
    "H1 = H1.detach().numpy()\n",
    "x_train, x_test = H1[:, ind_train],H1[:, ind_test]\n",
    "## encode labels to integers\n",
    "Encoder = LabelEncoder()\n",
    "Y_train = Encoder.fit_transform(y_train)\n",
    "Y_test = Encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "# Classifier - Algorithm - SVM -- linear kernel\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1., kernel='linear', degree=3, gamma='auto', random_state=82, class_weight='balanced')\n",
    "SVM.fit(x_train.T, Y_train)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(x_test.T) # make predictions\n",
    "print(classification_report(Y_test, predictions_SVM, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1 = top_keywords(W1, features, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34759513"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compute the coherence score for each topic\n",
    "coherence_vec = []\n",
    "for i in range(W1.shape[1]):  \n",
    "    coherence_vec.append(coherence(dic1[i], model_glove))\n",
    "    \n",
    "np.mean(coherence_vec)   ## the mean coherence score of all topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SGD with MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([869, 5572])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.FloatTensor(count_mat)\n",
    "#A. type\n",
    "print(A.shape)\n",
    "#A=A.type(torch.FloatTensor)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNMF_MI(GaussianNMF):\n",
    "    \"\"\"\n",
    "    Centralized training NMF model\n",
    "    class for Non-Negetive Matrix Factorization using Gaussian Method \n",
    "    with mutual information regularizer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, A, k, critic_config):\n",
    "        \"\"\" initialization\n",
    "        \n",
    "        :params[in]: A, k\n",
    "        :params[in]: critic_config, a dictionary, \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(GaussianNMF_MI, self).__init__(A, k)  ## instantiate super class\n",
    "        self.critic_config = critic_config\n",
    "        ### critic function for computing mutual information\n",
    "        self.critic_config['dim1'] = self.rows\n",
    "        self.critic_config['dim2'] = k\n",
    "        self.critic = SeparableCritic(**self.critic_config)\n",
    "  \n",
    "\n",
    "    def sgd_train(self, epochs, batch_size, lr, xi, step_size=10, gamma=0.9, W_init = None):\n",
    "        \"\"\"\n",
    "        train with stochastic gradient descent\n",
    "        :params[in]: epochs,\n",
    "        \n",
    "        :params[out]: W, H\n",
    "        \n",
    "        ** \n",
    "        x = mini_data.T\n",
    "        y = mini_datah.T\n",
    "        \"\"\"\n",
    "        if W_init is not None:\n",
    "            self.W.data = W_init.data\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        data_index = list(range(self.cols))   ## all column indices\n",
    "        for i in range(epochs):\n",
    "            mini_batches = self.chunks(data_index, batch_size)\n",
    "            for it in mini_batches:\n",
    "                mini_data = A[:, it]\n",
    "                mini_datah= self.H[:, it]\n",
    "\n",
    "                # calculate mi \n",
    "                mi = estimate_mutual_information('smile', mini_data.T, mini_datah.T, \n",
    "                                                 self.critic)\n",
    "                ## data in a minibatch\n",
    "                pred = self.forward()[:, it]  ## prediction\n",
    "                #guassian\n",
    "                loss = (mini_data - pred).pow(2).sum() - xi * mi\n",
    "                #possian\n",
    "                #loss = (pred-mini_data*torch.log(pred)).sum()-xi*mi\n",
    "              \n",
    "                ## backward\n",
    "                #lr = scheduler.get_lr()\n",
    "                optimizer.zero_grad()   ## zero all gradients\n",
    "                loss.backward()## find derivatives\n",
    "                #load_state_dict(state_dict)\n",
    "                optimizer.step()  \n",
    "                self.W.data[self.W.data < 0] = 0. \n",
    "                self.H.data[self.H.data < 0] = 0. \n",
    "            print()\n",
    "            ## renew learning rate\n",
    "            #print('Epoch前:', i,'LR:', lr)\n",
    "            scheduler.step()\n",
    "            #print('Epoch:', i,'LR:', lr)\n",
    "            ## shuffle indices\n",
    "            np.random.shuffle(data_index) \n",
    "            ## current loss\n",
    "            cur_loss = (self.A - self.W@self.H).pow(2).sum()\n",
    "            print('loss at Epoch ',i, ' ',cur_loss.item())\n",
    "            #print(lr)\n",
    "        ## return\n",
    "        return self.W, self.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([869, 5572])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.FloatTensor(count_mat)\n",
    "#A. type\n",
    "print(A.shape)\n",
    "#A=A.type(torch.FloatTensor)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss at Epoch  0   99597952.0\n",
      "\n",
      "loss at Epoch  1   1095882.75\n",
      "\n",
      "loss at Epoch  2   108404.4375\n",
      "\n",
      "loss at Epoch  3   39850.5625\n",
      "\n",
      "loss at Epoch  4   39259.51953125\n",
      "\n",
      "loss at Epoch  5   37959.390625\n",
      "\n",
      "loss at Epoch  6   36357.328125\n",
      "\n",
      "loss at Epoch  7   35605.75\n",
      "\n",
      "loss at Epoch  8   34466.6640625\n",
      "\n",
      "loss at Epoch  9   33219.51953125\n",
      "\n",
      "loss at Epoch  10   31581.86328125\n",
      "\n",
      "loss at Epoch  11   30005.857421875\n",
      "\n",
      "loss at Epoch  12   28654.986328125\n",
      "\n",
      "loss at Epoch  13   27444.9765625\n",
      "\n",
      "loss at Epoch  14   26631.48828125\n",
      "\n",
      "loss at Epoch  15   25632.765625\n",
      "\n",
      "loss at Epoch  16   24948.7109375\n",
      "\n",
      "loss at Epoch  17   24186.6328125\n",
      "\n",
      "loss at Epoch  18   23456.22265625\n",
      "\n",
      "loss at Epoch  19   22825.390625\n",
      "\n",
      "loss at Epoch  20   22377.529296875\n",
      "\n",
      "loss at Epoch  21   21971.837890625\n",
      "\n",
      "loss at Epoch  22   21767.09765625\n",
      "\n",
      "loss at Epoch  23   21420.6328125\n",
      "\n",
      "loss at Epoch  24   21167.439453125\n",
      "\n",
      "loss at Epoch  25   20991.4296875\n",
      "\n",
      "loss at Epoch  26   20794.26171875\n",
      "\n",
      "loss at Epoch  27   20631.2109375\n",
      "\n",
      "loss at Epoch  28   20530.578125\n",
      "\n",
      "loss at Epoch  29   20428.5390625\n",
      "\n",
      "loss at Epoch  30   20372.9375\n",
      "\n",
      "loss at Epoch  31   20191.681640625\n",
      "\n",
      "loss at Epoch  32   20135.69140625\n",
      "\n",
      "loss at Epoch  33   20019.8359375\n",
      "\n",
      "loss at Epoch  34   19988.052734375\n",
      "\n",
      "loss at Epoch  35   19903.9921875\n",
      "\n",
      "loss at Epoch  36   19926.76171875\n",
      "\n",
      "loss at Epoch  37   19852.00390625\n",
      "\n",
      "loss at Epoch  38   19834.994140625\n",
      "\n",
      "loss at Epoch  39   19787.5234375\n",
      "\n",
      "loss at Epoch  40   19759.203125\n",
      "\n",
      "loss at Epoch  41   19692.83984375\n",
      "\n",
      "loss at Epoch  42   19741.8828125\n",
      "\n",
      "loss at Epoch  43   19698.59375\n",
      "\n",
      "loss at Epoch  44   19641.087890625\n",
      "\n",
      "loss at Epoch  45   19586.814453125\n",
      "\n",
      "loss at Epoch  46   19510.66015625\n",
      "\n",
      "loss at Epoch  47   19521.109375\n",
      "\n",
      "loss at Epoch  48   19476.953125\n",
      "\n",
      "loss at Epoch  49   19466.76171875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#NMF-MI method \n",
    "k=100\n",
    "\n",
    "#A = torch.randn(count_mat.shape[0],count_mat.shape[1])\n",
    "critic_params = {\n",
    "    'layers': 0,\n",
    "    'embed_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "##init nmf especially the W, H\n",
    "nmf1 = GaussianNMF_MI(A, k, critic_params)\n",
    "##put W in every client\n",
    "\n",
    "W, H = nmf1.sgd_train(epochs=50, batch_size=1024, lr=4.e-2, xi = 1.e-1, gamma=0.95)#, W_init = torch.rand([2153, 20])) \n",
    "#def server_train(self, epoch, client_num, batch_sieze, lr, xi)\n",
    "#W, H = nmf1.server_train(epoch=4, client_num = 3, batch_size=128, lr=2.e-4, xi=0 )\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf9e0f2750>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf1.critic.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0000, 0.0084, 0.0208,  ..., 0.0000, 0.0000, 0.0078],\n",
       "        [0.0564, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.1506],\n",
       "        [0.0000, 0.0256, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0191, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0308, 0.0640, 0.0000,  ..., 0.0000, 0.0011, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0491, 0.0000, 0.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf1.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('W',\n",
       "  Parameter containing:\n",
       "  tensor([[0.0000, 0.0084, 0.0208,  ..., 0.0000, 0.0000, 0.0078],\n",
       "          [0.0564, 0.0000, 0.0134,  ..., 0.0000, 0.0000, 0.1506],\n",
       "          [0.0000, 0.0256, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0191, 0.0000, 0.0067,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0308, 0.0640, 0.0000,  ..., 0.0000, 0.0011, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0491, 0.0000, 0.0000]],\n",
       "         requires_grad=True)),\n",
       " ('H',\n",
       "  Parameter containing:\n",
       "  tensor([[0.0000e+00, 1.8006e-04, 4.9687e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 3.9967e-03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.7767e-10, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          ...,\n",
       "          [0.0000e+00, 5.0677e-03, 0.0000e+00,  ..., 6.8839e-10, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 1.5829e-01,  ..., 5.6974e-10, 0.0000e+00,\n",
       "           0.0000e+00],\n",
       "          [0.0000e+00, 0.0000e+00, 1.4520e-02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "           0.0000e+00]], requires_grad=True)),\n",
       " ('critic._g.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0274, -0.0221,  0.0112,  ...,  0.0053,  0.0336,  0.0157],\n",
       "          [ 0.0312,  0.0091,  0.0096,  ..., -0.0105,  0.0264,  0.0074],\n",
       "          [-0.0220,  0.0042, -0.0291,  ..., -0.0173, -0.0040, -0.0220],\n",
       "          ...,\n",
       "          [ 0.0293, -0.0212,  0.0313,  ..., -0.0203, -0.0176,  0.0118],\n",
       "          [-0.0272,  0.0257,  0.0066,  ...,  0.0233, -0.0236,  0.0332],\n",
       "          [-0.0160,  0.0132, -0.0031,  ...,  0.0194, -0.0174,  0.0152]],\n",
       "         requires_grad=True)),\n",
       " ('critic._g.0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0143, -0.0025, -0.0048, -0.0185,  0.0280, -0.0244, -0.0329,  0.0219,\n",
       "          -0.0169,  0.0329,  0.0183, -0.0254,  0.0262, -0.0181,  0.0013, -0.0276,\n",
       "           0.0189,  0.0017, -0.0182,  0.0161,  0.0266, -0.0232, -0.0223,  0.0043,\n",
       "          -0.0187,  0.0229,  0.0073,  0.0327, -0.0024,  0.0225,  0.0218,  0.0208,\n",
       "           0.0164, -0.0248, -0.0143,  0.0034, -0.0219,  0.0268,  0.0113, -0.0038,\n",
       "           0.0295,  0.0222,  0.0300,  0.0043,  0.0276, -0.0323,  0.0199, -0.0275,\n",
       "          -0.0228,  0.0230,  0.0040, -0.0205,  0.0228,  0.0177,  0.0176, -0.0092,\n",
       "           0.0249,  0.0280,  0.0233, -0.0110,  0.0030,  0.0337,  0.0159, -0.0167,\n",
       "          -0.0072,  0.0268, -0.0084, -0.0207,  0.0093, -0.0164, -0.0220, -0.0296,\n",
       "           0.0272, -0.0028, -0.0153, -0.0275,  0.0078, -0.0157, -0.0087,  0.0072,\n",
       "          -0.0241,  0.0112,  0.0063, -0.0233,  0.0082,  0.0094, -0.0119,  0.0055,\n",
       "          -0.0068,  0.0115, -0.0091,  0.0062, -0.0281, -0.0249,  0.0078,  0.0199,\n",
       "          -0.0088, -0.0055,  0.0209, -0.0200, -0.0254, -0.0012,  0.0127, -0.0273,\n",
       "           0.0046,  0.0303,  0.0226,  0.0053,  0.0297, -0.0060, -0.0248, -0.0009,\n",
       "          -0.0100, -0.0336,  0.0299, -0.0286,  0.0018, -0.0160, -0.0235, -0.0107,\n",
       "          -0.0236,  0.0337, -0.0124, -0.0100,  0.0226, -0.0120,  0.0123,  0.0062,\n",
       "           0.0237, -0.0159,  0.0165, -0.0334, -0.0340, -0.0186,  0.0056,  0.0019,\n",
       "          -0.0262,  0.0128,  0.0276, -0.0195,  0.0095,  0.0315,  0.0079,  0.0152,\n",
       "          -0.0114,  0.0081, -0.0045,  0.0304, -0.0279, -0.0081,  0.0094, -0.0180,\n",
       "           0.0023, -0.0334, -0.0104,  0.0149,  0.0012,  0.0210, -0.0013, -0.0106,\n",
       "          -0.0148,  0.0218,  0.0132, -0.0093,  0.0019,  0.0078, -0.0015,  0.0175,\n",
       "          -0.0060,  0.0257,  0.0145,  0.0117, -0.0190, -0.0119, -0.0098, -0.0273,\n",
       "          -0.0142, -0.0248,  0.0190, -0.0299,  0.0060, -0.0285, -0.0317,  0.0181,\n",
       "           0.0326,  0.0168, -0.0295,  0.0263,  0.0270,  0.0213,  0.0302, -0.0170,\n",
       "           0.0037,  0.0334,  0.0147,  0.0302, -0.0315,  0.0269, -0.0171, -0.0043,\n",
       "           0.0161,  0.0156,  0.0159, -0.0250, -0.0218, -0.0061, -0.0216,  0.0032,\n",
       "          -0.0333,  0.0134,  0.0301,  0.0104, -0.0303,  0.0304,  0.0183, -0.0103,\n",
       "           0.0292, -0.0323, -0.0021, -0.0237,  0.0338,  0.0084,  0.0187,  0.0151,\n",
       "           0.0079, -0.0194, -0.0093, -0.0215, -0.0167, -0.0008, -0.0005,  0.0130,\n",
       "          -0.0005,  0.0175,  0.0087,  0.0024, -0.0289,  0.0005, -0.0214,  0.0264,\n",
       "          -0.0134, -0.0008,  0.0301,  0.0073, -0.0228,  0.0162,  0.0128, -0.0049,\n",
       "          -0.0248,  0.0149,  0.0309, -0.0233,  0.0011,  0.0100, -0.0149,  0.0065],\n",
       "         requires_grad=True)),\n",
       " ('critic._g.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0467, -0.0532,  0.0550,  ...,  0.0244,  0.0432,  0.0440],\n",
       "          [ 0.0551,  0.0039, -0.0147,  ...,  0.0226,  0.0279,  0.0609],\n",
       "          [-0.0167,  0.0478,  0.0316,  ...,  0.0252,  0.0387, -0.0137],\n",
       "          ...,\n",
       "          [-0.0289,  0.0461,  0.0051,  ..., -0.0397,  0.0234,  0.0323],\n",
       "          [ 0.0076,  0.0204,  0.0588,  ...,  0.0099,  0.0088, -0.0095],\n",
       "          [-0.0589,  0.0068,  0.0464,  ...,  0.0507, -0.0424, -0.0215]],\n",
       "         requires_grad=True)),\n",
       " ('critic._g.2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.0492,  0.0437, -0.0547, -0.0349,  0.0104,  0.0451, -0.0243, -0.0288,\n",
       "           0.0146, -0.0261,  0.0466, -0.0199, -0.0256, -0.0585,  0.0523,  0.0148,\n",
       "          -0.0005,  0.0298, -0.0124,  0.0032, -0.0436,  0.0286, -0.0540, -0.0008,\n",
       "          -0.0230,  0.0257, -0.0363,  0.0096, -0.0275, -0.0184, -0.0157,  0.0498],\n",
       "         requires_grad=True)),\n",
       " ('critic._h.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0144,  0.0514, -0.0357,  ...,  0.0716,  0.0411,  0.0774],\n",
       "          [ 0.0335,  0.0279,  0.0096,  ..., -0.0998,  0.0619, -0.0665],\n",
       "          [ 0.0804, -0.0223,  0.0951,  ..., -0.0020,  0.0423,  0.0995],\n",
       "          ...,\n",
       "          [ 0.0752, -0.0315,  0.0419,  ..., -0.0874, -0.0075,  0.0929],\n",
       "          [ 0.0934,  0.0334, -0.0873,  ..., -0.0527,  0.0006,  0.0081],\n",
       "          [-0.0006, -0.0071, -0.0560,  ...,  0.0942, -0.0537,  0.0628]],\n",
       "         requires_grad=True)),\n",
       " ('critic._h.0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-4.6769e-02, -3.7725e-02,  3.0711e-02, -1.4668e-02,  5.0426e-02,\n",
       "          -1.0204e-02, -2.3986e-02, -9.4230e-02, -8.2051e-02, -4.8702e-02,\n",
       "           3.4166e-02,  8.6483e-02,  3.6587e-02,  4.4733e-02,  1.6044e-02,\n",
       "           1.4734e-02,  9.1993e-02,  2.4049e-02,  6.5765e-02,  2.5407e-02,\n",
       "          -7.2045e-03, -3.1340e-02, -3.8970e-02, -6.1930e-02,  4.1250e-02,\n",
       "           6.3687e-02, -8.4673e-02, -8.2199e-02, -5.8761e-02,  1.0706e-02,\n",
       "           4.8957e-02, -9.0426e-02, -5.9933e-02, -2.8553e-02,  2.8647e-02,\n",
       "           3.6767e-02, -5.8580e-02,  1.8726e-02,  6.3151e-02,  1.6327e-03,\n",
       "           8.0826e-02,  4.3031e-02,  9.3673e-03,  6.6528e-02, -9.4733e-02,\n",
       "           8.8109e-02,  3.1941e-02,  8.6249e-04,  4.4898e-02,  1.9741e-02,\n",
       "          -4.9078e-02,  7.0099e-03,  6.4249e-02,  5.8535e-02,  3.2229e-02,\n",
       "           8.1988e-02, -3.6763e-02,  2.9517e-02,  7.0045e-02, -9.8229e-02,\n",
       "           9.6418e-02, -1.3971e-02,  9.3559e-02,  9.9298e-02, -3.5794e-02,\n",
       "          -7.6480e-02,  4.7660e-02,  3.6927e-02,  4.1690e-02,  2.4833e-02,\n",
       "          -9.5205e-02, -2.3973e-03,  6.8838e-02, -1.2847e-03, -8.6938e-02,\n",
       "           7.2299e-02, -4.4844e-02, -9.3079e-02,  4.3442e-02,  9.9013e-02,\n",
       "           6.5329e-02,  6.6919e-02,  3.5625e-02, -7.0686e-02, -9.2966e-02,\n",
       "           9.2613e-02,  6.5840e-02,  3.6855e-03,  9.1546e-03,  8.8556e-02,\n",
       "          -2.0864e-02, -1.7592e-02, -6.4122e-02,  1.5676e-02,  1.5532e-02,\n",
       "          -3.4465e-02, -4.2876e-02,  1.1554e-02,  4.1142e-02,  2.5755e-02,\n",
       "           8.5135e-02,  3.7640e-02, -6.0853e-02,  4.8831e-02,  5.3207e-02,\n",
       "          -7.8127e-02,  4.8644e-02,  1.4126e-02, -6.6965e-02,  5.6060e-02,\n",
       "          -7.6534e-02, -8.3260e-02,  4.8547e-03,  3.2605e-03,  2.2940e-03,\n",
       "           8.7167e-02,  4.5509e-02,  5.7910e-03,  6.0110e-02,  2.4269e-02,\n",
       "          -2.4121e-02, -3.7430e-02,  4.6907e-02, -6.2783e-02, -1.8656e-02,\n",
       "          -5.8957e-02, -9.3817e-02, -3.0978e-02,  2.9433e-02, -4.5827e-02,\n",
       "           8.3003e-02, -7.2492e-02,  5.2782e-02,  2.0876e-02,  9.5764e-02,\n",
       "          -5.4721e-03, -5.4244e-02, -4.0076e-02, -9.2714e-02, -6.4672e-02,\n",
       "          -5.2045e-02,  5.3694e-02,  2.8729e-02,  2.4410e-02,  9.9749e-02,\n",
       "           8.7475e-02, -9.2509e-05, -1.0514e-02, -7.4584e-02,  8.7494e-02,\n",
       "          -6.2244e-02,  5.3405e-02,  5.8688e-02, -2.2500e-02, -4.1281e-02,\n",
       "           3.5868e-02,  7.3085e-02,  1.4474e-03, -8.7827e-02, -3.2961e-02,\n",
       "          -9.4743e-02,  2.1627e-02, -5.0364e-02,  5.8940e-02,  6.3368e-02,\n",
       "           7.5572e-02,  9.3378e-02,  2.2855e-02,  3.8145e-02,  3.5070e-02,\n",
       "           7.2801e-02, -1.9549e-02, -7.0252e-02, -4.4035e-02,  2.1789e-02,\n",
       "          -5.4246e-02, -4.8184e-02,  3.2690e-02, -6.8900e-02,  2.4671e-02,\n",
       "           4.0700e-02,  5.6222e-02,  1.1238e-02,  7.9659e-02,  9.0059e-02,\n",
       "           5.1536e-02, -7.1181e-03, -4.5536e-02,  2.3052e-02, -4.3925e-02,\n",
       "          -3.6141e-03,  7.9352e-02, -1.9053e-02, -3.1636e-02, -3.3962e-03,\n",
       "           2.0022e-02, -8.7166e-02, -2.1191e-02,  3.0507e-02,  1.3836e-03,\n",
       "          -6.6192e-02, -3.6636e-02,  1.2744e-02,  6.2239e-02, -5.2779e-02,\n",
       "           9.1140e-02,  5.8181e-02,  3.2170e-02,  3.9490e-02, -6.9345e-02,\n",
       "           3.5795e-02, -9.3688e-02, -1.2563e-03, -4.7383e-02,  8.4438e-02,\n",
       "          -3.3842e-02,  2.3418e-02,  3.5568e-02, -7.7574e-02,  6.3143e-02,\n",
       "          -3.6491e-02,  8.4556e-02, -1.0589e-02, -7.7334e-02,  9.8870e-02,\n",
       "           9.6316e-03,  9.6689e-02,  4.9977e-02, -7.2354e-02,  3.3972e-02,\n",
       "           9.8672e-02,  2.9454e-02, -8.3864e-02, -1.0327e-02,  6.4143e-02,\n",
       "           7.3782e-02,  5.2721e-02,  2.7625e-02, -5.2067e-02,  4.3018e-02,\n",
       "           4.5867e-02,  1.1823e-02,  3.8325e-02, -2.7884e-02,  2.9250e-02,\n",
       "           4.8407e-02, -2.0361e-02, -8.6166e-02,  4.3410e-02, -3.0567e-02,\n",
       "          -5.8800e-03,  3.8076e-02, -3.0337e-02,  7.2419e-02,  3.3432e-02,\n",
       "           6.0577e-02], requires_grad=True)),\n",
       " ('critic._h.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0349,  0.0106,  0.0386,  ...,  0.0068,  0.0388,  0.0259],\n",
       "          [ 0.0077,  0.0495,  0.0412,  ...,  0.0417, -0.0575,  0.0255],\n",
       "          [ 0.0401, -0.0377, -0.0262,  ...,  0.0286, -0.0251, -0.0611],\n",
       "          ...,\n",
       "          [-0.0461, -0.0288, -0.0281,  ..., -0.0395, -0.0118, -0.0563],\n",
       "          [-0.0261,  0.0212,  0.0081,  ...,  0.0594,  0.0385,  0.0428],\n",
       "          [ 0.0003,  0.0237,  0.0495,  ...,  0.0468,  0.0397,  0.0147]],\n",
       "         requires_grad=True)),\n",
       " ('critic._h.2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0247, -0.0537, -0.0538,  0.0326,  0.0330,  0.0494,  0.0125,  0.0360,\n",
       "           0.0217,  0.0178,  0.0293,  0.0251,  0.0137,  0.0062,  0.0619,  0.0561,\n",
       "          -0.0546, -0.0137, -0.0039,  0.0510, -0.0352, -0.0303,  0.0137, -0.0315,\n",
       "          -0.0071,  0.0434, -0.0133,  0.0576, -0.0577, -0.0320, -0.0218,  0.0263],\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nmf1.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fafa4d45250>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf1.critic.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNMF_MI_separate(GaussianNMF):\n",
    "    \"\"\"\n",
    "    Centralized separately training NMF model and mutual information parameters\n",
    "    We train mutual information parameters and other NMF parameters separately...\n",
    "    train mutual information parameters first and NMF parameters later in each iteration\n",
    "    class for Non-Negetive Matrix Factorization using Gaussian Method \n",
    "    with mutual information regularizer\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, A, k, critic_config):\n",
    "        \"\"\" initialization\n",
    "        \n",
    "        :params[in]: A, k\n",
    "        :params[in]: critic_config, a dictionary, \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        super(GaussianNMF_MI_separate, self).__init__(A, k)  ## instantiate super class\n",
    "        self.critic_config = critic_config\n",
    "        ### critic function for computing mutual information\n",
    "        self.critic_config['dim1'] = self.rows\n",
    "        self.critic_config['dim2'] = k\n",
    "        self.critic = SeparableCritic(**self.critic_config)\n",
    "  \n",
    "\n",
    "    def sgd_train(self, epochs, batch_size, lr, mi_lr, xi, step_size=10, gamma=0.9, W_init = None):\n",
    "        \"\"\"\n",
    "        train with stochastic gradient descent\n",
    "        :params[in]: epochs,\n",
    "        \n",
    "        :params[out]: W, H\n",
    "        \n",
    "        ** \n",
    "        x = mini_data.T\n",
    "        y = mini_datah.T\n",
    "        \"\"\"\n",
    "        if W_init is not None:\n",
    "            self.W.data = W_init.data\n",
    "        optimizer = torch.optim.SGD([self.W, self.H], lr=lr)\n",
    "        mi_optimizer = torch.optim.SGD(nmf1.critic.parameters(), lr=mi_lr)\n",
    "        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "        mi_scheduler = StepLR(mi_optimizer, step_size=step_size, gamma=gamma)\n",
    "        data_index = list(range(self.cols))   ## all column indices\n",
    "        for i in range(epochs):\n",
    "            mini_batches = self.chunks(data_index, batch_size)\n",
    "            for it in mini_batches:\n",
    "                mini_data = A[:, it]\n",
    "                mini_datah= self.H[:, it]\n",
    "\n",
    "                # calculate mi \n",
    "                #mi = estimate_mutual_information('smile', mini_data.T, mini_datah.T, \n",
    "                #                                 self.critic)\n",
    "                mi_loss = -estimate_mutual_information('smile', mini_data.T, mini_datah.T.detach(), \n",
    "                                                 self.critic)\n",
    "                mi_optimizer.zero_grad()   ## zero all gradients\n",
    "                mi_loss.backward()         ## find derivatives\n",
    "                mi_optimizer.step()  \n",
    "                \n",
    "                ## data in a minibatch\n",
    "                pred = self.forward()[:, it]  ## prediction\n",
    "                ## compute mutual information\n",
    "                mi = estimate_mutual_information('smile', mini_data.T, mini_datah.T, \n",
    "                                                 self.critic)\n",
    "                #guassian\n",
    "                loss = (mini_data - pred).pow(2).sum() - xi * mi\n",
    "                ## backward\n",
    "                #lr = scheduler.get_lr()\n",
    "                optimizer.zero_grad()   ## zero all gradients\n",
    "                loss.backward()## find derivatives\n",
    "                #load_state_dict(state_dict)\n",
    "                optimizer.step()  \n",
    "                self.W.data[self.W.data < 0] = 0. \n",
    "                self.H.data[self.H.data < 0] = 0. \n",
    "            print()\n",
    "            ## renew learning rate\n",
    "            #print('Epoch前:', i,'LR:', lr)\n",
    "            scheduler.step()\n",
    "            mi_scheduler.step()\n",
    "            #print('Epoch:', i,'LR:', lr)\n",
    "            ## shuffle indices\n",
    "            np.random.shuffle(data_index) \n",
    "            ## current loss\n",
    "            cur_loss = (self.A - self.W@self.H).pow(2).sum()\n",
    "            print('loss at Epoch ',i, ' ',cur_loss.item())\n",
    "            #print(lr)\n",
    "        ## return\n",
    "        return self.W, self.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss at Epoch  0   4574194.0\n",
      "\n",
      "loss at Epoch  1   46369.63671875\n",
      "\n",
      "loss at Epoch  2   40113.1015625\n",
      "\n",
      "loss at Epoch  3   39845.421875\n",
      "\n",
      "loss at Epoch  4   39091.54296875\n",
      "\n",
      "loss at Epoch  5   36612.8984375\n",
      "\n",
      "loss at Epoch  6   35432.2265625\n",
      "\n",
      "loss at Epoch  7   33708.671875\n",
      "\n",
      "loss at Epoch  8   31520.47265625\n",
      "\n",
      "loss at Epoch  9   30095.58203125\n",
      "\n",
      "loss at Epoch  10   28205.390625\n",
      "\n",
      "loss at Epoch  11   26855.47265625\n",
      "\n",
      "loss at Epoch  12   25717.77734375\n",
      "\n",
      "loss at Epoch  13   24545.767578125\n",
      "\n",
      "loss at Epoch  14   23799.6328125\n",
      "\n",
      "loss at Epoch  15   23101.685546875\n",
      "\n",
      "loss at Epoch  16   22593.44140625\n",
      "\n",
      "loss at Epoch  17   22212.4453125\n",
      "\n",
      "loss at Epoch  18   21809.0\n",
      "\n",
      "loss at Epoch  19   21442.146484375\n",
      "\n",
      "loss at Epoch  20   21061.83203125\n",
      "\n",
      "loss at Epoch  21   20784.375\n",
      "\n",
      "loss at Epoch  22   20593.79296875\n",
      "\n",
      "loss at Epoch  23   20551.4296875\n",
      "\n",
      "loss at Epoch  24   20956.1875\n",
      "\n",
      "loss at Epoch  25   20512.0625\n",
      "\n",
      "loss at Epoch  26   20091.421875\n",
      "\n",
      "loss at Epoch  27   20069.775390625\n",
      "\n",
      "loss at Epoch  28   19856.390625\n",
      "\n",
      "loss at Epoch  29   19833.640625\n",
      "\n",
      "loss at Epoch  30   19835.16796875\n",
      "\n",
      "loss at Epoch  31   19679.5390625\n",
      "\n",
      "loss at Epoch  32   19627.91796875\n",
      "\n",
      "loss at Epoch  33   19605.3515625\n",
      "\n",
      "loss at Epoch  34   19581.6328125\n",
      "\n",
      "loss at Epoch  35   19553.55859375\n",
      "\n",
      "loss at Epoch  36   19507.14453125\n",
      "\n",
      "loss at Epoch  37   19486.451171875\n",
      "\n",
      "loss at Epoch  38   19458.5546875\n",
      "\n",
      "loss at Epoch  39   19444.375\n",
      "\n",
      "loss at Epoch  40   19423.87890625\n",
      "\n",
      "loss at Epoch  41   19417.08203125\n",
      "\n",
      "loss at Epoch  42   19400.5\n",
      "\n",
      "loss at Epoch  43   19405.8828125\n",
      "\n",
      "loss at Epoch  44   19391.47265625\n",
      "\n",
      "loss at Epoch  45   19380.73828125\n",
      "\n",
      "loss at Epoch  46   19416.017578125\n",
      "\n",
      "loss at Epoch  47   19393.73046875\n",
      "\n",
      "loss at Epoch  48   19620.56640625\n",
      "\n",
      "loss at Epoch  49   19495.50390625\n"
     ]
    }
   ],
   "source": [
    "#NMF-MI method \n",
    "k=100\n",
    "\n",
    "#A = torch.randn(count_mat.shape[0],count_mat.shape[1])\n",
    "critic_params = {\n",
    "    'layers': 0,\n",
    "    'embed_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "##init nmf especially the W, H\n",
    "nmf1 = GaussianNMF_MI_separate(A, k, critic_params)\n",
    "##put W in every client\n",
    "\n",
    "W, H = nmf1.sgd_train(epochs=50, batch_size=256, lr=4.e-2,mi_lr=4.e-2, xi = 1.e-1, gamma=0.95)#, W_init = torch.rand([2153, 20])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[3.2984e-03, 0.0000e+00, 0.0000e+00,  ..., 4.9399e-09, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [4.3528e-03, 0.0000e+00, 0.0000e+00,  ..., 5.4490e-09, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.5436e-10, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6703e-01,\n",
       "         0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [1.1457e-02, 1.3531e-02, 0.0000e+00,  ..., 0.0000e+00, 7.0634e-04,\n",
       "         0.0000e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf1.H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(labels)))   ## indices of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into train and test\n",
    "ind_train, ind_test, y_train, y_test = train_test_split(\n",
    "    indices, labels, test_size=0.2, random_state=2021, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 5572)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## train/test datasets\n",
    "\n",
    "H = H.detach().numpy()\n",
    "print(H.shape)\n",
    "x_train, x_test = H[:, ind_train],H[:, ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.986     0.977     0.982       966\n",
      "           1      0.861     0.913     0.886       149\n",
      "\n",
      "    accuracy                          0.969      1115\n",
      "   macro avg      0.924     0.945     0.934      1115\n",
      "weighted avg      0.970     0.969     0.969      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## encode labels to integers\n",
    "Encoder = LabelEncoder()\n",
    "Y_train = Encoder.fit_transform(y_train)\n",
    "Y_test = Encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "# Classifier - Algorithm - SVM -- linear kernel\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1., kernel='linear', degree=3, gamma='auto', random_state=82, class_weight='balanced')\n",
    "SVM.fit(x_train.T, Y_train)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(x_test.T) # make predictions\n",
    "print(classification_report(Y_test, predictions_SVM, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coherence score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = top_keywords(W, features, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34049606"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compute the coherence score for each topic\n",
    "coherence_vec = []\n",
    "for i in range(W.shape[1]):  \n",
    "    coherence_vec.append(coherence(dic[i], model_glove))\n",
    "\n",
    "np.mean(coherence_vec)   ## the mean coherence score of all topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated learning SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([869, 5572])\n"
     ]
    }
   ],
   "source": [
    "A = torch.FloatTensor(count_mat)\n",
    "#A. type\n",
    "print(A.shape)\n",
    "#A=A.type(torch.FloatTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(H, labels):\n",
    "    indices = list(range(len(labels)))   ## indices of documents\n",
    "    \n",
    "    ## split data into train and test\n",
    "    ind_train, ind_test, y_train, y_test = train_test_split(\n",
    "        indices, labels, test_size=0.2, random_state=2021, stratify=labels)\n",
    "    H_new = H.detach().numpy()\n",
    "    x_train, x_test = H_new[:, ind_train],H_new[:, ind_test]\n",
    "    \n",
    "    ## encode labels to integers\n",
    "    Encoder = LabelEncoder()\n",
    "    Y_train = Encoder.fit_transform(y_train)\n",
    "    Y_test = Encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "    # Classifier - Algorithm - SVM -- linear kernel\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1., kernel='linear', degree=3, gamma='auto', random_state=82, class_weight='balanced')\n",
    "    SVM.fit(x_train.T, Y_train)# predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(x_test.T) # make predictions\n",
    "    print(classification_report(Y_test, predictions_SVM, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-2c4881dab0bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'random_sample' is not defined"
     ]
    }
   ],
   "source": [
    "help(random_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Fed_NMF(nn.Module):\n",
    "    \"\"\"\n",
    "    Federaated NMF \n",
    "    \"\"\"\n",
    "    def __init__(self, A, k, K):\n",
    "        \"\"\"\n",
    "        initialization \n",
    "        :params[in], A, full matrix of all texts data\n",
    "        :params[in], k, number of topics\n",
    "        :params[in], K, number of clients\n",
    "        \n",
    "        \"\"\"\n",
    "        super(Fed_NMF, self).__init__()\n",
    "        self.A = A\n",
    "        self.rows,self.cols = A.size()\n",
    "        self.k = k  ## topic number\n",
    "        self.K = K  ## number of clients\n",
    "\n",
    "    def split_clients(self):\n",
    "        \"\"\"\n",
    "        split the full matrix into m clients by column iid\n",
    "        \n",
    "        :params[in]: K, the number of clients\n",
    "        \n",
    "        :params[out]: B, tuple of tensors\n",
    "        \"\"\"\n",
    "        data_index = list(range(self.cols))   ## all column indices\n",
    "        #np.random.shuffle(data_index) \n",
    "        ## split into chunks after shuffle\n",
    "        B = torch.chunk(self.A[:,data_index], self.K, dim = 1)\n",
    "        return B\n",
    "    \n",
    "    def split_clients_non_iid(self):\n",
    "        \"\"\"\n",
    "        produce non-iid datasets for clients\n",
    "        \"\"\"\n",
    "        \n",
    "        client_dis=dirichlet([1,1], self.K)\n",
    "        spam=[]\n",
    "        ham=[]\n",
    "        num=500\n",
    "        for i in range(len(labels)):\n",
    "            if labels[i]=='ham':\n",
    "                ham.append(i)\n",
    "            elif labels[i]=='spam':\n",
    "                spam.append(i)\n",
    "        client_sample=[]\n",
    "        for i in client_dis: \n",
    "            spam_sample=np.random.choice(spam, size=int(i[0]*num),replace=False)\n",
    "            ham_sample=np.random.choice(ham, size=num-int(i[0]*num))\n",
    "            client_sample.append(np.concatenate((spam_sample,ham_sample)))\n",
    "            n=torch.from_numpy(np.array(client_sample))\n",
    "            B=torch.chunk(n,self.K)\n",
    "        return B\n",
    "\n",
    "\n",
    "    def split_into_chunks(self, ls, batch_size):\n",
    "        \"\"\"\n",
    "        split a list of number into chunks up to a certain batch_size\n",
    "        \n",
    "        :params[in]: ls, a list of numbers\n",
    "        :params[in]: batch_size, an interger     \n",
    "        \n",
    "        :params[out]: a generator\n",
    "        \"\"\"\n",
    "        np.random.shuffle(ls) \n",
    "        for i in range(0, len(ls), batch_size): \n",
    "            yield ls[i:i + batch_size]\n",
    "\n",
    "    \n",
    "    def server_train(self, labels, iters, C, epoch, batch_size, lr, xi):\n",
    "        \"\"\"\n",
    "        federated learning\n",
    "        \n",
    "        :params[in], iters, the number of interations for fedrated learning\n",
    "        :params[in], C, the fraction of clients for each iteration\n",
    "        :params[in], epoch, the number of epochs for local SGD\n",
    "        :params[in], batch_size, the batch_size for local SGD\n",
    "        :params[in], lr, the learning rate for local SGD\n",
    "        :params[in], xi, the mutual information for local SGD\n",
    "        \n",
    "        :params[out]\n",
    "        :\n",
    "        \"\"\"\n",
    "        m = int(max(C*self.K,1))  ## number of clients for each iteration\n",
    "        ## split whole dataset into K clients\n",
    "        # torch.manual_seed(4)\n",
    "        B = self.split_clients()\n",
    "        ## column number for all clients -- using list comprehension\n",
    "        num_cols = [it.size(1) for it in B]\n",
    "        #Cols = num_cols.sum()\n",
    "        ## initialization of W tensor\n",
    "        W = torch.rand(self.rows, self.k)\n",
    "        ## nmf model for all clients using dictionary comprehension\n",
    "        nmf_models = {i: GaussianNMF_MI(B[i], self.k, critic_params) for i in range(self.K)}\n",
    "        random.seed(18)\n",
    "        for i in range(iters):     ## each iteration                  \n",
    "            set_clients = random.sample(list(range(self.K)), m)  ## selection of client's id \n",
    "            temp = [num_cols[it1] for it1 in set_clients]            \n",
    "            col_sum = sum(temp)\n",
    "            tmp_W = torch.zeros(self.rows, self.k)\n",
    "            ## for each selected client\n",
    "            for j in set_clients:\n",
    "                W1, _ = nmf_models[j].sgd_train(epoch, batch_size, lr, xi, W)\n",
    "                tmp_W += W1.detach().data * num_cols[j]/col_sum\n",
    "            W = tmp_W\n",
    "            H_list = [nmf_models[it2].H.detach().data for it2 in range(self.K) ]\n",
    "            ## after convergence\n",
    "            H = torch.cat(H_list , 1)\n",
    "            evaluation(H,labels)\n",
    "        return W, H\n",
    "\n",
    "    def server_train_epoch(self, labels, iters, C, epoch, batch_size, lr, xi):\n",
    "        \"\"\"\n",
    "        federated learning by training over all clients in each iteration\n",
    "        \n",
    "        :params[in], iters, the number of interations for fedrated learning\n",
    "        :params[in], list, each data's label\n",
    "        :params[in], C, the fraction of clients for each iteration\n",
    "        :params[in], epoch, the number of epochs for local SGD\n",
    "        :params[in], batch_size, the batch_size for local SGD\n",
    "        :params[in], lr, the learning rate for local SGD\n",
    "        :params[in], xi, the mutual information for local \n",
    "        SGD\n",
    "        \n",
    "        :params[out]\n",
    "        :\n",
    "        \"\"\"\n",
    "        m = int(max(C*self.K,1))  ## batch_size for clients\n",
    "        ## split whole dataset into K clients\n",
    "        #torch.manual_seed(4)\n",
    "        B = self.split_clients()\n",
    "        ## column number for all clients -- using list comprehension\n",
    "        num_cols = [it.size(1) for it in B]\n",
    "        #Cols = num_cols.sum()\n",
    "        ## initialization of W tensor\n",
    "        W = torch.rand(self.rows, self.k)\n",
    "        ## nmf model for all clients using dictionary comprehension\n",
    "        nmf_models = {i: GaussianNMF_MI(B[i], self.k, critic_params) for i in range(self.K)}\n",
    "        #random.seed(18)\n",
    "        for i in range(iters):     ## each iteration -- training over all clients \n",
    "            ## split clients into batches\n",
    "            batches = self.split_into_chunks(list(range(self.K)), m)  \n",
    "            ### for loop over all batches of clients\n",
    "            for set_clients in batches:   ## selection of client's id \n",
    "                temp = [num_cols[it1] for it1 in set_clients]            \n",
    "                col_sum = sum(temp)\n",
    "                tmp_W = torch.zeros(self.rows, self.k)\n",
    "                ## for each selected client\n",
    "                for j in set_clients:\n",
    "                    W1, _ = nmf_models[j].sgd_train(epoch, batch_size, lr, xi, W)\n",
    "                    tmp_W += W1.detach().data * num_cols[j]/col_sum\n",
    "                W = tmp_W     ##update W\n",
    "            ## list of H matrices for each iteration\n",
    "            H_list = [nmf_models[it2].H.detach().data for it2 in range(self.K) ]\n",
    "            ## merge into H\n",
    "            H = torch.cat(H_list , 1)\n",
    "            ### evaluate its performance on classification\n",
    "            evaluation(H, labels)\n",
    "        return W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf1 = Fed_NMF(A, 100,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  0   247790.21875\n",
      "loss at Epoch  1   165534.25\n",
      "loss at Epoch  2   124303.796875\n",
      "loss at Epoch  3   100022.9375\n",
      "loss at Epoch  4   84234.3125\n",
      "loss at Epoch  5   73235.3125\n",
      "loss at Epoch  6   65229.8671875\n",
      "loss at Epoch  7   59166.96875\n",
      "loss at Epoch  8   54453.28125\n",
      "loss at Epoch  9   50715.2734375\n",
      "loss at Epoch  10   47698.8515625\n",
      "loss at Epoch  11   45220.2578125\n",
      "loss at Epoch  12   43164.421875\n",
      "loss at Epoch  13   41438.2578125\n",
      "loss at Epoch  14   39979.16015625\n",
      "loss at Epoch  15   38734.69140625\n",
      "loss at Epoch  16   37667.62109375\n",
      "loss at Epoch  17   36747.921875\n",
      "loss at Epoch  18   35950.7265625\n",
      "loss at Epoch  19   35256.6171875\n",
      "loss at Epoch  20   34650.26171875\n",
      "loss at Epoch  21   34118.9375\n",
      "loss at Epoch  22   33651.75390625\n",
      "loss at Epoch  23   33240.078125\n",
      "loss at Epoch  24   32876.4609375\n",
      "loss at Epoch  25   32554.703125\n",
      "loss at Epoch  26   32269.51171875\n",
      "loss at Epoch  27   32016.369140625\n",
      "loss at Epoch  28   31791.20703125\n",
      "loss at Epoch  29   31590.763671875\n",
      "loss at Epoch  30   31412.126953125\n",
      "loss at Epoch  31   31252.73828125\n",
      "loss at Epoch  32   31110.41015625\n",
      "loss at Epoch  33   30983.220703125\n",
      "loss at Epoch  34   30869.494140625\n",
      "loss at Epoch  35   30767.72265625\n",
      "loss at Epoch  36   30676.58203125\n",
      "loss at Epoch  37   30594.923828125\n",
      "loss at Epoch  38   30521.732421875\n",
      "loss at Epoch  39   30456.10546875\n",
      "loss at Epoch  0   109607.296875\n",
      "loss at Epoch  1   59160.7890625\n",
      "loss at Epoch  2   37629.40234375\n",
      "loss at Epoch  3   26465.0703125\n",
      "loss at Epoch  4   19997.4453125\n",
      "loss at Epoch  5   15929.3486328125\n",
      "loss at Epoch  6   13203.55859375\n",
      "loss at Epoch  7   11289.1748046875\n",
      "loss at Epoch  8   9894.04296875\n",
      "loss at Epoch  9   8847.0615234375\n",
      "loss at Epoch  10   8041.955078125\n",
      "loss at Epoch  11   7410.3310546875\n",
      "loss at Epoch  12   6906.578125\n",
      "loss at Epoch  13   6499.0771484375\n",
      "loss at Epoch  14   6165.12158203125\n",
      "loss at Epoch  15   5888.0419921875\n",
      "loss at Epoch  16   5655.8173828125\n",
      "loss at Epoch  17   5459.51953125\n",
      "loss at Epoch  18   5292.31640625\n",
      "loss at Epoch  19   5149.27392578125\n",
      "loss at Epoch  20   5026.30126953125\n",
      "loss at Epoch  21   4919.947265625\n",
      "loss at Epoch  22   4827.478515625\n",
      "loss at Epoch  23   4746.79296875\n",
      "loss at Epoch  24   4676.140625\n",
      "loss at Epoch  25   4614.18359375\n",
      "loss at Epoch  26   4559.701171875\n",
      "loss at Epoch  27   4511.634765625\n",
      "loss at Epoch  28   4469.1904296875\n",
      "loss at Epoch  29   4431.5712890625\n",
      "loss at Epoch  30   4398.2236328125\n",
      "loss at Epoch  31   4368.59375\n",
      "loss at Epoch  32   4342.2548828125\n",
      "loss at Epoch  33   4318.7939453125\n",
      "loss at Epoch  34   4297.869140625\n",
      "loss at Epoch  35   4279.19287109375\n",
      "loss at Epoch  36   4262.5048828125\n",
      "loss at Epoch  37   4247.615234375\n",
      "loss at Epoch  38   4234.2646484375\n",
      "loss at Epoch  39   4222.33544921875\n",
      "loss at Epoch  0   3706.099365234375\n",
      "loss at Epoch  1   3202.298828125\n",
      "loss at Epoch  2   2857.3515625\n",
      "loss at Epoch  3   2610.82568359375\n",
      "loss at Epoch  4   2428.0615234375\n",
      "loss at Epoch  5   2289.531494140625\n",
      "loss at Epoch  6   2181.9072265625\n",
      "loss at Epoch  7   2095.71826171875\n",
      "loss at Epoch  8   2027.09619140625\n",
      "loss at Epoch  9   1970.4368896484375\n",
      "loss at Epoch  10   1923.9893798828125\n",
      "loss at Epoch  11   1885.4097900390625\n",
      "loss at Epoch  12   1852.8353271484375\n",
      "loss at Epoch  13   1825.42626953125\n",
      "loss at Epoch  14   1801.487060546875\n",
      "loss at Epoch  15   1781.217041015625\n",
      "loss at Epoch  16   1763.564208984375\n",
      "loss at Epoch  17   1748.511962890625\n",
      "loss at Epoch  18   1735.22802734375\n",
      "loss at Epoch  19   1723.591796875\n",
      "loss at Epoch  20   1713.2943115234375\n",
      "loss at Epoch  21   1704.333984375\n",
      "loss at Epoch  22   1696.42333984375\n",
      "loss at Epoch  23   1689.473876953125\n",
      "loss at Epoch  24   1683.38818359375\n",
      "loss at Epoch  25   1678.0587158203125\n",
      "loss at Epoch  26   1673.11474609375\n",
      "loss at Epoch  27   1668.843505859375\n",
      "loss at Epoch  28   1664.9881591796875\n",
      "loss at Epoch  29   1661.61865234375\n",
      "loss at Epoch  30   1658.54345703125\n",
      "loss at Epoch  31   1655.84814453125\n",
      "loss at Epoch  32   1653.43310546875\n",
      "loss at Epoch  33   1651.2734375\n",
      "loss at Epoch  34   1649.349853515625\n",
      "loss at Epoch  35   1647.603271484375\n",
      "loss at Epoch  36   1646.0419921875\n",
      "loss at Epoch  37   1644.656494140625\n",
      "loss at Epoch  38   1643.427001953125\n",
      "loss at Epoch  39   1642.2882080078125\n",
      "loss at Epoch  0   1541.685302734375\n",
      "loss at Epoch  1   1492.7171630859375\n",
      "loss at Epoch  2   1456.03564453125\n",
      "loss at Epoch  3   1424.6834716796875\n",
      "loss at Epoch  4   1403.453125\n",
      "loss at Epoch  5   1383.237060546875\n",
      "loss at Epoch  6   1368.66845703125\n",
      "loss at Epoch  7   1355.904296875\n",
      "loss at Epoch  8   1344.399658203125\n",
      "loss at Epoch  9   1335.33984375\n",
      "loss at Epoch  10   1327.4130859375\n",
      "loss at Epoch  11   1321.1220703125\n",
      "loss at Epoch  12   1314.9876708984375\n",
      "loss at Epoch  13   1310.0675048828125\n",
      "loss at Epoch  14   1305.975830078125\n",
      "loss at Epoch  15   1302.2596435546875\n",
      "loss at Epoch  16   1298.852294921875\n",
      "loss at Epoch  17   1295.742431640625\n",
      "loss at Epoch  18   1293.3846435546875\n",
      "loss at Epoch  19   1291.061767578125\n",
      "loss at Epoch  20   1289.0433349609375\n",
      "loss at Epoch  21   1287.324951171875\n",
      "loss at Epoch  22   1285.9525146484375\n",
      "loss at Epoch  23   1284.529296875\n",
      "loss at Epoch  24   1283.17529296875\n",
      "loss at Epoch  25   1282.1722412109375\n",
      "loss at Epoch  26   1281.1455078125\n",
      "loss at Epoch  27   1280.2823486328125\n",
      "loss at Epoch  28   1279.48779296875\n",
      "loss at Epoch  29   1278.795654296875\n",
      "loss at Epoch  30   1278.17724609375\n",
      "loss at Epoch  31   1277.629150390625\n",
      "loss at Epoch  32   1277.1005859375\n",
      "loss at Epoch  33   1276.6658935546875\n",
      "loss at Epoch  34   1276.3065185546875\n",
      "loss at Epoch  35   1275.939208984375\n",
      "loss at Epoch  36   1275.601806640625\n",
      "loss at Epoch  37   1275.2921142578125\n",
      "loss at Epoch  38   1275.05126953125\n",
      "loss at Epoch  39   1274.80712890625\n",
      "loss at Epoch  0   1329.10595703125\n",
      "loss at Epoch  1   1319.86376953125\n",
      "loss at Epoch  2   1311.520263671875\n",
      "loss at Epoch  3   1303.544677734375\n",
      "loss at Epoch  4   1297.325927734375\n",
      "loss at Epoch  5   1292.45849609375\n",
      "loss at Epoch  6   1288.9482421875\n",
      "loss at Epoch  7   1285.2847900390625\n",
      "loss at Epoch  8   1281.786865234375\n",
      "loss at Epoch  9   1279.1357421875\n",
      "loss at Epoch  10   1277.143798828125\n",
      "loss at Epoch  11   1275.025146484375\n",
      "loss at Epoch  12   1273.596923828125\n",
      "loss at Epoch  13   1272.1806640625\n",
      "loss at Epoch  14   1270.9471435546875\n",
      "loss at Epoch  15   1269.87939453125\n",
      "loss at Epoch  16   1268.694580078125\n",
      "loss at Epoch  17   1267.7935791015625\n",
      "loss at Epoch  18   1266.9698486328125\n",
      "loss at Epoch  19   1266.4444580078125\n",
      "loss at Epoch  20   1265.89453125\n",
      "loss at Epoch  21   1265.40673828125\n",
      "loss at Epoch  22   1264.7886962890625\n",
      "loss at Epoch  23   1264.40283203125\n",
      "loss at Epoch  24   1264.007080078125\n",
      "loss at Epoch  25   1263.725830078125\n",
      "loss at Epoch  26   1263.352294921875\n",
      "loss at Epoch  27   1263.131591796875\n",
      "loss at Epoch  28   1262.9354248046875\n",
      "loss at Epoch  29   1262.71630859375\n",
      "loss at Epoch  30   1262.458984375\n",
      "loss at Epoch  31   1262.2928466796875\n",
      "loss at Epoch  32   1262.130126953125\n",
      "loss at Epoch  33   1262.00439453125\n",
      "loss at Epoch  34   1261.8670654296875\n",
      "loss at Epoch  35   1261.758544921875\n",
      "loss at Epoch  36   1261.684814453125\n",
      "loss at Epoch  37   1261.5943603515625\n",
      "loss at Epoch  38   1261.497314453125\n",
      "loss at Epoch  39   1261.4205322265625\n",
      "loss at Epoch  0   1532.113525390625\n",
      "loss at Epoch  1   1529.118408203125\n",
      "loss at Epoch  2   1525.262451171875\n",
      "loss at Epoch  3   1523.552734375\n",
      "loss at Epoch  4   1521.522216796875\n",
      "loss at Epoch  5   1519.8419189453125\n",
      "loss at Epoch  6   1518.448974609375\n",
      "loss at Epoch  7   1517.611328125\n",
      "loss at Epoch  8   1516.065185546875\n",
      "loss at Epoch  9   1516.04345703125\n",
      "loss at Epoch  10   1515.0732421875\n",
      "loss at Epoch  11   1514.1082763671875\n",
      "loss at Epoch  12   1513.70703125\n",
      "loss at Epoch  13   1513.328369140625\n",
      "loss at Epoch  14   1512.6583251953125\n",
      "loss at Epoch  15   1512.5609130859375\n",
      "loss at Epoch  16   1511.9560546875\n",
      "loss at Epoch  17   1511.84130859375\n",
      "loss at Epoch  18   1511.3798828125\n",
      "loss at Epoch  19   1511.16015625\n",
      "loss at Epoch  20   1511.108642578125\n",
      "loss at Epoch  21   1510.81396484375\n",
      "loss at Epoch  22   1510.60400390625\n",
      "loss at Epoch  23   1510.490234375\n",
      "loss at Epoch  24   1510.316162109375\n",
      "loss at Epoch  25   1510.219482421875\n",
      "loss at Epoch  26   1510.07421875\n",
      "loss at Epoch  27   1509.948974609375\n",
      "loss at Epoch  28   1509.8876953125\n",
      "loss at Epoch  29   1509.815673828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  30   1509.7874755859375\n",
      "loss at Epoch  31   1509.72998046875\n",
      "loss at Epoch  32   1509.69140625\n",
      "loss at Epoch  33   1509.59912109375\n",
      "loss at Epoch  34   1509.535888671875\n",
      "loss at Epoch  35   1509.5040283203125\n",
      "loss at Epoch  36   1509.4638671875\n",
      "loss at Epoch  37   1509.439453125\n",
      "loss at Epoch  38   1509.397705078125\n",
      "loss at Epoch  39   1509.388916015625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.526     0.650       966\n",
      "           1      0.118     0.409     0.183       149\n",
      "\n",
      "    accuracy                          0.510      1115\n",
      "   macro avg      0.485     0.468     0.417      1115\n",
      "weighted avg      0.754     0.510     0.588      1115\n",
      "\n",
      "loss at Epoch  0   5751.79736328125\n",
      "loss at Epoch  1   3778.0849609375\n",
      "loss at Epoch  2   2913.8662109375\n",
      "loss at Epoch  3   2463.036376953125\n",
      "loss at Epoch  4   2198.453125\n",
      "loss at Epoch  5   2029.40185546875\n",
      "loss at Epoch  6   1914.748291015625\n",
      "loss at Epoch  7   1831.211669921875\n",
      "loss at Epoch  8   1769.04443359375\n",
      "loss at Epoch  9   1719.81201171875\n",
      "loss at Epoch  10   1681.5821533203125\n",
      "loss at Epoch  11   1649.968017578125\n",
      "loss at Epoch  12   1625.1168212890625\n",
      "loss at Epoch  13   1603.9420166015625\n",
      "loss at Epoch  14   1586.2105712890625\n",
      "loss at Epoch  15   1571.73779296875\n",
      "loss at Epoch  16   1559.152099609375\n",
      "loss at Epoch  17   1548.276123046875\n",
      "loss at Epoch  18   1538.93798828125\n",
      "loss at Epoch  19   1530.8154296875\n",
      "loss at Epoch  20   1523.7989501953125\n",
      "loss at Epoch  21   1517.9228515625\n",
      "loss at Epoch  22   1512.447998046875\n",
      "loss at Epoch  23   1507.850830078125\n",
      "loss at Epoch  24   1503.727783203125\n",
      "loss at Epoch  25   1500.033935546875\n",
      "loss at Epoch  26   1496.9674072265625\n",
      "loss at Epoch  27   1494.1021728515625\n",
      "loss at Epoch  28   1491.567626953125\n",
      "loss at Epoch  29   1489.302734375\n",
      "loss at Epoch  30   1487.260498046875\n",
      "loss at Epoch  31   1485.498046875\n",
      "loss at Epoch  32   1483.89453125\n",
      "loss at Epoch  33   1482.496826171875\n",
      "loss at Epoch  34   1481.23046875\n",
      "loss at Epoch  35   1480.111083984375\n",
      "loss at Epoch  36   1479.09814453125\n",
      "loss at Epoch  37   1478.205322265625\n",
      "loss at Epoch  38   1477.3836669921875\n",
      "loss at Epoch  39   1476.67919921875\n",
      "loss at Epoch  0   1531.628662109375\n",
      "loss at Epoch  1   1503.3369140625\n",
      "loss at Epoch  2   1483.566650390625\n",
      "loss at Epoch  3   1468.147216796875\n",
      "loss at Epoch  4   1454.849365234375\n",
      "loss at Epoch  5   1445.320556640625\n",
      "loss at Epoch  6   1437.3763427734375\n",
      "loss at Epoch  7   1430.613037109375\n",
      "loss at Epoch  8   1424.1656494140625\n",
      "loss at Epoch  9   1419.566650390625\n",
      "loss at Epoch  10   1416.14453125\n",
      "loss at Epoch  11   1412.69091796875\n",
      "loss at Epoch  12   1409.390625\n",
      "loss at Epoch  13   1407.2276611328125\n",
      "loss at Epoch  14   1405.361328125\n",
      "loss at Epoch  15   1403.208740234375\n",
      "loss at Epoch  16   1401.589111328125\n",
      "loss at Epoch  17   1400.22412109375\n",
      "loss at Epoch  18   1399.0166015625\n",
      "loss at Epoch  19   1397.7822265625\n",
      "loss at Epoch  20   1396.79345703125\n",
      "loss at Epoch  21   1395.951416015625\n",
      "loss at Epoch  22   1395.101318359375\n",
      "loss at Epoch  23   1394.374267578125\n",
      "loss at Epoch  24   1393.7099609375\n",
      "loss at Epoch  25   1393.111083984375\n",
      "loss at Epoch  26   1392.6072998046875\n",
      "loss at Epoch  27   1392.18603515625\n",
      "loss at Epoch  28   1391.79296875\n",
      "loss at Epoch  29   1391.4459228515625\n",
      "loss at Epoch  30   1391.109619140625\n",
      "loss at Epoch  31   1390.8236083984375\n",
      "loss at Epoch  32   1390.642333984375\n",
      "loss at Epoch  33   1390.369873046875\n",
      "loss at Epoch  34   1390.1278076171875\n",
      "loss at Epoch  35   1389.9705810546875\n",
      "loss at Epoch  36   1389.8349609375\n",
      "loss at Epoch  37   1389.6795654296875\n",
      "loss at Epoch  38   1389.5570068359375\n",
      "loss at Epoch  39   1389.4498291015625\n",
      "loss at Epoch  0   1330.54736328125\n",
      "loss at Epoch  1   1324.038330078125\n",
      "loss at Epoch  2   1320.848388671875\n",
      "loss at Epoch  3   1316.3101806640625\n",
      "loss at Epoch  4   1312.1583251953125\n",
      "loss at Epoch  5   1309.6414794921875\n",
      "loss at Epoch  6   1307.35498046875\n",
      "loss at Epoch  7   1305.8631591796875\n",
      "loss at Epoch  8   1303.6025390625\n",
      "loss at Epoch  9   1302.33984375\n",
      "loss at Epoch  10   1301.418212890625\n",
      "loss at Epoch  11   1299.978271484375\n",
      "loss at Epoch  12   1299.625244140625\n",
      "loss at Epoch  13   1298.615478515625\n",
      "loss at Epoch  14   1297.9306640625\n",
      "loss at Epoch  15   1297.4757080078125\n",
      "loss at Epoch  16   1296.6214599609375\n",
      "loss at Epoch  17   1296.31298828125\n",
      "loss at Epoch  18   1295.899658203125\n",
      "loss at Epoch  19   1295.433837890625\n",
      "loss at Epoch  20   1295.177978515625\n",
      "loss at Epoch  21   1294.73046875\n",
      "loss at Epoch  22   1294.533447265625\n",
      "loss at Epoch  23   1294.22216796875\n",
      "loss at Epoch  24   1293.972412109375\n",
      "loss at Epoch  25   1293.833984375\n",
      "loss at Epoch  26   1293.6904296875\n",
      "loss at Epoch  27   1293.499755859375\n",
      "loss at Epoch  28   1293.38134765625\n",
      "loss at Epoch  29   1293.296142578125\n",
      "loss at Epoch  30   1293.17138671875\n",
      "loss at Epoch  31   1293.0361328125\n",
      "loss at Epoch  32   1292.9818115234375\n",
      "loss at Epoch  33   1292.8671875\n",
      "loss at Epoch  34   1292.840576171875\n",
      "loss at Epoch  35   1292.736083984375\n",
      "loss at Epoch  36   1292.686767578125\n",
      "loss at Epoch  37   1292.64111328125\n",
      "loss at Epoch  38   1292.580078125\n",
      "loss at Epoch  39   1292.544677734375\n",
      "loss at Epoch  0   1245.754150390625\n",
      "loss at Epoch  1   1243.4971923828125\n",
      "loss at Epoch  2   1242.244140625\n",
      "loss at Epoch  3   1238.4990234375\n",
      "loss at Epoch  4   1238.311767578125\n",
      "loss at Epoch  5   1237.224365234375\n",
      "loss at Epoch  6   1236.501953125\n",
      "loss at Epoch  7   1235.7987060546875\n",
      "loss at Epoch  8   1235.4951171875\n",
      "loss at Epoch  9   1234.42626953125\n",
      "loss at Epoch  10   1233.6396484375\n",
      "loss at Epoch  11   1233.349365234375\n",
      "loss at Epoch  12   1232.863525390625\n",
      "loss at Epoch  13   1232.697509765625\n",
      "loss at Epoch  14   1232.22314453125\n",
      "loss at Epoch  15   1232.112060546875\n",
      "loss at Epoch  16   1231.9027099609375\n",
      "loss at Epoch  17   1231.7529296875\n",
      "loss at Epoch  18   1231.511474609375\n",
      "loss at Epoch  19   1231.449462890625\n",
      "loss at Epoch  20   1231.203857421875\n",
      "loss at Epoch  21   1231.190185546875\n",
      "loss at Epoch  22   1231.036865234375\n",
      "loss at Epoch  23   1230.8994140625\n",
      "loss at Epoch  24   1230.859619140625\n",
      "loss at Epoch  25   1230.7288818359375\n",
      "loss at Epoch  26   1230.759521484375\n",
      "loss at Epoch  27   1230.6405029296875\n",
      "loss at Epoch  28   1230.5621337890625\n",
      "loss at Epoch  29   1230.5418701171875\n",
      "loss at Epoch  30   1230.482177734375\n",
      "loss at Epoch  31   1230.4517822265625\n",
      "loss at Epoch  32   1230.44970703125\n",
      "loss at Epoch  33   1230.4364013671875\n",
      "loss at Epoch  34   1230.404541015625\n",
      "loss at Epoch  35   1230.345703125\n",
      "loss at Epoch  36   1230.30859375\n",
      "loss at Epoch  37   1230.299560546875\n",
      "loss at Epoch  38   1230.26904296875\n",
      "loss at Epoch  39   1230.25439453125\n",
      "loss at Epoch  0   1387.580322265625\n",
      "loss at Epoch  1   1387.84228515625\n",
      "loss at Epoch  2   1388.227783203125\n",
      "loss at Epoch  3   1388.6240234375\n",
      "loss at Epoch  4   1388.99951171875\n",
      "loss at Epoch  5   1389.3316650390625\n",
      "loss at Epoch  6   1389.637939453125\n",
      "loss at Epoch  7   1389.916259765625\n",
      "loss at Epoch  8   1390.1612548828125\n",
      "loss at Epoch  9   1390.38232421875\n",
      "loss at Epoch  10   1390.5743408203125\n",
      "loss at Epoch  11   1390.7369384765625\n",
      "loss at Epoch  12   1390.88818359375\n",
      "loss at Epoch  13   1391.019775390625\n",
      "loss at Epoch  14   1391.13916015625\n",
      "loss at Epoch  15   1391.2391357421875\n",
      "loss at Epoch  16   1391.3394775390625\n",
      "loss at Epoch  17   1391.424072265625\n",
      "loss at Epoch  18   1391.4984130859375\n",
      "loss at Epoch  19   1391.56005859375\n",
      "loss at Epoch  20   1391.62109375\n",
      "loss at Epoch  21   1391.6712646484375\n",
      "loss at Epoch  22   1391.718994140625\n",
      "loss at Epoch  23   1391.7626953125\n",
      "loss at Epoch  24   1391.80078125\n",
      "loss at Epoch  25   1391.83349609375\n",
      "loss at Epoch  26   1391.862548828125\n",
      "loss at Epoch  27   1391.889404296875\n",
      "loss at Epoch  28   1391.9132080078125\n",
      "loss at Epoch  29   1391.9345703125\n",
      "loss at Epoch  30   1391.954833984375\n",
      "loss at Epoch  31   1391.9716796875\n",
      "loss at Epoch  32   1391.988037109375\n",
      "loss at Epoch  33   1392.001953125\n",
      "loss at Epoch  34   1392.013916015625\n",
      "loss at Epoch  35   1392.026123046875\n",
      "loss at Epoch  36   1392.035400390625\n",
      "loss at Epoch  37   1392.044921875\n",
      "loss at Epoch  38   1392.052734375\n",
      "loss at Epoch  39   1392.0599365234375\n",
      "loss at Epoch  0   1352.232666015625\n",
      "loss at Epoch  1   1348.6568603515625\n",
      "loss at Epoch  2   1345.28076171875\n",
      "loss at Epoch  3   1343.7626953125\n",
      "loss at Epoch  4   1343.281982421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  5   1342.682861328125\n",
      "loss at Epoch  6   1341.9210205078125\n",
      "loss at Epoch  7   1341.800048828125\n",
      "loss at Epoch  8   1341.875244140625\n",
      "loss at Epoch  9   1341.440673828125\n",
      "loss at Epoch  10   1341.4962158203125\n",
      "loss at Epoch  11   1341.1614990234375\n",
      "loss at Epoch  12   1341.20703125\n",
      "loss at Epoch  13   1341.1004638671875\n",
      "loss at Epoch  14   1340.92431640625\n",
      "loss at Epoch  15   1340.83447265625\n",
      "loss at Epoch  16   1340.7724609375\n",
      "loss at Epoch  17   1340.69677734375\n",
      "loss at Epoch  18   1340.63427734375\n",
      "loss at Epoch  19   1340.540771484375\n",
      "loss at Epoch  20   1340.541259765625\n",
      "loss at Epoch  21   1340.430908203125\n",
      "loss at Epoch  22   1340.4591064453125\n",
      "loss at Epoch  23   1340.3974609375\n",
      "loss at Epoch  24   1340.4072265625\n",
      "loss at Epoch  25   1340.35107421875\n",
      "loss at Epoch  26   1340.3802490234375\n",
      "loss at Epoch  27   1340.3360595703125\n",
      "loss at Epoch  28   1340.347412109375\n",
      "loss at Epoch  29   1340.303466796875\n",
      "loss at Epoch  30   1340.286865234375\n",
      "loss at Epoch  31   1340.274169921875\n",
      "loss at Epoch  32   1340.2890625\n",
      "loss at Epoch  33   1340.2423095703125\n",
      "loss at Epoch  34   1340.2308349609375\n",
      "loss at Epoch  35   1340.224365234375\n",
      "loss at Epoch  36   1340.2177734375\n",
      "loss at Epoch  37   1340.1978759765625\n",
      "loss at Epoch  38   1340.1968994140625\n",
      "loss at Epoch  39   1340.191650390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1360.586669921875\n",
      "loss at Epoch  1   1356.357421875\n",
      "loss at Epoch  2   1354.695068359375\n",
      "loss at Epoch  3   1352.376953125\n",
      "loss at Epoch  4   1350.385009765625\n",
      "loss at Epoch  5   1348.974365234375\n",
      "loss at Epoch  6   1347.88623046875\n",
      "loss at Epoch  7   1346.917236328125\n",
      "loss at Epoch  8   1345.9443359375\n",
      "loss at Epoch  9   1345.5382080078125\n",
      "loss at Epoch  10   1344.6011962890625\n",
      "loss at Epoch  11   1344.269775390625\n",
      "loss at Epoch  12   1343.7720947265625\n",
      "loss at Epoch  13   1343.625\n",
      "loss at Epoch  14   1343.138427734375\n",
      "loss at Epoch  15   1342.6685791015625\n",
      "loss at Epoch  16   1342.536376953125\n",
      "loss at Epoch  17   1342.3095703125\n",
      "loss at Epoch  18   1342.17578125\n",
      "loss at Epoch  19   1341.9766845703125\n",
      "loss at Epoch  20   1341.751708984375\n",
      "loss at Epoch  21   1341.5294189453125\n",
      "loss at Epoch  22   1341.39111328125\n",
      "loss at Epoch  23   1341.406982421875\n",
      "loss at Epoch  24   1341.2158203125\n",
      "loss at Epoch  25   1341.1153564453125\n",
      "loss at Epoch  26   1341.10986328125\n",
      "loss at Epoch  27   1340.983642578125\n",
      "loss at Epoch  28   1340.941162109375\n",
      "loss at Epoch  29   1340.8564453125\n",
      "loss at Epoch  30   1340.80810546875\n",
      "loss at Epoch  31   1340.7698974609375\n",
      "loss at Epoch  32   1340.7279052734375\n",
      "loss at Epoch  33   1340.71337890625\n",
      "loss at Epoch  34   1340.66015625\n",
      "loss at Epoch  35   1340.626220703125\n",
      "loss at Epoch  36   1340.60546875\n",
      "loss at Epoch  37   1340.591064453125\n",
      "loss at Epoch  38   1340.5513916015625\n",
      "loss at Epoch  39   1340.552490234375\n",
      "loss at Epoch  0   1162.563232421875\n",
      "loss at Epoch  1   1160.4451904296875\n",
      "loss at Epoch  2   1159.435791015625\n",
      "loss at Epoch  3   1158.6654052734375\n",
      "loss at Epoch  4   1158.085693359375\n",
      "loss at Epoch  5   1156.201416015625\n",
      "loss at Epoch  6   1155.992431640625\n",
      "loss at Epoch  7   1155.388916015625\n",
      "loss at Epoch  8   1155.434814453125\n",
      "loss at Epoch  9   1154.8231201171875\n",
      "loss at Epoch  10   1154.69580078125\n",
      "loss at Epoch  11   1154.418701171875\n",
      "loss at Epoch  12   1154.156982421875\n",
      "loss at Epoch  13   1153.852783203125\n",
      "loss at Epoch  14   1153.8837890625\n",
      "loss at Epoch  15   1153.784423828125\n",
      "loss at Epoch  16   1153.6910400390625\n",
      "loss at Epoch  17   1153.476318359375\n",
      "loss at Epoch  18   1153.610107421875\n",
      "loss at Epoch  19   1153.435302734375\n",
      "loss at Epoch  20   1153.243896484375\n",
      "loss at Epoch  21   1153.113525390625\n",
      "loss at Epoch  22   1153.0501708984375\n",
      "loss at Epoch  23   1152.951416015625\n",
      "loss at Epoch  24   1152.962646484375\n",
      "loss at Epoch  25   1152.9974365234375\n",
      "loss at Epoch  26   1152.8721923828125\n",
      "loss at Epoch  27   1152.8160400390625\n",
      "loss at Epoch  28   1152.76220703125\n",
      "loss at Epoch  29   1152.7509765625\n",
      "loss at Epoch  30   1152.69677734375\n",
      "loss at Epoch  31   1152.694580078125\n",
      "loss at Epoch  32   1152.66943359375\n",
      "loss at Epoch  33   1152.652587890625\n",
      "loss at Epoch  34   1152.652587890625\n",
      "loss at Epoch  35   1152.630615234375\n",
      "loss at Epoch  36   1152.6162109375\n",
      "loss at Epoch  37   1152.606201171875\n",
      "loss at Epoch  38   1152.5958251953125\n",
      "loss at Epoch  39   1152.57958984375\n",
      "loss at Epoch  0   2028.257080078125\n",
      "loss at Epoch  1   2027.453857421875\n",
      "loss at Epoch  2   2024.086669921875\n",
      "loss at Epoch  3   2026.178466796875\n",
      "loss at Epoch  4   2025.0009765625\n",
      "loss at Epoch  5   2025.645751953125\n",
      "loss at Epoch  6   2024.7064208984375\n",
      "loss at Epoch  7   2024.735107421875\n",
      "loss at Epoch  8   2024.00732421875\n",
      "loss at Epoch  9   2023.8253173828125\n",
      "loss at Epoch  10   2023.5966796875\n",
      "loss at Epoch  11   2024.138916015625\n",
      "loss at Epoch  12   2024.2169189453125\n",
      "loss at Epoch  13   2023.7607421875\n",
      "loss at Epoch  14   2023.78759765625\n",
      "loss at Epoch  15   2023.65478515625\n",
      "loss at Epoch  16   2023.6607666015625\n",
      "loss at Epoch  17   2023.569091796875\n",
      "loss at Epoch  18   2023.4833984375\n",
      "loss at Epoch  19   2023.570068359375\n",
      "loss at Epoch  20   2023.49755859375\n",
      "loss at Epoch  21   2023.5928955078125\n",
      "loss at Epoch  22   2023.385498046875\n",
      "loss at Epoch  23   2023.3663330078125\n",
      "loss at Epoch  24   2023.43896484375\n",
      "loss at Epoch  25   2023.405029296875\n",
      "loss at Epoch  26   2023.43017578125\n",
      "loss at Epoch  27   2023.37744140625\n",
      "loss at Epoch  28   2023.392333984375\n",
      "loss at Epoch  29   2023.33837890625\n",
      "loss at Epoch  30   2023.3577880859375\n",
      "loss at Epoch  31   2023.350830078125\n",
      "loss at Epoch  32   2023.338623046875\n",
      "loss at Epoch  33   2023.347900390625\n",
      "loss at Epoch  34   2023.3458251953125\n",
      "loss at Epoch  35   2023.358154296875\n",
      "loss at Epoch  36   2023.3297119140625\n",
      "loss at Epoch  37   2023.3507080078125\n",
      "loss at Epoch  38   2023.3533935546875\n",
      "loss at Epoch  39   2023.32958984375\n",
      "loss at Epoch  0   1228.122314453125\n",
      "loss at Epoch  1   1226.7584228515625\n",
      "loss at Epoch  2   1227.05712890625\n",
      "loss at Epoch  3   1228.566162109375\n",
      "loss at Epoch  4   1227.188720703125\n",
      "loss at Epoch  5   1225.3385009765625\n",
      "loss at Epoch  6   1225.5380859375\n",
      "loss at Epoch  7   1224.84228515625\n",
      "loss at Epoch  8   1225.19970703125\n",
      "loss at Epoch  9   1225.224853515625\n",
      "loss at Epoch  10   1225.3304443359375\n",
      "loss at Epoch  11   1225.3564453125\n",
      "loss at Epoch  12   1225.4722900390625\n",
      "loss at Epoch  13   1225.33984375\n",
      "loss at Epoch  14   1225.140625\n",
      "loss at Epoch  15   1225.25341796875\n",
      "loss at Epoch  16   1225.19921875\n",
      "loss at Epoch  17   1225.0648193359375\n",
      "loss at Epoch  18   1224.974365234375\n",
      "loss at Epoch  19   1225.065185546875\n",
      "loss at Epoch  20   1225.14697265625\n",
      "loss at Epoch  21   1225.132080078125\n",
      "loss at Epoch  22   1225.18408203125\n",
      "loss at Epoch  23   1225.11181640625\n",
      "loss at Epoch  24   1225.1802978515625\n",
      "loss at Epoch  25   1225.168701171875\n",
      "loss at Epoch  26   1225.148681640625\n",
      "loss at Epoch  27   1225.133544921875\n",
      "loss at Epoch  28   1225.140625\n",
      "loss at Epoch  29   1225.154052734375\n",
      "loss at Epoch  30   1225.142578125\n",
      "loss at Epoch  31   1225.10693359375\n",
      "loss at Epoch  32   1225.147705078125\n",
      "loss at Epoch  33   1225.196044921875\n",
      "loss at Epoch  34   1225.160888671875\n",
      "loss at Epoch  35   1225.149169921875\n",
      "loss at Epoch  36   1225.12646484375\n",
      "loss at Epoch  37   1225.139892578125\n",
      "loss at Epoch  38   1225.1456298828125\n",
      "loss at Epoch  39   1225.140380859375\n",
      "loss at Epoch  0   1376.771728515625\n",
      "loss at Epoch  1   1376.05859375\n",
      "loss at Epoch  2   1375.25537109375\n",
      "loss at Epoch  3   1374.84375\n",
      "loss at Epoch  4   1375.04345703125\n",
      "loss at Epoch  5   1375.12109375\n",
      "loss at Epoch  6   1374.9404296875\n",
      "loss at Epoch  7   1374.564453125\n",
      "loss at Epoch  8   1374.6943359375\n",
      "loss at Epoch  9   1374.73974609375\n",
      "loss at Epoch  10   1374.58203125\n",
      "loss at Epoch  11   1374.6993408203125\n",
      "loss at Epoch  12   1374.799072265625\n",
      "loss at Epoch  13   1374.75830078125\n",
      "loss at Epoch  14   1374.700927734375\n",
      "loss at Epoch  15   1374.7176513671875\n",
      "loss at Epoch  16   1374.9002685546875\n",
      "loss at Epoch  17   1374.88330078125\n",
      "loss at Epoch  18   1374.805908203125\n",
      "loss at Epoch  19   1374.7244873046875\n",
      "loss at Epoch  20   1374.83056640625\n",
      "loss at Epoch  21   1374.9561767578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  22   1374.9580078125\n",
      "loss at Epoch  23   1374.891357421875\n",
      "loss at Epoch  24   1374.8909912109375\n",
      "loss at Epoch  25   1374.862060546875\n",
      "loss at Epoch  26   1374.89892578125\n",
      "loss at Epoch  27   1374.896728515625\n",
      "loss at Epoch  28   1374.89599609375\n",
      "loss at Epoch  29   1374.889404296875\n",
      "loss at Epoch  30   1374.89990234375\n",
      "loss at Epoch  31   1374.8919677734375\n",
      "loss at Epoch  32   1374.92333984375\n",
      "loss at Epoch  33   1374.916015625\n",
      "loss at Epoch  34   1374.9283447265625\n",
      "loss at Epoch  35   1374.9345703125\n",
      "loss at Epoch  36   1374.9315185546875\n",
      "loss at Epoch  37   1374.929931640625\n",
      "loss at Epoch  38   1374.931884765625\n",
      "loss at Epoch  39   1374.9302978515625\n",
      "loss at Epoch  0   1378.8218994140625\n",
      "loss at Epoch  1   1377.76220703125\n",
      "loss at Epoch  2   1377.685546875\n",
      "loss at Epoch  3   1377.1903076171875\n",
      "loss at Epoch  4   1377.5152587890625\n",
      "loss at Epoch  5   1376.528076171875\n",
      "loss at Epoch  6   1377.19091796875\n",
      "loss at Epoch  7   1376.811767578125\n",
      "loss at Epoch  8   1376.9052734375\n",
      "loss at Epoch  9   1377.3121337890625\n",
      "loss at Epoch  10   1377.052734375\n",
      "loss at Epoch  11   1377.35400390625\n",
      "loss at Epoch  12   1377.0443115234375\n",
      "loss at Epoch  13   1377.404296875\n",
      "loss at Epoch  14   1377.334228515625\n",
      "loss at Epoch  15   1377.1929931640625\n",
      "loss at Epoch  16   1377.3369140625\n",
      "loss at Epoch  17   1377.325439453125\n",
      "loss at Epoch  18   1377.35498046875\n",
      "loss at Epoch  19   1377.5440673828125\n",
      "loss at Epoch  20   1377.492431640625\n",
      "loss at Epoch  21   1377.44873046875\n",
      "loss at Epoch  22   1377.4261474609375\n",
      "loss at Epoch  23   1377.4267578125\n",
      "loss at Epoch  24   1377.516845703125\n",
      "loss at Epoch  25   1377.458984375\n",
      "loss at Epoch  26   1377.495849609375\n",
      "loss at Epoch  27   1377.534912109375\n",
      "loss at Epoch  28   1377.4993896484375\n",
      "loss at Epoch  29   1377.5079345703125\n",
      "loss at Epoch  30   1377.5341796875\n",
      "loss at Epoch  31   1377.55908203125\n",
      "loss at Epoch  32   1377.543212890625\n",
      "loss at Epoch  33   1377.553955078125\n",
      "loss at Epoch  34   1377.5537109375\n",
      "loss at Epoch  35   1377.551025390625\n",
      "loss at Epoch  36   1377.5625\n",
      "loss at Epoch  37   1377.571533203125\n",
      "loss at Epoch  38   1377.57763671875\n",
      "loss at Epoch  39   1377.5797119140625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.526     0.650       966\n",
      "           1      0.118     0.409     0.183       149\n",
      "\n",
      "    accuracy                          0.510      1115\n",
      "   macro avg      0.485     0.468     0.417      1115\n",
      "weighted avg      0.754     0.510     0.588      1115\n",
      "\n",
      "loss at Epoch  0   1262.265380859375\n",
      "loss at Epoch  1   1263.62646484375\n",
      "loss at Epoch  2   1263.101318359375\n",
      "loss at Epoch  3   1263.173828125\n",
      "loss at Epoch  4   1263.731689453125\n",
      "loss at Epoch  5   1263.202392578125\n",
      "loss at Epoch  6   1262.75439453125\n",
      "loss at Epoch  7   1263.296142578125\n",
      "loss at Epoch  8   1263.239013671875\n",
      "loss at Epoch  9   1263.6328125\n",
      "loss at Epoch  10   1263.7305908203125\n",
      "loss at Epoch  11   1263.5699462890625\n",
      "loss at Epoch  12   1263.75927734375\n",
      "loss at Epoch  13   1263.63916015625\n",
      "loss at Epoch  14   1263.7177734375\n",
      "loss at Epoch  15   1263.7529296875\n",
      "loss at Epoch  16   1263.8154296875\n",
      "loss at Epoch  17   1263.928466796875\n",
      "loss at Epoch  18   1263.8436279296875\n",
      "loss at Epoch  19   1263.880126953125\n",
      "loss at Epoch  20   1263.8873291015625\n",
      "loss at Epoch  21   1263.85400390625\n",
      "loss at Epoch  22   1263.9376220703125\n",
      "loss at Epoch  23   1263.913330078125\n",
      "loss at Epoch  24   1263.9537353515625\n",
      "loss at Epoch  25   1263.94677734375\n",
      "loss at Epoch  26   1263.9432373046875\n",
      "loss at Epoch  27   1263.951416015625\n",
      "loss at Epoch  28   1263.994384765625\n",
      "loss at Epoch  29   1263.95263671875\n",
      "loss at Epoch  30   1264.012939453125\n",
      "loss at Epoch  31   1264.0167236328125\n",
      "loss at Epoch  32   1264.010009765625\n",
      "loss at Epoch  33   1264.01708984375\n",
      "loss at Epoch  34   1264.03173828125\n",
      "loss at Epoch  35   1264.018798828125\n",
      "loss at Epoch  36   1264.0196533203125\n",
      "loss at Epoch  37   1264.03466796875\n",
      "loss at Epoch  38   1264.0435791015625\n",
      "loss at Epoch  39   1264.0428466796875\n",
      "loss at Epoch  0   1218.326171875\n",
      "loss at Epoch  1   1217.595947265625\n",
      "loss at Epoch  2   1214.887451171875\n",
      "loss at Epoch  3   1214.65185546875\n",
      "loss at Epoch  4   1214.713623046875\n",
      "loss at Epoch  5   1214.8995361328125\n",
      "loss at Epoch  6   1214.78173828125\n",
      "loss at Epoch  7   1214.332763671875\n",
      "loss at Epoch  8   1214.854248046875\n",
      "loss at Epoch  9   1214.81396484375\n",
      "loss at Epoch  10   1214.62841796875\n",
      "loss at Epoch  11   1214.500732421875\n",
      "loss at Epoch  12   1214.8486328125\n",
      "loss at Epoch  13   1214.711669921875\n",
      "loss at Epoch  14   1214.745849609375\n",
      "loss at Epoch  15   1214.7353515625\n",
      "loss at Epoch  16   1214.7174072265625\n",
      "loss at Epoch  17   1214.650634765625\n",
      "loss at Epoch  18   1214.7490234375\n",
      "loss at Epoch  19   1214.747314453125\n",
      "loss at Epoch  20   1214.7684326171875\n",
      "loss at Epoch  21   1214.7286376953125\n",
      "loss at Epoch  22   1214.701171875\n",
      "loss at Epoch  23   1214.771484375\n",
      "loss at Epoch  24   1214.7501220703125\n",
      "loss at Epoch  25   1214.705078125\n",
      "loss at Epoch  26   1214.761474609375\n",
      "loss at Epoch  27   1214.7747802734375\n",
      "loss at Epoch  28   1214.7701416015625\n",
      "loss at Epoch  29   1214.772705078125\n",
      "loss at Epoch  30   1214.7908935546875\n",
      "loss at Epoch  31   1214.79833984375\n",
      "loss at Epoch  32   1214.793701171875\n",
      "loss at Epoch  33   1214.800048828125\n",
      "loss at Epoch  34   1214.7884521484375\n",
      "loss at Epoch  35   1214.79296875\n",
      "loss at Epoch  36   1214.8048095703125\n",
      "loss at Epoch  37   1214.798095703125\n",
      "loss at Epoch  38   1214.8048095703125\n",
      "loss at Epoch  39   1214.8076171875\n",
      "loss at Epoch  0   1531.3861083984375\n",
      "loss at Epoch  1   1531.5277099609375\n",
      "loss at Epoch  2   1532.00927734375\n",
      "loss at Epoch  3   1529.141845703125\n",
      "loss at Epoch  4   1530.256591796875\n",
      "loss at Epoch  5   1529.034912109375\n",
      "loss at Epoch  6   1529.43798828125\n",
      "loss at Epoch  7   1529.9052734375\n",
      "loss at Epoch  8   1529.59228515625\n",
      "loss at Epoch  9   1529.61083984375\n",
      "loss at Epoch  10   1530.10400390625\n",
      "loss at Epoch  11   1529.974365234375\n",
      "loss at Epoch  12   1529.68603515625\n",
      "loss at Epoch  13   1530.010009765625\n",
      "loss at Epoch  14   1529.788818359375\n",
      "loss at Epoch  15   1529.85107421875\n",
      "loss at Epoch  16   1530.23486328125\n",
      "loss at Epoch  17   1530.1484375\n",
      "loss at Epoch  18   1530.0775146484375\n",
      "loss at Epoch  19   1530.106201171875\n",
      "loss at Epoch  20   1530.0\n",
      "loss at Epoch  21   1530.121337890625\n",
      "loss at Epoch  22   1530.144775390625\n",
      "loss at Epoch  23   1530.1663818359375\n",
      "loss at Epoch  24   1530.124755859375\n",
      "loss at Epoch  25   1530.1181640625\n",
      "loss at Epoch  26   1530.118896484375\n",
      "loss at Epoch  27   1530.206787109375\n",
      "loss at Epoch  28   1530.1812744140625\n",
      "loss at Epoch  29   1530.18115234375\n",
      "loss at Epoch  30   1530.15380859375\n",
      "loss at Epoch  31   1530.171142578125\n",
      "loss at Epoch  32   1530.1572265625\n",
      "loss at Epoch  33   1530.1739501953125\n",
      "loss at Epoch  34   1530.1689453125\n",
      "loss at Epoch  35   1530.195068359375\n",
      "loss at Epoch  36   1530.195068359375\n",
      "loss at Epoch  37   1530.1839599609375\n",
      "loss at Epoch  38   1530.1815185546875\n",
      "loss at Epoch  39   1530.211669921875\n",
      "loss at Epoch  0   1196.322021484375\n",
      "loss at Epoch  1   1195.918212890625\n",
      "loss at Epoch  2   1195.277587890625\n",
      "loss at Epoch  3   1194.6480712890625\n",
      "loss at Epoch  4   1194.26171875\n",
      "loss at Epoch  5   1193.6968994140625\n",
      "loss at Epoch  6   1194.53857421875\n",
      "loss at Epoch  7   1194.3204345703125\n",
      "loss at Epoch  8   1194.103271484375\n",
      "loss at Epoch  9   1194.474609375\n",
      "loss at Epoch  10   1194.635986328125\n",
      "loss at Epoch  11   1194.6365966796875\n",
      "loss at Epoch  12   1194.6080322265625\n",
      "loss at Epoch  13   1194.761474609375\n",
      "loss at Epoch  14   1194.6968994140625\n",
      "loss at Epoch  15   1194.716064453125\n",
      "loss at Epoch  16   1194.798583984375\n",
      "loss at Epoch  17   1194.7662353515625\n",
      "loss at Epoch  18   1194.797119140625\n",
      "loss at Epoch  19   1194.894287109375\n",
      "loss at Epoch  20   1194.923583984375\n",
      "loss at Epoch  21   1194.95263671875\n",
      "loss at Epoch  22   1194.9267578125\n",
      "loss at Epoch  23   1194.92529296875\n",
      "loss at Epoch  24   1194.9365234375\n",
      "loss at Epoch  25   1194.9547119140625\n",
      "loss at Epoch  26   1195.014404296875\n",
      "loss at Epoch  27   1195.008544921875\n",
      "loss at Epoch  28   1195.006103515625\n",
      "loss at Epoch  29   1195.01806640625\n",
      "loss at Epoch  30   1195.06201171875\n",
      "loss at Epoch  31   1195.04541015625\n",
      "loss at Epoch  32   1195.057373046875\n",
      "loss at Epoch  33   1195.0740966796875\n",
      "loss at Epoch  34   1195.077880859375\n",
      "loss at Epoch  35   1195.0728759765625\n",
      "loss at Epoch  36   1195.068603515625\n",
      "loss at Epoch  37   1195.0860595703125\n",
      "loss at Epoch  38   1195.077392578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  39   1195.094970703125\n",
      "loss at Epoch  0   1324.45361328125\n",
      "loss at Epoch  1   1324.326171875\n",
      "loss at Epoch  2   1321.9622802734375\n",
      "loss at Epoch  3   1322.855224609375\n",
      "loss at Epoch  4   1322.09814453125\n",
      "loss at Epoch  5   1322.15185546875\n",
      "loss at Epoch  6   1322.0806884765625\n",
      "loss at Epoch  7   1322.862060546875\n",
      "loss at Epoch  8   1322.436279296875\n",
      "loss at Epoch  9   1322.774658203125\n",
      "loss at Epoch  10   1322.583740234375\n",
      "loss at Epoch  11   1322.627197265625\n",
      "loss at Epoch  12   1322.819091796875\n",
      "loss at Epoch  13   1322.925537109375\n",
      "loss at Epoch  14   1322.974365234375\n",
      "loss at Epoch  15   1322.9691162109375\n",
      "loss at Epoch  16   1323.03662109375\n",
      "loss at Epoch  17   1323.093994140625\n",
      "loss at Epoch  18   1323.010498046875\n",
      "loss at Epoch  19   1323.133056640625\n",
      "loss at Epoch  20   1323.074462890625\n",
      "loss at Epoch  21   1323.113037109375\n",
      "loss at Epoch  22   1323.22216796875\n",
      "loss at Epoch  23   1323.1884765625\n",
      "loss at Epoch  24   1323.1943359375\n",
      "loss at Epoch  25   1323.19921875\n",
      "loss at Epoch  26   1323.265380859375\n",
      "loss at Epoch  27   1323.26953125\n",
      "loss at Epoch  28   1323.2646484375\n",
      "loss at Epoch  29   1323.260498046875\n",
      "loss at Epoch  30   1323.279052734375\n",
      "loss at Epoch  31   1323.282958984375\n",
      "loss at Epoch  32   1323.2816162109375\n",
      "loss at Epoch  33   1323.2987060546875\n",
      "loss at Epoch  34   1323.2999267578125\n",
      "loss at Epoch  35   1323.30810546875\n",
      "loss at Epoch  36   1323.338134765625\n",
      "loss at Epoch  37   1323.348876953125\n",
      "loss at Epoch  38   1323.340087890625\n",
      "loss at Epoch  39   1323.334228515625\n",
      "loss at Epoch  0   1386.1234130859375\n",
      "loss at Epoch  1   1386.4376220703125\n",
      "loss at Epoch  2   1386.810791015625\n",
      "loss at Epoch  3   1387.2041015625\n",
      "loss at Epoch  4   1387.556396484375\n",
      "loss at Epoch  5   1387.8984375\n",
      "loss at Epoch  6   1388.195068359375\n",
      "loss at Epoch  7   1388.4638671875\n",
      "loss at Epoch  8   1388.69677734375\n",
      "loss at Epoch  9   1388.901611328125\n",
      "loss at Epoch  10   1389.0921630859375\n",
      "loss at Epoch  11   1389.2490234375\n",
      "loss at Epoch  12   1389.3994140625\n",
      "loss at Epoch  13   1389.5225830078125\n",
      "loss at Epoch  14   1389.634521484375\n",
      "loss at Epoch  15   1389.73486328125\n",
      "loss at Epoch  16   1389.8250732421875\n",
      "loss at Epoch  17   1389.9049072265625\n",
      "loss at Epoch  18   1389.975830078125\n",
      "loss at Epoch  19   1390.040283203125\n",
      "loss at Epoch  20   1390.0972900390625\n",
      "loss at Epoch  21   1390.149169921875\n",
      "loss at Epoch  22   1390.19384765625\n",
      "loss at Epoch  23   1390.235107421875\n",
      "loss at Epoch  24   1390.2708740234375\n",
      "loss at Epoch  25   1390.3038330078125\n",
      "loss at Epoch  26   1390.3341064453125\n",
      "loss at Epoch  27   1390.359619140625\n",
      "loss at Epoch  28   1390.383544921875\n",
      "loss at Epoch  29   1390.404541015625\n",
      "loss at Epoch  30   1390.4248046875\n",
      "loss at Epoch  31   1390.441650390625\n",
      "loss at Epoch  32   1390.45703125\n",
      "loss at Epoch  33   1390.47119140625\n",
      "loss at Epoch  34   1390.4835205078125\n",
      "loss at Epoch  35   1390.49560546875\n",
      "loss at Epoch  36   1390.505126953125\n",
      "loss at Epoch  37   1390.5142822265625\n",
      "loss at Epoch  38   1390.522216796875\n",
      "loss at Epoch  39   1390.530029296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.526     0.650       966\n",
      "           1      0.118     0.409     0.183       149\n",
      "\n",
      "    accuracy                          0.510      1115\n",
      "   macro avg      0.485     0.468     0.417      1115\n",
      "weighted avg      0.754     0.510     0.588      1115\n",
      "\n",
      "loss at Epoch  0   1371.9327392578125\n",
      "loss at Epoch  1   1372.9443359375\n",
      "loss at Epoch  2   1372.9945068359375\n",
      "loss at Epoch  3   1373.051025390625\n",
      "loss at Epoch  4   1372.9552001953125\n",
      "loss at Epoch  5   1372.936279296875\n",
      "loss at Epoch  6   1373.142822265625\n",
      "loss at Epoch  7   1373.168212890625\n",
      "loss at Epoch  8   1373.4056396484375\n",
      "loss at Epoch  9   1373.472900390625\n",
      "loss at Epoch  10   1373.597900390625\n",
      "loss at Epoch  11   1373.541259765625\n",
      "loss at Epoch  12   1373.4822998046875\n",
      "loss at Epoch  13   1373.844970703125\n",
      "loss at Epoch  14   1373.916015625\n",
      "loss at Epoch  15   1374.0126953125\n",
      "loss at Epoch  16   1373.94384765625\n",
      "loss at Epoch  17   1374.020751953125\n",
      "loss at Epoch  18   1374.00390625\n",
      "loss at Epoch  19   1373.95849609375\n",
      "loss at Epoch  20   1374.00341796875\n",
      "loss at Epoch  21   1374.029052734375\n",
      "loss at Epoch  22   1374.052490234375\n",
      "loss at Epoch  23   1374.0947265625\n",
      "loss at Epoch  24   1374.0997314453125\n",
      "loss at Epoch  25   1374.139404296875\n",
      "loss at Epoch  26   1374.151123046875\n",
      "loss at Epoch  27   1374.16015625\n",
      "loss at Epoch  28   1374.15234375\n",
      "loss at Epoch  29   1374.169677734375\n",
      "loss at Epoch  30   1374.198974609375\n",
      "loss at Epoch  31   1374.197265625\n",
      "loss at Epoch  32   1374.195556640625\n",
      "loss at Epoch  33   1374.193115234375\n",
      "loss at Epoch  34   1374.215576171875\n",
      "loss at Epoch  35   1374.22119140625\n",
      "loss at Epoch  36   1374.235595703125\n",
      "loss at Epoch  37   1374.240234375\n",
      "loss at Epoch  38   1374.23388671875\n",
      "loss at Epoch  39   1374.243896484375\n",
      "loss at Epoch  0   1212.427734375\n",
      "loss at Epoch  1   1211.359130859375\n",
      "loss at Epoch  2   1210.117919921875\n",
      "loss at Epoch  3   1209.7401123046875\n",
      "loss at Epoch  4   1209.795654296875\n",
      "loss at Epoch  5   1210.476318359375\n",
      "loss at Epoch  6   1209.9881591796875\n",
      "loss at Epoch  7   1209.9375\n",
      "loss at Epoch  8   1209.9075927734375\n",
      "loss at Epoch  9   1209.830322265625\n",
      "loss at Epoch  10   1210.06201171875\n",
      "loss at Epoch  11   1210.124267578125\n",
      "loss at Epoch  12   1210.2957763671875\n",
      "loss at Epoch  13   1210.216796875\n",
      "loss at Epoch  14   1210.2318115234375\n",
      "loss at Epoch  15   1210.34619140625\n",
      "loss at Epoch  16   1210.4886474609375\n",
      "loss at Epoch  17   1210.4805908203125\n",
      "loss at Epoch  18   1210.491455078125\n",
      "loss at Epoch  19   1210.512451171875\n",
      "loss at Epoch  20   1210.43603515625\n",
      "loss at Epoch  21   1210.4715576171875\n",
      "loss at Epoch  22   1210.49169921875\n",
      "loss at Epoch  23   1210.541015625\n",
      "loss at Epoch  24   1210.57275390625\n",
      "loss at Epoch  25   1210.6474609375\n",
      "loss at Epoch  26   1210.6435546875\n",
      "loss at Epoch  27   1210.66650390625\n",
      "loss at Epoch  28   1210.65283203125\n",
      "loss at Epoch  29   1210.673828125\n",
      "loss at Epoch  30   1210.683837890625\n",
      "loss at Epoch  31   1210.68896484375\n",
      "loss at Epoch  32   1210.708251953125\n",
      "loss at Epoch  33   1210.72802734375\n",
      "loss at Epoch  34   1210.742919921875\n",
      "loss at Epoch  35   1210.7520751953125\n",
      "loss at Epoch  36   1210.7432861328125\n",
      "loss at Epoch  37   1210.7449951171875\n",
      "loss at Epoch  38   1210.74462890625\n",
      "loss at Epoch  39   1210.74658203125\n",
      "loss at Epoch  0   1490.2470703125\n",
      "loss at Epoch  1   1489.07763671875\n",
      "loss at Epoch  2   1488.53515625\n",
      "loss at Epoch  3   1489.03515625\n",
      "loss at Epoch  4   1489.738525390625\n",
      "loss at Epoch  5   1489.162353515625\n",
      "loss at Epoch  6   1488.775634765625\n",
      "loss at Epoch  7   1488.8939208984375\n",
      "loss at Epoch  8   1489.35302734375\n",
      "loss at Epoch  9   1488.9913330078125\n",
      "loss at Epoch  10   1489.049560546875\n",
      "loss at Epoch  11   1489.166015625\n",
      "loss at Epoch  12   1489.578369140625\n",
      "loss at Epoch  13   1489.53369140625\n",
      "loss at Epoch  14   1489.59033203125\n",
      "loss at Epoch  15   1489.7579345703125\n",
      "loss at Epoch  16   1489.87158203125\n",
      "loss at Epoch  17   1489.9061279296875\n",
      "loss at Epoch  18   1489.986083984375\n",
      "loss at Epoch  19   1490.04833984375\n",
      "loss at Epoch  20   1490.02294921875\n",
      "loss at Epoch  21   1490.000244140625\n",
      "loss at Epoch  22   1490.0615234375\n",
      "loss at Epoch  23   1490.100830078125\n",
      "loss at Epoch  24   1490.08251953125\n",
      "loss at Epoch  25   1490.09716796875\n",
      "loss at Epoch  26   1490.136474609375\n",
      "loss at Epoch  27   1490.221923828125\n",
      "loss at Epoch  28   1490.1922607421875\n",
      "loss at Epoch  29   1490.248046875\n",
      "loss at Epoch  30   1490.219482421875\n",
      "loss at Epoch  31   1490.258544921875\n",
      "loss at Epoch  32   1490.248291015625\n",
      "loss at Epoch  33   1490.2918701171875\n",
      "loss at Epoch  34   1490.312255859375\n",
      "loss at Epoch  35   1490.290771484375\n",
      "loss at Epoch  36   1490.3143310546875\n",
      "loss at Epoch  37   1490.324462890625\n",
      "loss at Epoch  38   1490.3310546875\n",
      "loss at Epoch  39   1490.319580078125\n",
      "loss at Epoch  0   1386.20166015625\n",
      "loss at Epoch  1   1386.51953125\n",
      "loss at Epoch  2   1386.8995361328125\n",
      "loss at Epoch  3   1387.2659912109375\n",
      "loss at Epoch  4   1387.6337890625\n",
      "loss at Epoch  5   1387.97265625\n",
      "loss at Epoch  6   1388.2548828125\n",
      "loss at Epoch  7   1388.51318359375\n",
      "loss at Epoch  8   1388.7379150390625\n",
      "loss at Epoch  9   1388.935791015625\n",
      "loss at Epoch  10   1389.1142578125\n",
      "loss at Epoch  11   1389.274169921875\n",
      "loss at Epoch  12   1389.412353515625\n",
      "loss at Epoch  13   1389.53125\n",
      "loss at Epoch  14   1389.64697265625\n",
      "loss at Epoch  15   1389.744140625\n",
      "loss at Epoch  16   1389.826416015625\n",
      "loss at Epoch  17   1389.9022216796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  18   1389.97412109375\n",
      "loss at Epoch  19   1390.035888671875\n",
      "loss at Epoch  20   1390.09033203125\n",
      "loss at Epoch  21   1390.1414794921875\n",
      "loss at Epoch  22   1390.18505859375\n",
      "loss at Epoch  23   1390.225341796875\n",
      "loss at Epoch  24   1390.261474609375\n",
      "loss at Epoch  25   1390.29345703125\n",
      "loss at Epoch  26   1390.3221435546875\n",
      "loss at Epoch  27   1390.3482666015625\n",
      "loss at Epoch  28   1390.37109375\n",
      "loss at Epoch  29   1390.392578125\n",
      "loss at Epoch  30   1390.41162109375\n",
      "loss at Epoch  31   1390.42822265625\n",
      "loss at Epoch  32   1390.443359375\n",
      "loss at Epoch  33   1390.4561767578125\n",
      "loss at Epoch  34   1390.468505859375\n",
      "loss at Epoch  35   1390.47998046875\n",
      "loss at Epoch  36   1390.4896240234375\n",
      "loss at Epoch  37   1390.49853515625\n",
      "loss at Epoch  38   1390.5064697265625\n",
      "loss at Epoch  39   1390.513427734375\n",
      "loss at Epoch  0   1125.4716796875\n",
      "loss at Epoch  1   1121.175537109375\n",
      "loss at Epoch  2   1120.14794921875\n",
      "loss at Epoch  3   1118.952880859375\n",
      "loss at Epoch  4   1120.3359375\n",
      "loss at Epoch  5   1119.73583984375\n",
      "loss at Epoch  6   1120.451171875\n",
      "loss at Epoch  7   1119.902587890625\n",
      "loss at Epoch  8   1120.660888671875\n",
      "loss at Epoch  9   1121.0694580078125\n",
      "loss at Epoch  10   1120.657470703125\n",
      "loss at Epoch  11   1120.83447265625\n",
      "loss at Epoch  12   1120.66748046875\n",
      "loss at Epoch  13   1120.818115234375\n",
      "loss at Epoch  14   1120.808837890625\n",
      "loss at Epoch  15   1120.860595703125\n",
      "loss at Epoch  16   1120.78564453125\n",
      "loss at Epoch  17   1120.8572998046875\n",
      "loss at Epoch  18   1120.84423828125\n",
      "loss at Epoch  19   1120.900634765625\n",
      "loss at Epoch  20   1120.96044921875\n",
      "loss at Epoch  21   1120.9539794921875\n",
      "loss at Epoch  22   1121.07080078125\n",
      "loss at Epoch  23   1121.0728759765625\n",
      "loss at Epoch  24   1121.06494140625\n",
      "loss at Epoch  25   1121.131591796875\n",
      "loss at Epoch  26   1121.1417236328125\n",
      "loss at Epoch  27   1121.2177734375\n",
      "loss at Epoch  28   1121.2071533203125\n",
      "loss at Epoch  29   1121.17919921875\n",
      "loss at Epoch  30   1121.1881103515625\n",
      "loss at Epoch  31   1121.20263671875\n",
      "loss at Epoch  32   1121.218017578125\n",
      "loss at Epoch  33   1121.22119140625\n",
      "loss at Epoch  34   1121.2396240234375\n",
      "loss at Epoch  35   1121.260498046875\n",
      "loss at Epoch  36   1121.26708984375\n",
      "loss at Epoch  37   1121.264892578125\n",
      "loss at Epoch  38   1121.268310546875\n",
      "loss at Epoch  39   1121.2763671875\n",
      "loss at Epoch  0   1221.98193359375\n",
      "loss at Epoch  1   1221.6368408203125\n",
      "loss at Epoch  2   1220.7032470703125\n",
      "loss at Epoch  3   1220.43994140625\n",
      "loss at Epoch  4   1220.554443359375\n",
      "loss at Epoch  5   1220.5411376953125\n",
      "loss at Epoch  6   1220.903076171875\n",
      "loss at Epoch  7   1220.6103515625\n",
      "loss at Epoch  8   1220.5269775390625\n",
      "loss at Epoch  9   1220.60546875\n",
      "loss at Epoch  10   1221.5\n",
      "loss at Epoch  11   1221.40576171875\n",
      "loss at Epoch  12   1221.728271484375\n",
      "loss at Epoch  13   1221.6083984375\n",
      "loss at Epoch  14   1221.637939453125\n",
      "loss at Epoch  15   1221.6375732421875\n",
      "loss at Epoch  16   1221.7418212890625\n",
      "loss at Epoch  17   1221.816650390625\n",
      "loss at Epoch  18   1221.728271484375\n",
      "loss at Epoch  19   1221.8739013671875\n",
      "loss at Epoch  20   1221.939208984375\n",
      "loss at Epoch  21   1221.9599609375\n",
      "loss at Epoch  22   1221.9278564453125\n",
      "loss at Epoch  23   1222.0162353515625\n",
      "loss at Epoch  24   1221.998291015625\n",
      "loss at Epoch  25   1222.0517578125\n",
      "loss at Epoch  26   1222.057373046875\n",
      "loss at Epoch  27   1222.0728759765625\n",
      "loss at Epoch  28   1222.10693359375\n",
      "loss at Epoch  29   1222.14111328125\n",
      "loss at Epoch  30   1222.153564453125\n",
      "loss at Epoch  31   1222.184814453125\n",
      "loss at Epoch  32   1222.195556640625\n",
      "loss at Epoch  33   1222.18603515625\n",
      "loss at Epoch  34   1222.206298828125\n",
      "loss at Epoch  35   1222.25244140625\n",
      "loss at Epoch  36   1222.241455078125\n",
      "loss at Epoch  37   1222.23583984375\n",
      "loss at Epoch  38   1222.260498046875\n",
      "loss at Epoch  39   1222.24462890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1343.623779296875\n",
      "loss at Epoch  1   1344.81787109375\n",
      "loss at Epoch  2   1344.845703125\n",
      "loss at Epoch  3   1344.47021484375\n",
      "loss at Epoch  4   1344.713134765625\n",
      "loss at Epoch  5   1344.91259765625\n",
      "loss at Epoch  6   1344.7088623046875\n",
      "loss at Epoch  7   1345.01220703125\n",
      "loss at Epoch  8   1345.0977783203125\n",
      "loss at Epoch  9   1345.3892822265625\n",
      "loss at Epoch  10   1345.3341064453125\n",
      "loss at Epoch  11   1345.502685546875\n",
      "loss at Epoch  12   1345.39306640625\n",
      "loss at Epoch  13   1345.6253662109375\n",
      "loss at Epoch  14   1345.7099609375\n",
      "loss at Epoch  15   1345.682373046875\n",
      "loss at Epoch  16   1345.807861328125\n",
      "loss at Epoch  17   1345.7891845703125\n",
      "loss at Epoch  18   1345.864501953125\n",
      "loss at Epoch  19   1345.91650390625\n",
      "loss at Epoch  20   1345.98095703125\n",
      "loss at Epoch  21   1346.049560546875\n",
      "loss at Epoch  22   1346.030029296875\n",
      "loss at Epoch  23   1346.0570068359375\n",
      "loss at Epoch  24   1346.064208984375\n",
      "loss at Epoch  25   1346.08935546875\n",
      "loss at Epoch  26   1346.100341796875\n",
      "loss at Epoch  27   1346.1343994140625\n",
      "loss at Epoch  28   1346.1287841796875\n",
      "loss at Epoch  29   1346.16455078125\n",
      "loss at Epoch  30   1346.165771484375\n",
      "loss at Epoch  31   1346.1766357421875\n",
      "loss at Epoch  32   1346.170166015625\n",
      "loss at Epoch  33   1346.2021484375\n",
      "loss at Epoch  34   1346.192138671875\n",
      "loss at Epoch  35   1346.198486328125\n",
      "loss at Epoch  36   1346.1978759765625\n",
      "loss at Epoch  37   1346.2255859375\n",
      "loss at Epoch  38   1346.2275390625\n",
      "loss at Epoch  39   1346.21240234375\n",
      "loss at Epoch  0   1308.94091796875\n",
      "loss at Epoch  1   1309.267333984375\n",
      "loss at Epoch  2   1306.0360107421875\n",
      "loss at Epoch  3   1307.6483154296875\n",
      "loss at Epoch  4   1306.9228515625\n",
      "loss at Epoch  5   1306.840576171875\n",
      "loss at Epoch  6   1307.53759765625\n",
      "loss at Epoch  7   1307.09716796875\n",
      "loss at Epoch  8   1306.837890625\n",
      "loss at Epoch  9   1307.45751953125\n",
      "loss at Epoch  10   1307.500244140625\n",
      "loss at Epoch  11   1307.30517578125\n",
      "loss at Epoch  12   1307.359375\n",
      "loss at Epoch  13   1307.400390625\n",
      "loss at Epoch  14   1307.399169921875\n",
      "loss at Epoch  15   1307.5703125\n",
      "loss at Epoch  16   1307.513916015625\n",
      "loss at Epoch  17   1307.54150390625\n",
      "loss at Epoch  18   1307.512451171875\n",
      "loss at Epoch  19   1307.667724609375\n",
      "loss at Epoch  20   1307.6573486328125\n",
      "loss at Epoch  21   1307.773193359375\n",
      "loss at Epoch  22   1307.7880859375\n",
      "loss at Epoch  23   1307.729736328125\n",
      "loss at Epoch  24   1307.767822265625\n",
      "loss at Epoch  25   1307.7388916015625\n",
      "loss at Epoch  26   1307.775146484375\n",
      "loss at Epoch  27   1307.805908203125\n",
      "loss at Epoch  28   1307.839111328125\n",
      "loss at Epoch  29   1307.858154296875\n",
      "loss at Epoch  30   1307.876220703125\n",
      "loss at Epoch  31   1307.866455078125\n",
      "loss at Epoch  32   1307.8997802734375\n",
      "loss at Epoch  33   1307.904296875\n",
      "loss at Epoch  34   1307.89697265625\n",
      "loss at Epoch  35   1307.900390625\n",
      "loss at Epoch  36   1307.906494140625\n",
      "loss at Epoch  37   1307.89794921875\n",
      "loss at Epoch  38   1307.8994140625\n",
      "loss at Epoch  39   1307.903564453125\n",
      "loss at Epoch  0   1241.224609375\n",
      "loss at Epoch  1   1238.5755615234375\n",
      "loss at Epoch  2   1240.0859375\n",
      "loss at Epoch  3   1239.5643310546875\n",
      "loss at Epoch  4   1238.4034423828125\n",
      "loss at Epoch  5   1239.46337890625\n",
      "loss at Epoch  6   1238.89013671875\n",
      "loss at Epoch  7   1239.3759765625\n",
      "loss at Epoch  8   1238.58935546875\n",
      "loss at Epoch  9   1238.66259765625\n",
      "loss at Epoch  10   1239.5986328125\n",
      "loss at Epoch  11   1239.360595703125\n",
      "loss at Epoch  12   1239.32763671875\n",
      "loss at Epoch  13   1239.166748046875\n",
      "loss at Epoch  14   1239.408203125\n",
      "loss at Epoch  15   1239.4547119140625\n",
      "loss at Epoch  16   1239.6561279296875\n",
      "loss at Epoch  17   1239.77490234375\n",
      "loss at Epoch  18   1239.812744140625\n",
      "loss at Epoch  19   1239.822998046875\n",
      "loss at Epoch  20   1239.7779541015625\n",
      "loss at Epoch  21   1239.80859375\n",
      "loss at Epoch  22   1239.803466796875\n",
      "loss at Epoch  23   1239.870361328125\n",
      "loss at Epoch  24   1239.849609375\n",
      "loss at Epoch  25   1239.86572265625\n",
      "loss at Epoch  26   1239.903076171875\n",
      "loss at Epoch  27   1239.89208984375\n",
      "loss at Epoch  28   1239.91748046875\n",
      "loss at Epoch  29   1239.927978515625\n",
      "loss at Epoch  30   1239.92529296875\n",
      "loss at Epoch  31   1239.931884765625\n",
      "loss at Epoch  32   1239.9560546875\n",
      "loss at Epoch  33   1239.96337890625\n",
      "loss at Epoch  34   1239.9881591796875\n",
      "loss at Epoch  35   1239.9927978515625\n",
      "loss at Epoch  36   1239.99267578125\n",
      "loss at Epoch  37   1239.99365234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  38   1239.998046875\n",
      "loss at Epoch  39   1240.012451171875\n",
      "loss at Epoch  0   1530.10107421875\n",
      "loss at Epoch  1   1530.424072265625\n",
      "loss at Epoch  2   1527.6334228515625\n",
      "loss at Epoch  3   1527.702880859375\n",
      "loss at Epoch  4   1528.17333984375\n",
      "loss at Epoch  5   1528.070556640625\n",
      "loss at Epoch  6   1528.88720703125\n",
      "loss at Epoch  7   1528.28076171875\n",
      "loss at Epoch  8   1528.5244140625\n",
      "loss at Epoch  9   1528.5760498046875\n",
      "loss at Epoch  10   1528.5343017578125\n",
      "loss at Epoch  11   1528.632080078125\n",
      "loss at Epoch  12   1528.4581298828125\n",
      "loss at Epoch  13   1528.60888671875\n",
      "loss at Epoch  14   1528.75537109375\n",
      "loss at Epoch  15   1528.6649169921875\n",
      "loss at Epoch  16   1528.955078125\n",
      "loss at Epoch  17   1528.89599609375\n",
      "loss at Epoch  18   1528.96337890625\n",
      "loss at Epoch  19   1529.052490234375\n",
      "loss at Epoch  20   1529.0682373046875\n",
      "loss at Epoch  21   1529.05908203125\n",
      "loss at Epoch  22   1529.11767578125\n",
      "loss at Epoch  23   1529.207275390625\n",
      "loss at Epoch  24   1529.1541748046875\n",
      "loss at Epoch  25   1529.1279296875\n",
      "loss at Epoch  26   1529.24609375\n",
      "loss at Epoch  27   1529.2640380859375\n",
      "loss at Epoch  28   1529.2305908203125\n",
      "loss at Epoch  29   1529.235595703125\n",
      "loss at Epoch  30   1529.23681640625\n",
      "loss at Epoch  31   1529.306884765625\n",
      "loss at Epoch  32   1529.300048828125\n",
      "loss at Epoch  33   1529.298095703125\n",
      "loss at Epoch  34   1529.298583984375\n",
      "loss at Epoch  35   1529.3154296875\n",
      "loss at Epoch  36   1529.321533203125\n",
      "loss at Epoch  37   1529.3299560546875\n",
      "loss at Epoch  38   1529.33642578125\n",
      "loss at Epoch  39   1529.338623046875\n",
      "loss at Epoch  0   1221.779541015625\n",
      "loss at Epoch  1   1221.101806640625\n",
      "loss at Epoch  2   1221.419677734375\n",
      "loss at Epoch  3   1221.182861328125\n",
      "loss at Epoch  4   1220.868408203125\n",
      "loss at Epoch  5   1220.4534912109375\n",
      "loss at Epoch  6   1220.3251953125\n",
      "loss at Epoch  7   1220.815185546875\n",
      "loss at Epoch  8   1221.30712890625\n",
      "loss at Epoch  9   1221.1689453125\n",
      "loss at Epoch  10   1221.376220703125\n",
      "loss at Epoch  11   1221.54052734375\n",
      "loss at Epoch  12   1221.3170166015625\n",
      "loss at Epoch  13   1221.304931640625\n",
      "loss at Epoch  14   1221.60400390625\n",
      "loss at Epoch  15   1221.564453125\n",
      "loss at Epoch  16   1221.632568359375\n",
      "loss at Epoch  17   1221.763671875\n",
      "loss at Epoch  18   1221.951416015625\n",
      "loss at Epoch  19   1221.83837890625\n",
      "loss at Epoch  20   1221.8729248046875\n",
      "loss at Epoch  21   1221.9189453125\n",
      "loss at Epoch  22   1221.987548828125\n",
      "loss at Epoch  23   1221.9912109375\n",
      "loss at Epoch  24   1221.97216796875\n",
      "loss at Epoch  25   1221.982666015625\n",
      "loss at Epoch  26   1221.9632568359375\n",
      "loss at Epoch  27   1222.0087890625\n",
      "loss at Epoch  28   1222.03955078125\n",
      "loss at Epoch  29   1222.048095703125\n",
      "loss at Epoch  30   1222.08154296875\n",
      "loss at Epoch  31   1222.104736328125\n",
      "loss at Epoch  32   1222.092529296875\n",
      "loss at Epoch  33   1222.10107421875\n",
      "loss at Epoch  34   1222.093994140625\n",
      "loss at Epoch  35   1222.0992431640625\n",
      "loss at Epoch  36   1222.1109619140625\n",
      "loss at Epoch  37   1222.1123046875\n",
      "loss at Epoch  38   1222.134521484375\n",
      "loss at Epoch  39   1222.131103515625\n",
      "loss at Epoch  0   1195.457275390625\n",
      "loss at Epoch  1   1194.094482421875\n",
      "loss at Epoch  2   1194.8203125\n",
      "loss at Epoch  3   1193.292724609375\n",
      "loss at Epoch  4   1193.181884765625\n",
      "loss at Epoch  5   1193.4864501953125\n",
      "loss at Epoch  6   1193.80908203125\n",
      "loss at Epoch  7   1193.6676025390625\n",
      "loss at Epoch  8   1193.763427734375\n",
      "loss at Epoch  9   1193.7066650390625\n",
      "loss at Epoch  10   1193.6905517578125\n",
      "loss at Epoch  11   1193.8944091796875\n",
      "loss at Epoch  12   1193.95166015625\n",
      "loss at Epoch  13   1194.2801513671875\n",
      "loss at Epoch  14   1194.1466064453125\n",
      "loss at Epoch  15   1194.197998046875\n",
      "loss at Epoch  16   1194.20361328125\n",
      "loss at Epoch  17   1194.3223876953125\n",
      "loss at Epoch  18   1194.3853759765625\n",
      "loss at Epoch  19   1194.365234375\n",
      "loss at Epoch  20   1194.3826904296875\n",
      "loss at Epoch  21   1194.413330078125\n",
      "loss at Epoch  22   1194.414306640625\n",
      "loss at Epoch  23   1194.453125\n",
      "loss at Epoch  24   1194.4765625\n",
      "loss at Epoch  25   1194.505859375\n",
      "loss at Epoch  26   1194.52294921875\n",
      "loss at Epoch  27   1194.5567626953125\n",
      "loss at Epoch  28   1194.5849609375\n",
      "loss at Epoch  29   1194.559326171875\n",
      "loss at Epoch  30   1194.5670166015625\n",
      "loss at Epoch  31   1194.578369140625\n",
      "loss at Epoch  32   1194.5958251953125\n",
      "loss at Epoch  33   1194.617919921875\n",
      "loss at Epoch  34   1194.630859375\n",
      "loss at Epoch  35   1194.6304931640625\n",
      "loss at Epoch  36   1194.639892578125\n",
      "loss at Epoch  37   1194.6287841796875\n",
      "loss at Epoch  38   1194.63427734375\n",
      "loss at Epoch  39   1194.639404296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1212.4337158203125\n",
      "loss at Epoch  1   1213.45849609375\n",
      "loss at Epoch  2   1213.5789794921875\n",
      "loss at Epoch  3   1213.6812744140625\n",
      "loss at Epoch  4   1213.704833984375\n",
      "loss at Epoch  5   1214.1231689453125\n",
      "loss at Epoch  6   1214.825439453125\n",
      "loss at Epoch  7   1214.8125\n",
      "loss at Epoch  8   1215.4541015625\n",
      "loss at Epoch  9   1215.411865234375\n",
      "loss at Epoch  10   1215.2158203125\n",
      "loss at Epoch  11   1215.494140625\n",
      "loss at Epoch  12   1215.5040283203125\n",
      "loss at Epoch  13   1215.705810546875\n",
      "loss at Epoch  14   1215.8160400390625\n",
      "loss at Epoch  15   1215.766845703125\n",
      "loss at Epoch  16   1215.9041748046875\n",
      "loss at Epoch  17   1215.8485107421875\n",
      "loss at Epoch  18   1216.07373046875\n",
      "loss at Epoch  19   1216.1058349609375\n",
      "loss at Epoch  20   1216.11865234375\n",
      "loss at Epoch  21   1216.10888671875\n",
      "loss at Epoch  22   1216.164794921875\n",
      "loss at Epoch  23   1216.2117919921875\n",
      "loss at Epoch  24   1216.294921875\n",
      "loss at Epoch  25   1216.310302734375\n",
      "loss at Epoch  26   1216.331787109375\n",
      "loss at Epoch  27   1216.3387451171875\n",
      "loss at Epoch  28   1216.368408203125\n",
      "loss at Epoch  29   1216.3798828125\n",
      "loss at Epoch  30   1216.4189453125\n",
      "loss at Epoch  31   1216.4208984375\n",
      "loss at Epoch  32   1216.42578125\n",
      "loss at Epoch  33   1216.435791015625\n",
      "loss at Epoch  34   1216.4395751953125\n",
      "loss at Epoch  35   1216.4586181640625\n",
      "loss at Epoch  36   1216.4525146484375\n",
      "loss at Epoch  37   1216.46923828125\n",
      "loss at Epoch  38   1216.4736328125\n",
      "loss at Epoch  39   1216.47705078125\n",
      "loss at Epoch  0   1374.23095703125\n",
      "loss at Epoch  1   1373.0709228515625\n",
      "loss at Epoch  2   1372.94970703125\n",
      "loss at Epoch  3   1372.73974609375\n",
      "loss at Epoch  4   1373.0018310546875\n",
      "loss at Epoch  5   1372.52734375\n",
      "loss at Epoch  6   1372.55712890625\n",
      "loss at Epoch  7   1372.8282470703125\n",
      "loss at Epoch  8   1372.9715576171875\n",
      "loss at Epoch  9   1373.251953125\n",
      "loss at Epoch  10   1373.0989990234375\n",
      "loss at Epoch  11   1373.01220703125\n",
      "loss at Epoch  12   1373.06201171875\n",
      "loss at Epoch  13   1373.1920166015625\n",
      "loss at Epoch  14   1373.3017578125\n",
      "loss at Epoch  15   1373.30908203125\n",
      "loss at Epoch  16   1373.391357421875\n",
      "loss at Epoch  17   1373.4195556640625\n",
      "loss at Epoch  18   1373.423583984375\n",
      "loss at Epoch  19   1373.52490234375\n",
      "loss at Epoch  20   1373.487060546875\n",
      "loss at Epoch  21   1373.5152587890625\n",
      "loss at Epoch  22   1373.553955078125\n",
      "loss at Epoch  23   1373.55029296875\n",
      "loss at Epoch  24   1373.61083984375\n",
      "loss at Epoch  25   1373.6278076171875\n",
      "loss at Epoch  26   1373.614501953125\n",
      "loss at Epoch  27   1373.662109375\n",
      "loss at Epoch  28   1373.651611328125\n",
      "loss at Epoch  29   1373.682373046875\n",
      "loss at Epoch  30   1373.699462890625\n",
      "loss at Epoch  31   1373.7161865234375\n",
      "loss at Epoch  32   1373.7138671875\n",
      "loss at Epoch  33   1373.7371826171875\n",
      "loss at Epoch  34   1373.737060546875\n",
      "loss at Epoch  35   1373.7476806640625\n",
      "loss at Epoch  36   1373.7738037109375\n",
      "loss at Epoch  37   1373.7557373046875\n",
      "loss at Epoch  38   1373.7672119140625\n",
      "loss at Epoch  39   1373.765380859375\n",
      "loss at Epoch  0   1324.2470703125\n",
      "loss at Epoch  1   1322.8929443359375\n",
      "loss at Epoch  2   1322.141845703125\n",
      "loss at Epoch  3   1324.695556640625\n",
      "loss at Epoch  4   1322.4544677734375\n",
      "loss at Epoch  5   1322.072021484375\n",
      "loss at Epoch  6   1322.360595703125\n",
      "loss at Epoch  7   1322.24267578125\n",
      "loss at Epoch  8   1322.780517578125\n",
      "loss at Epoch  9   1322.5423583984375\n",
      "loss at Epoch  10   1322.643310546875\n",
      "loss at Epoch  11   1323.132568359375\n",
      "loss at Epoch  12   1322.954345703125\n",
      "loss at Epoch  13   1323.1651611328125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  14   1323.0325927734375\n",
      "loss at Epoch  15   1323.0885009765625\n",
      "loss at Epoch  16   1323.0634765625\n",
      "loss at Epoch  17   1323.3468017578125\n",
      "loss at Epoch  18   1323.2900390625\n",
      "loss at Epoch  19   1323.3841552734375\n",
      "loss at Epoch  20   1323.31640625\n",
      "loss at Epoch  21   1323.3133544921875\n",
      "loss at Epoch  22   1323.330810546875\n",
      "loss at Epoch  23   1323.3446044921875\n",
      "loss at Epoch  24   1323.42333984375\n",
      "loss at Epoch  25   1323.4549560546875\n",
      "loss at Epoch  26   1323.4725341796875\n",
      "loss at Epoch  27   1323.494384765625\n",
      "loss at Epoch  28   1323.479248046875\n",
      "loss at Epoch  29   1323.499267578125\n",
      "loss at Epoch  30   1323.51904296875\n",
      "loss at Epoch  31   1323.525146484375\n",
      "loss at Epoch  32   1323.5701904296875\n",
      "loss at Epoch  33   1323.56005859375\n",
      "loss at Epoch  34   1323.556640625\n",
      "loss at Epoch  35   1323.557373046875\n",
      "loss at Epoch  36   1323.565673828125\n",
      "loss at Epoch  37   1323.5732421875\n",
      "loss at Epoch  38   1323.5791015625\n",
      "loss at Epoch  39   1323.5888671875\n",
      "loss at Epoch  0   1333.17626953125\n",
      "loss at Epoch  1   1331.4534912109375\n",
      "loss at Epoch  2   1333.1610107421875\n",
      "loss at Epoch  3   1331.6815185546875\n",
      "loss at Epoch  4   1331.279541015625\n",
      "loss at Epoch  5   1332.178955078125\n",
      "loss at Epoch  6   1331.89306640625\n",
      "loss at Epoch  7   1331.9417724609375\n",
      "loss at Epoch  8   1331.8779296875\n",
      "loss at Epoch  9   1332.60888671875\n",
      "loss at Epoch  10   1332.47900390625\n",
      "loss at Epoch  11   1332.4697265625\n",
      "loss at Epoch  12   1332.2994384765625\n",
      "loss at Epoch  13   1332.43603515625\n",
      "loss at Epoch  14   1332.62255859375\n",
      "loss at Epoch  15   1332.519775390625\n",
      "loss at Epoch  16   1332.606689453125\n",
      "loss at Epoch  17   1332.577880859375\n",
      "loss at Epoch  18   1332.6435546875\n",
      "loss at Epoch  19   1332.7525634765625\n",
      "loss at Epoch  20   1332.706787109375\n",
      "loss at Epoch  21   1332.72900390625\n",
      "loss at Epoch  22   1332.722900390625\n",
      "loss at Epoch  23   1332.7833251953125\n",
      "loss at Epoch  24   1332.78515625\n",
      "loss at Epoch  25   1332.7979736328125\n",
      "loss at Epoch  26   1332.831298828125\n",
      "loss at Epoch  27   1332.87353515625\n",
      "loss at Epoch  28   1332.868896484375\n",
      "loss at Epoch  29   1332.897705078125\n",
      "loss at Epoch  30   1332.905517578125\n",
      "loss at Epoch  31   1332.904296875\n",
      "loss at Epoch  32   1332.919677734375\n",
      "loss at Epoch  33   1332.919677734375\n",
      "loss at Epoch  34   1332.945068359375\n",
      "loss at Epoch  35   1332.95703125\n",
      "loss at Epoch  36   1332.955810546875\n",
      "loss at Epoch  37   1332.9632568359375\n",
      "loss at Epoch  38   1332.9686279296875\n",
      "loss at Epoch  39   1332.970703125\n",
      "loss at Epoch  0   1322.3759765625\n",
      "loss at Epoch  1   1320.2783203125\n",
      "loss at Epoch  2   1319.72119140625\n",
      "loss at Epoch  3   1320.43798828125\n",
      "loss at Epoch  4   1320.115966796875\n",
      "loss at Epoch  5   1320.579833984375\n",
      "loss at Epoch  6   1320.322021484375\n",
      "loss at Epoch  7   1320.5751953125\n",
      "loss at Epoch  8   1320.251708984375\n",
      "loss at Epoch  9   1320.311279296875\n",
      "loss at Epoch  10   1320.356201171875\n",
      "loss at Epoch  11   1320.491455078125\n",
      "loss at Epoch  12   1320.3524169921875\n",
      "loss at Epoch  13   1320.5501708984375\n",
      "loss at Epoch  14   1320.704345703125\n",
      "loss at Epoch  15   1320.525146484375\n",
      "loss at Epoch  16   1320.5537109375\n",
      "loss at Epoch  17   1320.701171875\n",
      "loss at Epoch  18   1320.696044921875\n",
      "loss at Epoch  19   1320.701171875\n",
      "loss at Epoch  20   1320.7890625\n",
      "loss at Epoch  21   1320.7816162109375\n",
      "loss at Epoch  22   1320.76513671875\n",
      "loss at Epoch  23   1320.872314453125\n",
      "loss at Epoch  24   1320.845947265625\n",
      "loss at Epoch  25   1320.859130859375\n",
      "loss at Epoch  26   1320.8748779296875\n",
      "loss at Epoch  27   1320.928955078125\n",
      "loss at Epoch  28   1320.88818359375\n",
      "loss at Epoch  29   1320.9349365234375\n",
      "loss at Epoch  30   1320.95263671875\n",
      "loss at Epoch  31   1320.9642333984375\n",
      "loss at Epoch  32   1320.95654296875\n",
      "loss at Epoch  33   1320.957763671875\n",
      "loss at Epoch  34   1320.9434814453125\n",
      "loss at Epoch  35   1320.962646484375\n",
      "loss at Epoch  36   1320.9765625\n",
      "loss at Epoch  37   1320.97021484375\n",
      "loss at Epoch  38   1320.974365234375\n",
      "loss at Epoch  39   1320.997314453125\n",
      "loss at Epoch  0   1222.0640869140625\n",
      "loss at Epoch  1   1221.6185302734375\n",
      "loss at Epoch  2   1220.973876953125\n",
      "loss at Epoch  3   1220.353515625\n",
      "loss at Epoch  4   1221.818115234375\n",
      "loss at Epoch  5   1220.9215087890625\n",
      "loss at Epoch  6   1221.141357421875\n",
      "loss at Epoch  7   1220.980224609375\n",
      "loss at Epoch  8   1220.93505859375\n",
      "loss at Epoch  9   1221.249267578125\n",
      "loss at Epoch  10   1221.433837890625\n",
      "loss at Epoch  11   1221.1806640625\n",
      "loss at Epoch  12   1221.2357177734375\n",
      "loss at Epoch  13   1221.232177734375\n",
      "loss at Epoch  14   1221.407470703125\n",
      "loss at Epoch  15   1221.4498291015625\n",
      "loss at Epoch  16   1221.522216796875\n",
      "loss at Epoch  17   1221.66259765625\n",
      "loss at Epoch  18   1221.619140625\n",
      "loss at Epoch  19   1221.6778564453125\n",
      "loss at Epoch  20   1221.782958984375\n",
      "loss at Epoch  21   1221.743408203125\n",
      "loss at Epoch  22   1221.78564453125\n",
      "loss at Epoch  23   1221.802734375\n",
      "loss at Epoch  24   1221.82177734375\n",
      "loss at Epoch  25   1221.8240966796875\n",
      "loss at Epoch  26   1221.91259765625\n",
      "loss at Epoch  27   1221.9132080078125\n",
      "loss at Epoch  28   1221.900146484375\n",
      "loss at Epoch  29   1221.935546875\n",
      "loss at Epoch  30   1221.941650390625\n",
      "loss at Epoch  31   1221.9571533203125\n",
      "loss at Epoch  32   1221.999755859375\n",
      "loss at Epoch  33   1221.998779296875\n",
      "loss at Epoch  34   1221.99169921875\n",
      "loss at Epoch  35   1222.0126953125\n",
      "loss at Epoch  36   1222.018310546875\n",
      "loss at Epoch  37   1222.01416015625\n",
      "loss at Epoch  38   1222.0284423828125\n",
      "loss at Epoch  39   1222.04736328125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1374.75634765625\n",
      "loss at Epoch  1   1375.015380859375\n",
      "loss at Epoch  2   1375.130859375\n",
      "loss at Epoch  3   1375.223876953125\n",
      "loss at Epoch  4   1376.103759765625\n",
      "loss at Epoch  5   1376.0594482421875\n",
      "loss at Epoch  6   1376.448486328125\n",
      "loss at Epoch  7   1376.68994140625\n",
      "loss at Epoch  8   1376.3153076171875\n",
      "loss at Epoch  9   1376.35498046875\n",
      "loss at Epoch  10   1376.7454833984375\n",
      "loss at Epoch  11   1377.15380859375\n",
      "loss at Epoch  12   1376.96630859375\n",
      "loss at Epoch  13   1377.081298828125\n",
      "loss at Epoch  14   1377.0953369140625\n",
      "loss at Epoch  15   1377.10107421875\n",
      "loss at Epoch  16   1377.279296875\n",
      "loss at Epoch  17   1377.3140869140625\n",
      "loss at Epoch  18   1377.40966796875\n",
      "loss at Epoch  19   1377.4173583984375\n",
      "loss at Epoch  20   1377.441162109375\n",
      "loss at Epoch  21   1377.482666015625\n",
      "loss at Epoch  22   1377.4482421875\n",
      "loss at Epoch  23   1377.5841064453125\n",
      "loss at Epoch  24   1377.611083984375\n",
      "loss at Epoch  25   1377.585693359375\n",
      "loss at Epoch  26   1377.59033203125\n",
      "loss at Epoch  27   1377.628173828125\n",
      "loss at Epoch  28   1377.63623046875\n",
      "loss at Epoch  29   1377.674560546875\n",
      "loss at Epoch  30   1377.66845703125\n",
      "loss at Epoch  31   1377.673095703125\n",
      "loss at Epoch  32   1377.672607421875\n",
      "loss at Epoch  33   1377.702392578125\n",
      "loss at Epoch  34   1377.71728515625\n",
      "loss at Epoch  35   1377.72021484375\n",
      "loss at Epoch  36   1377.716064453125\n",
      "loss at Epoch  37   1377.727294921875\n",
      "loss at Epoch  38   1377.729248046875\n",
      "loss at Epoch  39   1377.7371826171875\n",
      "loss at Epoch  0   1325.2850341796875\n",
      "loss at Epoch  1   1323.4801025390625\n",
      "loss at Epoch  2   1323.910400390625\n",
      "loss at Epoch  3   1323.99755859375\n",
      "loss at Epoch  4   1322.701904296875\n",
      "loss at Epoch  5   1322.975341796875\n",
      "loss at Epoch  6   1323.017333984375\n",
      "loss at Epoch  7   1323.2177734375\n",
      "loss at Epoch  8   1323.7060546875\n",
      "loss at Epoch  9   1323.758544921875\n",
      "loss at Epoch  10   1323.534423828125\n",
      "loss at Epoch  11   1323.909423828125\n",
      "loss at Epoch  12   1323.800537109375\n",
      "loss at Epoch  13   1323.75537109375\n",
      "loss at Epoch  14   1323.7451171875\n",
      "loss at Epoch  15   1323.860595703125\n",
      "loss at Epoch  16   1324.063232421875\n",
      "loss at Epoch  17   1324.211669921875\n",
      "loss at Epoch  18   1324.1112060546875\n",
      "loss at Epoch  19   1324.100341796875\n",
      "loss at Epoch  20   1324.125732421875\n",
      "loss at Epoch  21   1324.154541015625\n",
      "loss at Epoch  22   1324.234375\n",
      "loss at Epoch  23   1324.2203369140625\n",
      "loss at Epoch  24   1324.2314453125\n",
      "loss at Epoch  25   1324.230712890625\n",
      "loss at Epoch  26   1324.267822265625\n",
      "loss at Epoch  27   1324.30322265625\n",
      "loss at Epoch  28   1324.297119140625\n",
      "loss at Epoch  29   1324.35546875\n",
      "loss at Epoch  30   1324.362060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  31   1324.376708984375\n",
      "loss at Epoch  32   1324.382080078125\n",
      "loss at Epoch  33   1324.3685302734375\n",
      "loss at Epoch  34   1324.3817138671875\n",
      "loss at Epoch  35   1324.3978271484375\n",
      "loss at Epoch  36   1324.3951416015625\n",
      "loss at Epoch  37   1324.416748046875\n",
      "loss at Epoch  38   1324.426025390625\n",
      "loss at Epoch  39   1324.424560546875\n",
      "loss at Epoch  0   2018.9921875\n",
      "loss at Epoch  1   2017.4461669921875\n",
      "loss at Epoch  2   2016.4072265625\n",
      "loss at Epoch  3   2018.447021484375\n",
      "loss at Epoch  4   2018.0479736328125\n",
      "loss at Epoch  5   2016.0411376953125\n",
      "loss at Epoch  6   2017.129150390625\n",
      "loss at Epoch  7   2016.931640625\n",
      "loss at Epoch  8   2017.299560546875\n",
      "loss at Epoch  9   2017.47021484375\n",
      "loss at Epoch  10   2017.201171875\n",
      "loss at Epoch  11   2017.54833984375\n",
      "loss at Epoch  12   2017.866455078125\n",
      "loss at Epoch  13   2018.0216064453125\n",
      "loss at Epoch  14   2018.029052734375\n",
      "loss at Epoch  15   2018.227783203125\n",
      "loss at Epoch  16   2017.9190673828125\n",
      "loss at Epoch  17   2018.05810546875\n",
      "loss at Epoch  18   2018.1981201171875\n",
      "loss at Epoch  19   2018.442626953125\n",
      "loss at Epoch  20   2018.4156494140625\n",
      "loss at Epoch  21   2018.35986328125\n",
      "loss at Epoch  22   2018.3873291015625\n",
      "loss at Epoch  23   2018.37841796875\n",
      "loss at Epoch  24   2018.4176025390625\n",
      "loss at Epoch  25   2018.4266357421875\n",
      "loss at Epoch  26   2018.486328125\n",
      "loss at Epoch  27   2018.487060546875\n",
      "loss at Epoch  28   2018.53955078125\n",
      "loss at Epoch  29   2018.5301513671875\n",
      "loss at Epoch  30   2018.541015625\n",
      "loss at Epoch  31   2018.53955078125\n",
      "loss at Epoch  32   2018.5557861328125\n",
      "loss at Epoch  33   2018.5576171875\n",
      "loss at Epoch  34   2018.56884765625\n",
      "loss at Epoch  35   2018.5869140625\n",
      "loss at Epoch  36   2018.589599609375\n",
      "loss at Epoch  37   2018.5828857421875\n",
      "loss at Epoch  38   2018.606689453125\n",
      "loss at Epoch  39   2018.604248046875\n",
      "loss at Epoch  0   1296.1865234375\n",
      "loss at Epoch  1   1292.5916748046875\n",
      "loss at Epoch  2   1289.6019287109375\n",
      "loss at Epoch  3   1287.0584716796875\n",
      "loss at Epoch  4   1285.44091796875\n",
      "loss at Epoch  5   1283.978271484375\n",
      "loss at Epoch  6   1282.7078857421875\n",
      "loss at Epoch  7   1281.73828125\n",
      "loss at Epoch  8   1280.916259765625\n",
      "loss at Epoch  9   1280.209716796875\n",
      "loss at Epoch  10   1279.60693359375\n",
      "loss at Epoch  11   1279.1722412109375\n",
      "loss at Epoch  12   1278.6866455078125\n",
      "loss at Epoch  13   1278.363037109375\n",
      "loss at Epoch  14   1278.009521484375\n",
      "loss at Epoch  15   1277.743896484375\n",
      "loss at Epoch  16   1277.447021484375\n",
      "loss at Epoch  17   1277.2535400390625\n",
      "loss at Epoch  18   1276.972412109375\n",
      "loss at Epoch  19   1276.799560546875\n",
      "loss at Epoch  20   1276.6600341796875\n",
      "loss at Epoch  21   1276.5230712890625\n",
      "loss at Epoch  22   1276.4227294921875\n",
      "loss at Epoch  23   1276.309326171875\n",
      "loss at Epoch  24   1276.2139892578125\n",
      "loss at Epoch  25   1276.123291015625\n",
      "loss at Epoch  26   1276.0418701171875\n",
      "loss at Epoch  27   1275.988525390625\n",
      "loss at Epoch  28   1275.923583984375\n",
      "loss at Epoch  29   1275.8642578125\n",
      "loss at Epoch  30   1275.81494140625\n",
      "loss at Epoch  31   1275.774658203125\n",
      "loss at Epoch  32   1275.736328125\n",
      "loss at Epoch  33   1275.698974609375\n",
      "loss at Epoch  34   1275.666259765625\n",
      "loss at Epoch  35   1275.644775390625\n",
      "loss at Epoch  36   1275.61865234375\n",
      "loss at Epoch  37   1275.5899658203125\n",
      "loss at Epoch  38   1275.571044921875\n",
      "loss at Epoch  39   1275.5540771484375\n",
      "loss at Epoch  0   1215.0338134765625\n",
      "loss at Epoch  1   1212.36669921875\n",
      "loss at Epoch  2   1212.69970703125\n",
      "loss at Epoch  3   1212.420654296875\n",
      "loss at Epoch  4   1213.40478515625\n",
      "loss at Epoch  5   1212.66845703125\n",
      "loss at Epoch  6   1212.656494140625\n",
      "loss at Epoch  7   1213.4019775390625\n",
      "loss at Epoch  8   1213.172119140625\n",
      "loss at Epoch  9   1213.052734375\n",
      "loss at Epoch  10   1213.071044921875\n",
      "loss at Epoch  11   1213.0162353515625\n",
      "loss at Epoch  12   1213.0675048828125\n",
      "loss at Epoch  13   1213.233154296875\n",
      "loss at Epoch  14   1213.343017578125\n",
      "loss at Epoch  15   1213.3402099609375\n",
      "loss at Epoch  16   1213.529541015625\n",
      "loss at Epoch  17   1213.387451171875\n",
      "loss at Epoch  18   1213.45556640625\n",
      "loss at Epoch  19   1213.544677734375\n",
      "loss at Epoch  20   1213.6041259765625\n",
      "loss at Epoch  21   1213.689453125\n",
      "loss at Epoch  22   1213.7210693359375\n",
      "loss at Epoch  23   1213.710693359375\n",
      "loss at Epoch  24   1213.7264404296875\n",
      "loss at Epoch  25   1213.7763671875\n",
      "loss at Epoch  26   1213.8330078125\n",
      "loss at Epoch  27   1213.82958984375\n",
      "loss at Epoch  28   1213.853759765625\n",
      "loss at Epoch  29   1213.873779296875\n",
      "loss at Epoch  30   1213.8929443359375\n",
      "loss at Epoch  31   1213.8818359375\n",
      "loss at Epoch  32   1213.8984375\n",
      "loss at Epoch  33   1213.900146484375\n",
      "loss at Epoch  34   1213.90283203125\n",
      "loss at Epoch  35   1213.912841796875\n",
      "loss at Epoch  36   1213.929443359375\n",
      "loss at Epoch  37   1213.92626953125\n",
      "loss at Epoch  38   1213.9267578125\n",
      "loss at Epoch  39   1213.9439697265625\n",
      "loss at Epoch  0   1328.716552734375\n",
      "loss at Epoch  1   1327.34130859375\n",
      "loss at Epoch  2   1328.435791015625\n",
      "loss at Epoch  3   1326.8258056640625\n",
      "loss at Epoch  4   1326.48046875\n",
      "loss at Epoch  5   1326.5732421875\n",
      "loss at Epoch  6   1326.7763671875\n",
      "loss at Epoch  7   1326.3814697265625\n",
      "loss at Epoch  8   1326.72314453125\n",
      "loss at Epoch  9   1327.137451171875\n",
      "loss at Epoch  10   1326.9176025390625\n",
      "loss at Epoch  11   1326.970458984375\n",
      "loss at Epoch  12   1327.270751953125\n",
      "loss at Epoch  13   1327.4044189453125\n",
      "loss at Epoch  14   1327.61474609375\n",
      "loss at Epoch  15   1327.5693359375\n",
      "loss at Epoch  16   1327.422607421875\n",
      "loss at Epoch  17   1327.4940185546875\n",
      "loss at Epoch  18   1327.7381591796875\n",
      "loss at Epoch  19   1327.6724853515625\n",
      "loss at Epoch  20   1327.7193603515625\n",
      "loss at Epoch  21   1327.72265625\n",
      "loss at Epoch  22   1327.8310546875\n",
      "loss at Epoch  23   1327.80810546875\n",
      "loss at Epoch  24   1327.7979736328125\n",
      "loss at Epoch  25   1327.835693359375\n",
      "loss at Epoch  26   1327.8587646484375\n",
      "loss at Epoch  27   1327.8818359375\n",
      "loss at Epoch  28   1327.8701171875\n",
      "loss at Epoch  29   1327.88427734375\n",
      "loss at Epoch  30   1327.90771484375\n",
      "loss at Epoch  31   1327.90478515625\n",
      "loss at Epoch  32   1327.9139404296875\n",
      "loss at Epoch  33   1327.9254150390625\n",
      "loss at Epoch  34   1327.94140625\n",
      "loss at Epoch  35   1327.9437255859375\n",
      "loss at Epoch  36   1327.977783203125\n",
      "loss at Epoch  37   1327.96875\n",
      "loss at Epoch  38   1327.9698486328125\n",
      "loss at Epoch  39   1327.979248046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1487.575927734375\n",
      "loss at Epoch  1   1487.5604248046875\n",
      "loss at Epoch  2   1488.264892578125\n",
      "loss at Epoch  3   1488.35791015625\n",
      "loss at Epoch  4   1488.98046875\n",
      "loss at Epoch  5   1488.9588623046875\n",
      "loss at Epoch  6   1489.2508544921875\n",
      "loss at Epoch  7   1489.040283203125\n",
      "loss at Epoch  8   1489.419921875\n",
      "loss at Epoch  9   1489.917236328125\n",
      "loss at Epoch  10   1489.7867431640625\n",
      "loss at Epoch  11   1490.271484375\n",
      "loss at Epoch  12   1490.4566650390625\n",
      "loss at Epoch  13   1490.5570068359375\n",
      "loss at Epoch  14   1490.509521484375\n",
      "loss at Epoch  15   1490.5279541015625\n",
      "loss at Epoch  16   1490.6068115234375\n",
      "loss at Epoch  17   1490.802734375\n",
      "loss at Epoch  18   1490.868408203125\n",
      "loss at Epoch  19   1490.9010009765625\n",
      "loss at Epoch  20   1490.9176025390625\n",
      "loss at Epoch  21   1490.948974609375\n",
      "loss at Epoch  22   1490.958984375\n",
      "loss at Epoch  23   1490.968505859375\n",
      "loss at Epoch  24   1491.01220703125\n",
      "loss at Epoch  25   1491.051025390625\n",
      "loss at Epoch  26   1491.123779296875\n",
      "loss at Epoch  27   1491.15283203125\n",
      "loss at Epoch  28   1491.171142578125\n",
      "loss at Epoch  29   1491.13134765625\n",
      "loss at Epoch  30   1491.1807861328125\n",
      "loss at Epoch  31   1491.199951171875\n",
      "loss at Epoch  32   1491.208740234375\n",
      "loss at Epoch  33   1491.227294921875\n",
      "loss at Epoch  34   1491.240234375\n",
      "loss at Epoch  35   1491.2265625\n",
      "loss at Epoch  36   1491.2354736328125\n",
      "loss at Epoch  37   1491.242431640625\n",
      "loss at Epoch  38   1491.254150390625\n",
      "loss at Epoch  39   1491.26025390625\n",
      "loss at Epoch  0   1324.423828125\n",
      "loss at Epoch  1   1323.4033203125\n",
      "loss at Epoch  2   1324.009765625\n",
      "loss at Epoch  3   1323.79052734375\n",
      "loss at Epoch  4   1323.047607421875\n",
      "loss at Epoch  5   1322.3994140625\n",
      "loss at Epoch  6   1322.902587890625\n",
      "loss at Epoch  7   1322.36279296875\n",
      "loss at Epoch  8   1322.739990234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  9   1323.1685791015625\n",
      "loss at Epoch  10   1322.568603515625\n",
      "loss at Epoch  11   1322.4305419921875\n",
      "loss at Epoch  12   1323.0906982421875\n",
      "loss at Epoch  13   1323.039794921875\n",
      "loss at Epoch  14   1322.8214111328125\n",
      "loss at Epoch  15   1322.924072265625\n",
      "loss at Epoch  16   1323.0494384765625\n",
      "loss at Epoch  17   1323.136474609375\n",
      "loss at Epoch  18   1323.16943359375\n",
      "loss at Epoch  19   1323.226806640625\n",
      "loss at Epoch  20   1323.2757568359375\n",
      "loss at Epoch  21   1323.318603515625\n",
      "loss at Epoch  22   1323.3900146484375\n",
      "loss at Epoch  23   1323.3394775390625\n",
      "loss at Epoch  24   1323.3619384765625\n",
      "loss at Epoch  25   1323.3720703125\n",
      "loss at Epoch  26   1323.442138671875\n",
      "loss at Epoch  27   1323.4281005859375\n",
      "loss at Epoch  28   1323.472900390625\n",
      "loss at Epoch  29   1323.482177734375\n",
      "loss at Epoch  30   1323.52734375\n",
      "loss at Epoch  31   1323.533447265625\n",
      "loss at Epoch  32   1323.523193359375\n",
      "loss at Epoch  33   1323.531494140625\n",
      "loss at Epoch  34   1323.522216796875\n",
      "loss at Epoch  35   1323.53857421875\n",
      "loss at Epoch  36   1323.552001953125\n",
      "loss at Epoch  37   1323.558837890625\n",
      "loss at Epoch  38   1323.571533203125\n",
      "loss at Epoch  39   1323.5782470703125\n",
      "loss at Epoch  0   1346.7623291015625\n",
      "loss at Epoch  1   1346.6763916015625\n",
      "loss at Epoch  2   1345.18505859375\n",
      "loss at Epoch  3   1345.783447265625\n",
      "loss at Epoch  4   1344.634521484375\n",
      "loss at Epoch  5   1344.1619873046875\n",
      "loss at Epoch  6   1344.4970703125\n",
      "loss at Epoch  7   1344.8897705078125\n",
      "loss at Epoch  8   1345.341064453125\n",
      "loss at Epoch  9   1345.383544921875\n",
      "loss at Epoch  10   1345.426513671875\n",
      "loss at Epoch  11   1345.2906494140625\n",
      "loss at Epoch  12   1345.47998046875\n",
      "loss at Epoch  13   1345.5806884765625\n",
      "loss at Epoch  14   1345.625\n",
      "loss at Epoch  15   1345.70849609375\n",
      "loss at Epoch  16   1345.77294921875\n",
      "loss at Epoch  17   1345.8355712890625\n",
      "loss at Epoch  18   1345.8809814453125\n",
      "loss at Epoch  19   1345.80908203125\n",
      "loss at Epoch  20   1345.858642578125\n",
      "loss at Epoch  21   1345.86669921875\n",
      "loss at Epoch  22   1345.89892578125\n",
      "loss at Epoch  23   1345.9281005859375\n",
      "loss at Epoch  24   1345.93701171875\n",
      "loss at Epoch  25   1345.9364013671875\n",
      "loss at Epoch  26   1345.940185546875\n",
      "loss at Epoch  27   1345.987548828125\n",
      "loss at Epoch  28   1346.011474609375\n",
      "loss at Epoch  29   1346.011962890625\n",
      "loss at Epoch  30   1346.037353515625\n",
      "loss at Epoch  31   1346.0506591796875\n",
      "loss at Epoch  32   1346.0584716796875\n",
      "loss at Epoch  33   1346.064208984375\n",
      "loss at Epoch  34   1346.098388671875\n",
      "loss at Epoch  35   1346.0748291015625\n",
      "loss at Epoch  36   1346.086181640625\n",
      "loss at Epoch  37   1346.07177734375\n",
      "loss at Epoch  38   1346.0928955078125\n",
      "loss at Epoch  39   1346.10009765625\n",
      "loss at Epoch  0   1329.4132080078125\n",
      "loss at Epoch  1   1329.41455078125\n",
      "loss at Epoch  2   1327.083740234375\n",
      "loss at Epoch  3   1327.921630859375\n",
      "loss at Epoch  4   1327.34326171875\n",
      "loss at Epoch  5   1326.42041015625\n",
      "loss at Epoch  6   1326.91455078125\n",
      "loss at Epoch  7   1326.742919921875\n",
      "loss at Epoch  8   1327.33447265625\n",
      "loss at Epoch  9   1327.51025390625\n",
      "loss at Epoch  10   1327.46044921875\n",
      "loss at Epoch  11   1327.6468505859375\n",
      "loss at Epoch  12   1327.8193359375\n",
      "loss at Epoch  13   1327.755615234375\n",
      "loss at Epoch  14   1327.747314453125\n",
      "loss at Epoch  15   1327.969482421875\n",
      "loss at Epoch  16   1327.8616943359375\n",
      "loss at Epoch  17   1327.8394775390625\n",
      "loss at Epoch  18   1328.0301513671875\n",
      "loss at Epoch  19   1328.0382080078125\n",
      "loss at Epoch  20   1327.9779052734375\n",
      "loss at Epoch  21   1327.967529296875\n",
      "loss at Epoch  22   1328.107666015625\n",
      "loss at Epoch  23   1328.1610107421875\n",
      "loss at Epoch  24   1328.141845703125\n",
      "loss at Epoch  25   1328.1297607421875\n",
      "loss at Epoch  26   1328.1314697265625\n",
      "loss at Epoch  27   1328.1488037109375\n",
      "loss at Epoch  28   1328.1728515625\n",
      "loss at Epoch  29   1328.18408203125\n",
      "loss at Epoch  30   1328.220458984375\n",
      "loss at Epoch  31   1328.222412109375\n",
      "loss at Epoch  32   1328.2366943359375\n",
      "loss at Epoch  33   1328.25341796875\n",
      "loss at Epoch  34   1328.239013671875\n",
      "loss at Epoch  35   1328.257568359375\n",
      "loss at Epoch  36   1328.2647705078125\n",
      "loss at Epoch  37   1328.26611328125\n",
      "loss at Epoch  38   1328.26953125\n",
      "loss at Epoch  39   1328.279541015625\n",
      "loss at Epoch  0   1233.8818359375\n",
      "loss at Epoch  1   1232.382568359375\n",
      "loss at Epoch  2   1231.9315185546875\n",
      "loss at Epoch  3   1232.4317626953125\n",
      "loss at Epoch  4   1232.3218994140625\n",
      "loss at Epoch  5   1232.5567626953125\n",
      "loss at Epoch  6   1232.39453125\n",
      "loss at Epoch  7   1232.38232421875\n",
      "loss at Epoch  8   1232.493896484375\n",
      "loss at Epoch  9   1232.6541748046875\n",
      "loss at Epoch  10   1233.1396484375\n",
      "loss at Epoch  11   1232.7052001953125\n",
      "loss at Epoch  12   1232.7490234375\n",
      "loss at Epoch  13   1232.73095703125\n",
      "loss at Epoch  14   1232.769775390625\n",
      "loss at Epoch  15   1232.80712890625\n",
      "loss at Epoch  16   1232.841796875\n",
      "loss at Epoch  17   1233.033203125\n",
      "loss at Epoch  18   1233.01025390625\n",
      "loss at Epoch  19   1232.9912109375\n",
      "loss at Epoch  20   1233.0697021484375\n",
      "loss at Epoch  21   1233.14697265625\n",
      "loss at Epoch  22   1233.148681640625\n",
      "loss at Epoch  23   1233.15625\n",
      "loss at Epoch  24   1233.2122802734375\n",
      "loss at Epoch  25   1233.20068359375\n",
      "loss at Epoch  26   1233.2080078125\n",
      "loss at Epoch  27   1233.248046875\n",
      "loss at Epoch  28   1233.269775390625\n",
      "loss at Epoch  29   1233.284423828125\n",
      "loss at Epoch  30   1233.2813720703125\n",
      "loss at Epoch  31   1233.30859375\n",
      "loss at Epoch  32   1233.31201171875\n",
      "loss at Epoch  33   1233.3214111328125\n",
      "loss at Epoch  34   1233.329345703125\n",
      "loss at Epoch  35   1233.333251953125\n",
      "loss at Epoch  36   1233.349365234375\n",
      "loss at Epoch  37   1233.3369140625\n",
      "loss at Epoch  38   1233.34130859375\n",
      "loss at Epoch  39   1233.357177734375\n",
      "loss at Epoch  0   1308.97314453125\n",
      "loss at Epoch  1   1308.7724609375\n",
      "loss at Epoch  2   1307.93408203125\n",
      "loss at Epoch  3   1306.584228515625\n",
      "loss at Epoch  4   1307.134521484375\n",
      "loss at Epoch  5   1307.1904296875\n",
      "loss at Epoch  6   1307.37744140625\n",
      "loss at Epoch  7   1307.370361328125\n",
      "loss at Epoch  8   1307.17236328125\n",
      "loss at Epoch  9   1307.10693359375\n",
      "loss at Epoch  10   1307.095458984375\n",
      "loss at Epoch  11   1307.246337890625\n",
      "loss at Epoch  12   1307.486083984375\n",
      "loss at Epoch  13   1307.6971435546875\n",
      "loss at Epoch  14   1307.394287109375\n",
      "loss at Epoch  15   1307.6436767578125\n",
      "loss at Epoch  16   1307.6572265625\n",
      "loss at Epoch  17   1307.6142578125\n",
      "loss at Epoch  18   1307.773681640625\n",
      "loss at Epoch  19   1307.739013671875\n",
      "loss at Epoch  20   1307.835205078125\n",
      "loss at Epoch  21   1307.85498046875\n",
      "loss at Epoch  22   1307.830078125\n",
      "loss at Epoch  23   1307.8837890625\n",
      "loss at Epoch  24   1307.9130859375\n",
      "loss at Epoch  25   1307.920654296875\n",
      "loss at Epoch  26   1307.907470703125\n",
      "loss at Epoch  27   1307.9449462890625\n",
      "loss at Epoch  28   1307.9561767578125\n",
      "loss at Epoch  29   1307.9775390625\n",
      "loss at Epoch  30   1307.99169921875\n",
      "loss at Epoch  31   1307.981689453125\n",
      "loss at Epoch  32   1308.0140380859375\n",
      "loss at Epoch  33   1308.01806640625\n",
      "loss at Epoch  34   1308.035400390625\n",
      "loss at Epoch  35   1308.04345703125\n",
      "loss at Epoch  36   1308.046142578125\n",
      "loss at Epoch  37   1308.0491943359375\n",
      "loss at Epoch  38   1308.047119140625\n",
      "loss at Epoch  39   1308.0633544921875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1237.5794677734375\n",
      "loss at Epoch  1   1238.2861328125\n",
      "loss at Epoch  2   1238.6572265625\n",
      "loss at Epoch  3   1238.714599609375\n",
      "loss at Epoch  4   1238.835205078125\n",
      "loss at Epoch  5   1238.109375\n",
      "loss at Epoch  6   1238.9720458984375\n",
      "loss at Epoch  7   1238.9158935546875\n",
      "loss at Epoch  8   1239.1396484375\n",
      "loss at Epoch  9   1239.1700439453125\n",
      "loss at Epoch  10   1239.1993408203125\n",
      "loss at Epoch  11   1239.371826171875\n",
      "loss at Epoch  12   1239.443359375\n",
      "loss at Epoch  13   1239.774169921875\n",
      "loss at Epoch  14   1239.798583984375\n",
      "loss at Epoch  15   1239.8658447265625\n",
      "loss at Epoch  16   1239.895263671875\n",
      "loss at Epoch  17   1239.94873046875\n",
      "loss at Epoch  18   1240.015625\n",
      "loss at Epoch  19   1239.99462890625\n",
      "loss at Epoch  20   1240.093505859375\n",
      "loss at Epoch  21   1240.2008056640625\n",
      "loss at Epoch  22   1240.117919921875\n",
      "loss at Epoch  23   1240.2236328125\n",
      "loss at Epoch  24   1240.238525390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  25   1240.2442626953125\n",
      "loss at Epoch  26   1240.25732421875\n",
      "loss at Epoch  27   1240.264892578125\n",
      "loss at Epoch  28   1240.26904296875\n",
      "loss at Epoch  29   1240.322265625\n",
      "loss at Epoch  30   1240.313232421875\n",
      "loss at Epoch  31   1240.3134765625\n",
      "loss at Epoch  32   1240.334716796875\n",
      "loss at Epoch  33   1240.348388671875\n",
      "loss at Epoch  34   1240.3837890625\n",
      "loss at Epoch  35   1240.396240234375\n",
      "loss at Epoch  36   1240.398681640625\n",
      "loss at Epoch  37   1240.3909912109375\n",
      "loss at Epoch  38   1240.386962890625\n",
      "loss at Epoch  39   1240.397216796875\n",
      "loss at Epoch  0   1122.164794921875\n",
      "loss at Epoch  1   1120.7186279296875\n",
      "loss at Epoch  2   1119.9471435546875\n",
      "loss at Epoch  3   1119.02783203125\n",
      "loss at Epoch  4   1120.007080078125\n",
      "loss at Epoch  5   1120.03955078125\n",
      "loss at Epoch  6   1119.4881591796875\n",
      "loss at Epoch  7   1119.2994384765625\n",
      "loss at Epoch  8   1120.015380859375\n",
      "loss at Epoch  9   1119.956787109375\n",
      "loss at Epoch  10   1120.237548828125\n",
      "loss at Epoch  11   1120.750244140625\n",
      "loss at Epoch  12   1120.65478515625\n",
      "loss at Epoch  13   1120.7655029296875\n",
      "loss at Epoch  14   1120.885986328125\n",
      "loss at Epoch  15   1120.812744140625\n",
      "loss at Epoch  16   1120.9891357421875\n",
      "loss at Epoch  17   1121.0615234375\n",
      "loss at Epoch  18   1121.061279296875\n",
      "loss at Epoch  19   1121.047607421875\n",
      "loss at Epoch  20   1121.1612548828125\n",
      "loss at Epoch  21   1121.162841796875\n",
      "loss at Epoch  22   1121.2249755859375\n",
      "loss at Epoch  23   1121.2529296875\n",
      "loss at Epoch  24   1121.325927734375\n",
      "loss at Epoch  25   1121.3267822265625\n",
      "loss at Epoch  26   1121.3289794921875\n",
      "loss at Epoch  27   1121.33740234375\n",
      "loss at Epoch  28   1121.366943359375\n",
      "loss at Epoch  29   1121.38525390625\n",
      "loss at Epoch  30   1121.388671875\n",
      "loss at Epoch  31   1121.4342041015625\n",
      "loss at Epoch  32   1121.4141845703125\n",
      "loss at Epoch  33   1121.4300537109375\n",
      "loss at Epoch  34   1121.437255859375\n",
      "loss at Epoch  35   1121.48291015625\n",
      "loss at Epoch  36   1121.472900390625\n",
      "loss at Epoch  37   1121.4697265625\n",
      "loss at Epoch  38   1121.484130859375\n",
      "loss at Epoch  39   1121.4798583984375\n",
      "loss at Epoch  0   1529.406494140625\n",
      "loss at Epoch  1   1528.634521484375\n",
      "loss at Epoch  2   1528.316162109375\n",
      "loss at Epoch  3   1527.3138427734375\n",
      "loss at Epoch  4   1528.3154296875\n",
      "loss at Epoch  5   1527.9071044921875\n",
      "loss at Epoch  6   1528.1533203125\n",
      "loss at Epoch  7   1527.90771484375\n",
      "loss at Epoch  8   1528.8800048828125\n",
      "loss at Epoch  9   1528.229736328125\n",
      "loss at Epoch  10   1528.979248046875\n",
      "loss at Epoch  11   1528.445068359375\n",
      "loss at Epoch  12   1528.3431396484375\n",
      "loss at Epoch  13   1528.610107421875\n",
      "loss at Epoch  14   1528.986083984375\n",
      "loss at Epoch  15   1528.745361328125\n",
      "loss at Epoch  16   1528.9158935546875\n",
      "loss at Epoch  17   1528.8035888671875\n",
      "loss at Epoch  18   1528.755615234375\n",
      "loss at Epoch  19   1528.8017578125\n",
      "loss at Epoch  20   1528.8184814453125\n",
      "loss at Epoch  21   1528.974365234375\n",
      "loss at Epoch  22   1528.977783203125\n",
      "loss at Epoch  23   1529.007568359375\n",
      "loss at Epoch  24   1529.0677490234375\n",
      "loss at Epoch  25   1529.1143798828125\n",
      "loss at Epoch  26   1529.0565185546875\n",
      "loss at Epoch  27   1529.07373046875\n",
      "loss at Epoch  28   1529.176513671875\n",
      "loss at Epoch  29   1529.17578125\n",
      "loss at Epoch  30   1529.16650390625\n",
      "loss at Epoch  31   1529.2066650390625\n",
      "loss at Epoch  32   1529.20947265625\n",
      "loss at Epoch  33   1529.223388671875\n",
      "loss at Epoch  34   1529.207763671875\n",
      "loss at Epoch  35   1529.229736328125\n",
      "loss at Epoch  36   1529.2181396484375\n",
      "loss at Epoch  37   1529.221923828125\n",
      "loss at Epoch  38   1529.2442626953125\n",
      "loss at Epoch  39   1529.241455078125\n",
      "loss at Epoch  0   2018.0848388671875\n",
      "loss at Epoch  1   2017.8551025390625\n",
      "loss at Epoch  2   2016.876220703125\n",
      "loss at Epoch  3   2017.317626953125\n",
      "loss at Epoch  4   2017.19189453125\n",
      "loss at Epoch  5   2016.994873046875\n",
      "loss at Epoch  6   2017.08984375\n",
      "loss at Epoch  7   2017.029296875\n",
      "loss at Epoch  8   2017.3699951171875\n",
      "loss at Epoch  9   2016.76171875\n",
      "loss at Epoch  10   2016.895263671875\n",
      "loss at Epoch  11   2017.2457275390625\n",
      "loss at Epoch  12   2017.59228515625\n",
      "loss at Epoch  13   2017.4530029296875\n",
      "loss at Epoch  14   2017.494384765625\n",
      "loss at Epoch  15   2017.663818359375\n",
      "loss at Epoch  16   2017.794189453125\n",
      "loss at Epoch  17   2017.7579345703125\n",
      "loss at Epoch  18   2017.80126953125\n",
      "loss at Epoch  19   2017.7589111328125\n",
      "loss at Epoch  20   2017.7841796875\n",
      "loss at Epoch  21   2017.8695068359375\n",
      "loss at Epoch  22   2017.9241943359375\n",
      "loss at Epoch  23   2018.0186767578125\n",
      "loss at Epoch  24   2018.0045166015625\n",
      "loss at Epoch  25   2018.031005859375\n",
      "loss at Epoch  26   2018.0238037109375\n",
      "loss at Epoch  27   2018.071044921875\n",
      "loss at Epoch  28   2018.08203125\n",
      "loss at Epoch  29   2018.089599609375\n",
      "loss at Epoch  30   2018.151123046875\n",
      "loss at Epoch  31   2018.116455078125\n",
      "loss at Epoch  32   2018.120849609375\n",
      "loss at Epoch  33   2018.1348876953125\n",
      "loss at Epoch  34   2018.150146484375\n",
      "loss at Epoch  35   2018.171875\n",
      "loss at Epoch  36   2018.167236328125\n",
      "loss at Epoch  37   2018.171875\n",
      "loss at Epoch  38   2018.185546875\n",
      "loss at Epoch  39   2018.17236328125\n",
      "loss at Epoch  0   1333.712646484375\n",
      "loss at Epoch  1   1331.764404296875\n",
      "loss at Epoch  2   1332.4246826171875\n",
      "loss at Epoch  3   1332.835693359375\n",
      "loss at Epoch  4   1332.489013671875\n",
      "loss at Epoch  5   1332.008544921875\n",
      "loss at Epoch  6   1332.091552734375\n",
      "loss at Epoch  7   1332.52099609375\n",
      "loss at Epoch  8   1331.97509765625\n",
      "loss at Epoch  9   1332.370849609375\n",
      "loss at Epoch  10   1332.3828125\n",
      "loss at Epoch  11   1332.298095703125\n",
      "loss at Epoch  12   1332.593017578125\n",
      "loss at Epoch  13   1332.4639892578125\n",
      "loss at Epoch  14   1332.581787109375\n",
      "loss at Epoch  15   1332.667724609375\n",
      "loss at Epoch  16   1332.644287109375\n",
      "loss at Epoch  17   1332.6708984375\n",
      "loss at Epoch  18   1332.77294921875\n",
      "loss at Epoch  19   1332.8173828125\n",
      "loss at Epoch  20   1332.80615234375\n",
      "loss at Epoch  21   1332.94189453125\n",
      "loss at Epoch  22   1332.9130859375\n",
      "loss at Epoch  23   1332.9088134765625\n",
      "loss at Epoch  24   1332.963134765625\n",
      "loss at Epoch  25   1333.03369140625\n",
      "loss at Epoch  26   1332.9957275390625\n",
      "loss at Epoch  27   1333.0107421875\n",
      "loss at Epoch  28   1333.0220947265625\n",
      "loss at Epoch  29   1333.03173828125\n",
      "loss at Epoch  30   1333.0400390625\n",
      "loss at Epoch  31   1333.0501708984375\n",
      "loss at Epoch  32   1333.0537109375\n",
      "loss at Epoch  33   1333.0650634765625\n",
      "loss at Epoch  34   1333.09912109375\n",
      "loss at Epoch  35   1333.086181640625\n",
      "loss at Epoch  36   1333.085693359375\n",
      "loss at Epoch  37   1333.0985107421875\n",
      "loss at Epoch  38   1333.11181640625\n",
      "loss at Epoch  39   1333.102294921875\n",
      "loss at Epoch  0   1122.693115234375\n",
      "loss at Epoch  1   1122.35986328125\n",
      "loss at Epoch  2   1120.619140625\n",
      "loss at Epoch  3   1120.18994140625\n",
      "loss at Epoch  4   1120.394287109375\n",
      "loss at Epoch  5   1120.752685546875\n",
      "loss at Epoch  6   1120.444091796875\n",
      "loss at Epoch  7   1120.920654296875\n",
      "loss at Epoch  8   1120.350830078125\n",
      "loss at Epoch  9   1120.9793701171875\n",
      "loss at Epoch  10   1120.836181640625\n",
      "loss at Epoch  11   1120.958984375\n",
      "loss at Epoch  12   1121.064208984375\n",
      "loss at Epoch  13   1121.159423828125\n",
      "loss at Epoch  14   1121.04638671875\n",
      "loss at Epoch  15   1121.246337890625\n",
      "loss at Epoch  16   1121.3309326171875\n",
      "loss at Epoch  17   1121.39599609375\n",
      "loss at Epoch  18   1121.396484375\n",
      "loss at Epoch  19   1121.456787109375\n",
      "loss at Epoch  20   1121.387939453125\n",
      "loss at Epoch  21   1121.5341796875\n",
      "loss at Epoch  22   1121.594482421875\n",
      "loss at Epoch  23   1121.583740234375\n",
      "loss at Epoch  24   1121.5965576171875\n",
      "loss at Epoch  25   1121.5977783203125\n",
      "loss at Epoch  26   1121.603759765625\n",
      "loss at Epoch  27   1121.643798828125\n",
      "loss at Epoch  28   1121.7197265625\n",
      "loss at Epoch  29   1121.6951904296875\n",
      "loss at Epoch  30   1121.736083984375\n",
      "loss at Epoch  31   1121.7222900390625\n",
      "loss at Epoch  32   1121.737060546875\n",
      "loss at Epoch  33   1121.73046875\n",
      "loss at Epoch  34   1121.75244140625\n",
      "loss at Epoch  35   1121.7216796875\n",
      "loss at Epoch  36   1121.7479248046875\n",
      "loss at Epoch  37   1121.760986328125\n",
      "loss at Epoch  38   1121.7578125\n",
      "loss at Epoch  39   1121.7762451171875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1374.71044921875\n",
      "loss at Epoch  1   1375.2669677734375\n",
      "loss at Epoch  2   1375.646484375\n",
      "loss at Epoch  3   1375.43212890625\n",
      "loss at Epoch  4   1376.1256103515625\n",
      "loss at Epoch  5   1376.38818359375\n",
      "loss at Epoch  6   1376.508544921875\n",
      "loss at Epoch  7   1376.3209228515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  8   1376.700927734375\n",
      "loss at Epoch  9   1376.744384765625\n",
      "loss at Epoch  10   1376.924560546875\n",
      "loss at Epoch  11   1376.990478515625\n",
      "loss at Epoch  12   1376.963623046875\n",
      "loss at Epoch  13   1376.95654296875\n",
      "loss at Epoch  14   1376.871337890625\n",
      "loss at Epoch  15   1377.1357421875\n",
      "loss at Epoch  16   1377.1171875\n",
      "loss at Epoch  17   1377.139892578125\n",
      "loss at Epoch  18   1377.22021484375\n",
      "loss at Epoch  19   1377.3555908203125\n",
      "loss at Epoch  20   1377.31689453125\n",
      "loss at Epoch  21   1377.4189453125\n",
      "loss at Epoch  22   1377.3798828125\n",
      "loss at Epoch  23   1377.4600830078125\n",
      "loss at Epoch  24   1377.448486328125\n",
      "loss at Epoch  25   1377.4638671875\n",
      "loss at Epoch  26   1377.4931640625\n",
      "loss at Epoch  27   1377.492919921875\n",
      "loss at Epoch  28   1377.552978515625\n",
      "loss at Epoch  29   1377.5458984375\n",
      "loss at Epoch  30   1377.5511474609375\n",
      "loss at Epoch  31   1377.568359375\n",
      "loss at Epoch  32   1377.56396484375\n",
      "loss at Epoch  33   1377.582275390625\n",
      "loss at Epoch  34   1377.59619140625\n",
      "loss at Epoch  35   1377.599609375\n",
      "loss at Epoch  36   1377.605224609375\n",
      "loss at Epoch  37   1377.624755859375\n",
      "loss at Epoch  38   1377.623046875\n",
      "loss at Epoch  39   1377.631103515625\n",
      "loss at Epoch  0   1325.493408203125\n",
      "loss at Epoch  1   1325.9683837890625\n",
      "loss at Epoch  2   1323.833740234375\n",
      "loss at Epoch  3   1323.32275390625\n",
      "loss at Epoch  4   1323.071533203125\n",
      "loss at Epoch  5   1322.755126953125\n",
      "loss at Epoch  6   1323.52001953125\n",
      "loss at Epoch  7   1323.468505859375\n",
      "loss at Epoch  8   1323.302978515625\n",
      "loss at Epoch  9   1323.6876220703125\n",
      "loss at Epoch  10   1323.863037109375\n",
      "loss at Epoch  11   1323.9119873046875\n",
      "loss at Epoch  12   1324.264404296875\n",
      "loss at Epoch  13   1324.10791015625\n",
      "loss at Epoch  14   1324.046142578125\n",
      "loss at Epoch  15   1324.2122802734375\n",
      "loss at Epoch  16   1324.1898193359375\n",
      "loss at Epoch  17   1324.315185546875\n",
      "loss at Epoch  18   1324.25439453125\n",
      "loss at Epoch  19   1324.29248046875\n",
      "loss at Epoch  20   1324.3577880859375\n",
      "loss at Epoch  21   1324.3470458984375\n",
      "loss at Epoch  22   1324.3536376953125\n",
      "loss at Epoch  23   1324.364501953125\n",
      "loss at Epoch  24   1324.4017333984375\n",
      "loss at Epoch  25   1324.4635009765625\n",
      "loss at Epoch  26   1324.4642333984375\n",
      "loss at Epoch  27   1324.4937744140625\n",
      "loss at Epoch  28   1324.50146484375\n",
      "loss at Epoch  29   1324.5118408203125\n",
      "loss at Epoch  30   1324.5565185546875\n",
      "loss at Epoch  31   1324.54296875\n",
      "loss at Epoch  32   1324.5850830078125\n",
      "loss at Epoch  33   1324.5640869140625\n",
      "loss at Epoch  34   1324.5673828125\n",
      "loss at Epoch  35   1324.58203125\n",
      "loss at Epoch  36   1324.593505859375\n",
      "loss at Epoch  37   1324.598388671875\n",
      "loss at Epoch  38   1324.5947265625\n",
      "loss at Epoch  39   1324.597900390625\n",
      "loss at Epoch  0   1234.091064453125\n",
      "loss at Epoch  1   1234.762451171875\n",
      "loss at Epoch  2   1233.212158203125\n",
      "loss at Epoch  3   1232.146484375\n",
      "loss at Epoch  4   1232.3717041015625\n",
      "loss at Epoch  5   1232.0615234375\n",
      "loss at Epoch  6   1232.053955078125\n",
      "loss at Epoch  7   1232.3118896484375\n",
      "loss at Epoch  8   1232.0880126953125\n",
      "loss at Epoch  9   1232.353759765625\n",
      "loss at Epoch  10   1232.3525390625\n",
      "loss at Epoch  11   1232.8359375\n",
      "loss at Epoch  12   1232.719482421875\n",
      "loss at Epoch  13   1232.741943359375\n",
      "loss at Epoch  14   1232.78759765625\n",
      "loss at Epoch  15   1232.802734375\n",
      "loss at Epoch  16   1232.8399658203125\n",
      "loss at Epoch  17   1232.8690185546875\n",
      "loss at Epoch  18   1232.9853515625\n",
      "loss at Epoch  19   1232.997314453125\n",
      "loss at Epoch  20   1233.033447265625\n",
      "loss at Epoch  21   1233.0316162109375\n",
      "loss at Epoch  22   1233.093505859375\n",
      "loss at Epoch  23   1233.13037109375\n",
      "loss at Epoch  24   1233.159912109375\n",
      "loss at Epoch  25   1233.177490234375\n",
      "loss at Epoch  26   1233.21142578125\n",
      "loss at Epoch  27   1233.216064453125\n",
      "loss at Epoch  28   1233.20751953125\n",
      "loss at Epoch  29   1233.250732421875\n",
      "loss at Epoch  30   1233.2335205078125\n",
      "loss at Epoch  31   1233.283935546875\n",
      "loss at Epoch  32   1233.2838134765625\n",
      "loss at Epoch  33   1233.31103515625\n",
      "loss at Epoch  34   1233.31103515625\n",
      "loss at Epoch  35   1233.3046875\n",
      "loss at Epoch  36   1233.3076171875\n",
      "loss at Epoch  37   1233.333251953125\n",
      "loss at Epoch  38   1233.328125\n",
      "loss at Epoch  39   1233.3377685546875\n",
      "loss at Epoch  0   1261.2047119140625\n",
      "loss at Epoch  1   1261.34130859375\n",
      "loss at Epoch  2   1259.6654052734375\n",
      "loss at Epoch  3   1259.8153076171875\n",
      "loss at Epoch  4   1260.0400390625\n",
      "loss at Epoch  5   1260.121826171875\n",
      "loss at Epoch  6   1260.3466796875\n",
      "loss at Epoch  7   1260.381591796875\n",
      "loss at Epoch  8   1260.554931640625\n",
      "loss at Epoch  9   1260.4979248046875\n",
      "loss at Epoch  10   1260.579345703125\n",
      "loss at Epoch  11   1260.4071044921875\n",
      "loss at Epoch  12   1260.55419921875\n",
      "loss at Epoch  13   1260.76953125\n",
      "loss at Epoch  14   1260.71630859375\n",
      "loss at Epoch  15   1260.789306640625\n",
      "loss at Epoch  16   1260.92236328125\n",
      "loss at Epoch  17   1261.1015625\n",
      "loss at Epoch  18   1260.97216796875\n",
      "loss at Epoch  19   1261.06591796875\n",
      "loss at Epoch  20   1261.1949462890625\n",
      "loss at Epoch  21   1261.2158203125\n",
      "loss at Epoch  22   1261.2080078125\n",
      "loss at Epoch  23   1261.302978515625\n",
      "loss at Epoch  24   1261.25390625\n",
      "loss at Epoch  25   1261.2808837890625\n",
      "loss at Epoch  26   1261.295166015625\n",
      "loss at Epoch  27   1261.330078125\n",
      "loss at Epoch  28   1261.323486328125\n",
      "loss at Epoch  29   1261.341552734375\n",
      "loss at Epoch  30   1261.374267578125\n",
      "loss at Epoch  31   1261.39501953125\n",
      "loss at Epoch  32   1261.390380859375\n",
      "loss at Epoch  33   1261.3935546875\n",
      "loss at Epoch  34   1261.41259765625\n",
      "loss at Epoch  35   1261.413330078125\n",
      "loss at Epoch  36   1261.423095703125\n",
      "loss at Epoch  37   1261.41796875\n",
      "loss at Epoch  38   1261.4427490234375\n",
      "loss at Epoch  39   1261.44677734375\n",
      "loss at Epoch  0   1216.867431640625\n",
      "loss at Epoch  1   1216.068603515625\n",
      "loss at Epoch  2   1215.29638671875\n",
      "loss at Epoch  3   1214.22705078125\n",
      "loss at Epoch  4   1214.2059326171875\n",
      "loss at Epoch  5   1214.95361328125\n",
      "loss at Epoch  6   1214.8592529296875\n",
      "loss at Epoch  7   1215.45458984375\n",
      "loss at Epoch  8   1215.011474609375\n",
      "loss at Epoch  9   1215.083251953125\n",
      "loss at Epoch  10   1215.297119140625\n",
      "loss at Epoch  11   1215.1552734375\n",
      "loss at Epoch  12   1215.197021484375\n",
      "loss at Epoch  13   1215.476806640625\n",
      "loss at Epoch  14   1215.593505859375\n",
      "loss at Epoch  15   1215.7158203125\n",
      "loss at Epoch  16   1215.901123046875\n",
      "loss at Epoch  17   1215.8343505859375\n",
      "loss at Epoch  18   1215.837890625\n",
      "loss at Epoch  19   1215.802001953125\n",
      "loss at Epoch  20   1215.9053955078125\n",
      "loss at Epoch  21   1215.89990234375\n",
      "loss at Epoch  22   1216.074951171875\n",
      "loss at Epoch  23   1216.052734375\n",
      "loss at Epoch  24   1216.0740966796875\n",
      "loss at Epoch  25   1216.0052490234375\n",
      "loss at Epoch  26   1216.107666015625\n",
      "loss at Epoch  27   1216.141357421875\n",
      "loss at Epoch  28   1216.115966796875\n",
      "loss at Epoch  29   1216.13330078125\n",
      "loss at Epoch  30   1216.158447265625\n",
      "loss at Epoch  31   1216.16357421875\n",
      "loss at Epoch  32   1216.1768798828125\n",
      "loss at Epoch  33   1216.181884765625\n",
      "loss at Epoch  34   1216.234619140625\n",
      "loss at Epoch  35   1216.218505859375\n",
      "loss at Epoch  36   1216.2232666015625\n",
      "loss at Epoch  37   1216.25341796875\n",
      "loss at Epoch  38   1216.239013671875\n",
      "loss at Epoch  39   1216.240478515625\n",
      "loss at Epoch  0   1322.84375\n",
      "loss at Epoch  1   1321.493408203125\n",
      "loss at Epoch  2   1319.853759765625\n",
      "loss at Epoch  3   1320.1607666015625\n",
      "loss at Epoch  4   1320.9404296875\n",
      "loss at Epoch  5   1320.669189453125\n",
      "loss at Epoch  6   1321.167724609375\n",
      "loss at Epoch  7   1320.9443359375\n",
      "loss at Epoch  8   1320.52685546875\n",
      "loss at Epoch  9   1320.667236328125\n",
      "loss at Epoch  10   1320.942626953125\n",
      "loss at Epoch  11   1321.0106201171875\n",
      "loss at Epoch  12   1321.03515625\n",
      "loss at Epoch  13   1320.9097900390625\n",
      "loss at Epoch  14   1320.891357421875\n",
      "loss at Epoch  15   1321.0076904296875\n",
      "loss at Epoch  16   1321.0623779296875\n",
      "loss at Epoch  17   1321.00732421875\n",
      "loss at Epoch  18   1321.090087890625\n",
      "loss at Epoch  19   1321.1142578125\n",
      "loss at Epoch  20   1321.171142578125\n",
      "loss at Epoch  21   1321.208251953125\n",
      "loss at Epoch  22   1321.2022705078125\n",
      "loss at Epoch  23   1321.218505859375\n",
      "loss at Epoch  24   1321.284912109375\n",
      "loss at Epoch  25   1321.279052734375\n",
      "loss at Epoch  26   1321.28515625\n",
      "loss at Epoch  27   1321.310791015625\n",
      "loss at Epoch  28   1321.3277587890625\n",
      "loss at Epoch  29   1321.322265625\n",
      "loss at Epoch  30   1321.360595703125\n",
      "loss at Epoch  31   1321.361083984375\n",
      "loss at Epoch  32   1321.3519287109375\n",
      "loss at Epoch  33   1321.374755859375\n",
      "loss at Epoch  34   1321.3642578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  35   1321.385986328125\n",
      "loss at Epoch  36   1321.39306640625\n",
      "loss at Epoch  37   1321.41064453125\n",
      "loss at Epoch  38   1321.39794921875\n",
      "loss at Epoch  39   1321.402587890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1237.36376953125\n",
      "loss at Epoch  1   1237.210205078125\n",
      "loss at Epoch  2   1237.517578125\n",
      "loss at Epoch  3   1238.279541015625\n",
      "loss at Epoch  4   1239.6951904296875\n",
      "loss at Epoch  5   1239.01513671875\n",
      "loss at Epoch  6   1238.542236328125\n",
      "loss at Epoch  7   1238.42578125\n",
      "loss at Epoch  8   1238.7469482421875\n",
      "loss at Epoch  9   1239.2109375\n",
      "loss at Epoch  10   1239.6737060546875\n",
      "loss at Epoch  11   1239.43701171875\n",
      "loss at Epoch  12   1239.5655517578125\n",
      "loss at Epoch  13   1239.805419921875\n",
      "loss at Epoch  14   1239.7041015625\n",
      "loss at Epoch  15   1239.835205078125\n",
      "loss at Epoch  16   1239.807861328125\n",
      "loss at Epoch  17   1239.880126953125\n",
      "loss at Epoch  18   1239.9837646484375\n",
      "loss at Epoch  19   1240.0848388671875\n",
      "loss at Epoch  20   1240.140869140625\n",
      "loss at Epoch  21   1240.17333984375\n",
      "loss at Epoch  22   1240.1708984375\n",
      "loss at Epoch  23   1240.1822509765625\n",
      "loss at Epoch  24   1240.231201171875\n",
      "loss at Epoch  25   1240.275390625\n",
      "loss at Epoch  26   1240.282470703125\n",
      "loss at Epoch  27   1240.336669921875\n",
      "loss at Epoch  28   1240.3385009765625\n",
      "loss at Epoch  29   1240.35400390625\n",
      "loss at Epoch  30   1240.338623046875\n",
      "loss at Epoch  31   1240.36669921875\n",
      "loss at Epoch  32   1240.36669921875\n",
      "loss at Epoch  33   1240.385986328125\n",
      "loss at Epoch  34   1240.400634765625\n",
      "loss at Epoch  35   1240.4212646484375\n",
      "loss at Epoch  36   1240.4237060546875\n",
      "loss at Epoch  37   1240.41796875\n",
      "loss at Epoch  38   1240.4287109375\n",
      "loss at Epoch  39   1240.43212890625\n",
      "loss at Epoch  0   1529.8486328125\n",
      "loss at Epoch  1   1528.07763671875\n",
      "loss at Epoch  2   1528.9320068359375\n",
      "loss at Epoch  3   1529.8720703125\n",
      "loss at Epoch  4   1528.181396484375\n",
      "loss at Epoch  5   1527.73486328125\n",
      "loss at Epoch  6   1527.52880859375\n",
      "loss at Epoch  7   1528.42138671875\n",
      "loss at Epoch  8   1528.1162109375\n",
      "loss at Epoch  9   1528.1282958984375\n",
      "loss at Epoch  10   1528.197021484375\n",
      "loss at Epoch  11   1528.49169921875\n",
      "loss at Epoch  12   1528.7054443359375\n",
      "loss at Epoch  13   1528.544189453125\n",
      "loss at Epoch  14   1528.599853515625\n",
      "loss at Epoch  15   1528.69970703125\n",
      "loss at Epoch  16   1528.9788818359375\n",
      "loss at Epoch  17   1528.840087890625\n",
      "loss at Epoch  18   1528.8729248046875\n",
      "loss at Epoch  19   1528.8995361328125\n",
      "loss at Epoch  20   1528.9169921875\n",
      "loss at Epoch  21   1529.031494140625\n",
      "loss at Epoch  22   1528.989013671875\n",
      "loss at Epoch  23   1529.025146484375\n",
      "loss at Epoch  24   1528.9912109375\n",
      "loss at Epoch  25   1529.058837890625\n",
      "loss at Epoch  26   1529.10791015625\n",
      "loss at Epoch  27   1529.089111328125\n",
      "loss at Epoch  28   1529.12841796875\n",
      "loss at Epoch  29   1529.1358642578125\n",
      "loss at Epoch  30   1529.14501953125\n",
      "loss at Epoch  31   1529.1881103515625\n",
      "loss at Epoch  32   1529.198486328125\n",
      "loss at Epoch  33   1529.18359375\n",
      "loss at Epoch  34   1529.176513671875\n",
      "loss at Epoch  35   1529.189697265625\n",
      "loss at Epoch  36   1529.2275390625\n",
      "loss at Epoch  37   1529.207763671875\n",
      "loss at Epoch  38   1529.220458984375\n",
      "loss at Epoch  39   1529.221435546875\n",
      "loss at Epoch  0   1281.9644775390625\n",
      "loss at Epoch  1   1281.7786865234375\n",
      "loss at Epoch  2   1280.62451171875\n",
      "loss at Epoch  3   1280.521240234375\n",
      "loss at Epoch  4   1281.60986328125\n",
      "loss at Epoch  5   1281.32275390625\n",
      "loss at Epoch  6   1280.8887939453125\n",
      "loss at Epoch  7   1281.1619873046875\n",
      "loss at Epoch  8   1281.9078369140625\n",
      "loss at Epoch  9   1281.5816650390625\n",
      "loss at Epoch  10   1281.6630859375\n",
      "loss at Epoch  11   1281.4742431640625\n",
      "loss at Epoch  12   1281.59814453125\n",
      "loss at Epoch  13   1281.66748046875\n",
      "loss at Epoch  14   1281.926025390625\n",
      "loss at Epoch  15   1281.837646484375\n",
      "loss at Epoch  16   1281.86328125\n",
      "loss at Epoch  17   1281.962646484375\n",
      "loss at Epoch  18   1281.9658203125\n",
      "loss at Epoch  19   1282.047119140625\n",
      "loss at Epoch  20   1282.0753173828125\n",
      "loss at Epoch  21   1282.14794921875\n",
      "loss at Epoch  22   1282.124267578125\n",
      "loss at Epoch  23   1282.1728515625\n",
      "loss at Epoch  24   1282.16796875\n",
      "loss at Epoch  25   1282.1541748046875\n",
      "loss at Epoch  26   1282.2215576171875\n",
      "loss at Epoch  27   1282.229736328125\n",
      "loss at Epoch  28   1282.230224609375\n",
      "loss at Epoch  29   1282.2430419921875\n",
      "loss at Epoch  30   1282.244873046875\n",
      "loss at Epoch  31   1282.2532958984375\n",
      "loss at Epoch  32   1282.28955078125\n",
      "loss at Epoch  33   1282.2745361328125\n",
      "loss at Epoch  34   1282.2706298828125\n",
      "loss at Epoch  35   1282.2830810546875\n",
      "loss at Epoch  36   1282.3052978515625\n",
      "loss at Epoch  37   1282.3145751953125\n",
      "loss at Epoch  38   1282.317626953125\n",
      "loss at Epoch  39   1282.3232421875\n",
      "loss at Epoch  0   1322.9998779296875\n",
      "loss at Epoch  1   1322.33935546875\n",
      "loss at Epoch  2   1322.052490234375\n",
      "loss at Epoch  3   1320.7578125\n",
      "loss at Epoch  4   1320.776611328125\n",
      "loss at Epoch  5   1320.8682861328125\n",
      "loss at Epoch  6   1320.8626708984375\n",
      "loss at Epoch  7   1320.7244873046875\n",
      "loss at Epoch  8   1320.51318359375\n",
      "loss at Epoch  9   1320.7615966796875\n",
      "loss at Epoch  10   1321.2406005859375\n",
      "loss at Epoch  11   1321.135498046875\n",
      "loss at Epoch  12   1321.107666015625\n",
      "loss at Epoch  13   1321.2889404296875\n",
      "loss at Epoch  14   1321.4248046875\n",
      "loss at Epoch  15   1321.264404296875\n",
      "loss at Epoch  16   1321.332763671875\n",
      "loss at Epoch  17   1321.34716796875\n",
      "loss at Epoch  18   1321.333740234375\n",
      "loss at Epoch  19   1321.3507080078125\n",
      "loss at Epoch  20   1321.4814453125\n",
      "loss at Epoch  21   1321.456298828125\n",
      "loss at Epoch  22   1321.4600830078125\n",
      "loss at Epoch  23   1321.484130859375\n",
      "loss at Epoch  24   1321.527587890625\n",
      "loss at Epoch  25   1321.525146484375\n",
      "loss at Epoch  26   1321.5316162109375\n",
      "loss at Epoch  27   1321.547119140625\n",
      "loss at Epoch  28   1321.572021484375\n",
      "loss at Epoch  29   1321.5723876953125\n",
      "loss at Epoch  30   1321.588623046875\n",
      "loss at Epoch  31   1321.5869140625\n",
      "loss at Epoch  32   1321.5987548828125\n",
      "loss at Epoch  33   1321.611572265625\n",
      "loss at Epoch  34   1321.6094970703125\n",
      "loss at Epoch  35   1321.620849609375\n",
      "loss at Epoch  36   1321.6171875\n",
      "loss at Epoch  37   1321.6497802734375\n",
      "loss at Epoch  38   1321.6441650390625\n",
      "loss at Epoch  39   1321.66455078125\n",
      "loss at Epoch  0   1195.83837890625\n",
      "loss at Epoch  1   1194.846923828125\n",
      "loss at Epoch  2   1194.29052734375\n",
      "loss at Epoch  3   1194.5029296875\n",
      "loss at Epoch  4   1193.43896484375\n",
      "loss at Epoch  5   1193.685791015625\n",
      "loss at Epoch  6   1193.7490234375\n",
      "loss at Epoch  7   1193.67138671875\n",
      "loss at Epoch  8   1194.00244140625\n",
      "loss at Epoch  9   1194.084716796875\n",
      "loss at Epoch  10   1194.1572265625\n",
      "loss at Epoch  11   1194.2213134765625\n",
      "loss at Epoch  12   1194.2484130859375\n",
      "loss at Epoch  13   1194.16552734375\n",
      "loss at Epoch  14   1194.43359375\n",
      "loss at Epoch  15   1194.434814453125\n",
      "loss at Epoch  16   1194.5054931640625\n",
      "loss at Epoch  17   1194.46142578125\n",
      "loss at Epoch  18   1194.5848388671875\n",
      "loss at Epoch  19   1194.5167236328125\n",
      "loss at Epoch  20   1194.674072265625\n",
      "loss at Epoch  21   1194.6484375\n",
      "loss at Epoch  22   1194.668701171875\n",
      "loss at Epoch  23   1194.714599609375\n",
      "loss at Epoch  24   1194.699462890625\n",
      "loss at Epoch  25   1194.720947265625\n",
      "loss at Epoch  26   1194.7197265625\n",
      "loss at Epoch  27   1194.7578125\n",
      "loss at Epoch  28   1194.7841796875\n",
      "loss at Epoch  29   1194.809814453125\n",
      "loss at Epoch  30   1194.7939453125\n",
      "loss at Epoch  31   1194.8162841796875\n",
      "loss at Epoch  32   1194.806884765625\n",
      "loss at Epoch  33   1194.834716796875\n",
      "loss at Epoch  34   1194.83740234375\n",
      "loss at Epoch  35   1194.863037109375\n",
      "loss at Epoch  36   1194.85400390625\n",
      "loss at Epoch  37   1194.8587646484375\n",
      "loss at Epoch  38   1194.86669921875\n",
      "loss at Epoch  39   1194.868408203125\n",
      "loss at Epoch  0   1264.96728515625\n",
      "loss at Epoch  1   1265.219482421875\n",
      "loss at Epoch  2   1264.107666015625\n",
      "loss at Epoch  3   1262.9080810546875\n",
      "loss at Epoch  4   1262.479248046875\n",
      "loss at Epoch  5   1262.7100830078125\n",
      "loss at Epoch  6   1262.592529296875\n",
      "loss at Epoch  7   1262.8447265625\n",
      "loss at Epoch  8   1262.998046875\n",
      "loss at Epoch  9   1263.332275390625\n",
      "loss at Epoch  10   1263.039794921875\n",
      "loss at Epoch  11   1263.3404541015625\n",
      "loss at Epoch  12   1263.2618408203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  13   1263.45703125\n",
      "loss at Epoch  14   1263.4072265625\n",
      "loss at Epoch  15   1263.5177001953125\n",
      "loss at Epoch  16   1263.494384765625\n",
      "loss at Epoch  17   1263.663818359375\n",
      "loss at Epoch  18   1263.7333984375\n",
      "loss at Epoch  19   1263.81396484375\n",
      "loss at Epoch  20   1263.68701171875\n",
      "loss at Epoch  21   1263.744140625\n",
      "loss at Epoch  22   1263.768310546875\n",
      "loss at Epoch  23   1263.775146484375\n",
      "loss at Epoch  24   1263.7509765625\n",
      "loss at Epoch  25   1263.806884765625\n",
      "loss at Epoch  26   1263.822998046875\n",
      "loss at Epoch  27   1263.8238525390625\n",
      "loss at Epoch  28   1263.85205078125\n",
      "loss at Epoch  29   1263.844482421875\n",
      "loss at Epoch  30   1263.873291015625\n",
      "loss at Epoch  31   1263.8917236328125\n",
      "loss at Epoch  32   1263.8931884765625\n",
      "loss at Epoch  33   1263.901123046875\n",
      "loss at Epoch  34   1263.886962890625\n",
      "loss at Epoch  35   1263.8994140625\n",
      "loss at Epoch  36   1263.921630859375\n",
      "loss at Epoch  37   1263.9337158203125\n",
      "loss at Epoch  38   1263.9288330078125\n",
      "loss at Epoch  39   1263.929931640625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1207.861083984375\n",
      "loss at Epoch  1   1207.5888671875\n",
      "loss at Epoch  2   1208.77685546875\n",
      "loss at Epoch  3   1207.89599609375\n",
      "loss at Epoch  4   1208.06689453125\n",
      "loss at Epoch  5   1209.3388671875\n",
      "loss at Epoch  6   1208.931396484375\n",
      "loss at Epoch  7   1209.2564697265625\n",
      "loss at Epoch  8   1209.6728515625\n",
      "loss at Epoch  9   1209.665771484375\n",
      "loss at Epoch  10   1209.847900390625\n",
      "loss at Epoch  11   1209.91455078125\n",
      "loss at Epoch  12   1210.0506591796875\n",
      "loss at Epoch  13   1210.29248046875\n",
      "loss at Epoch  14   1210.093994140625\n",
      "loss at Epoch  15   1210.181884765625\n",
      "loss at Epoch  16   1210.303466796875\n",
      "loss at Epoch  17   1210.35009765625\n",
      "loss at Epoch  18   1210.4041748046875\n",
      "loss at Epoch  19   1210.464599609375\n",
      "loss at Epoch  20   1210.559814453125\n",
      "loss at Epoch  21   1210.497314453125\n",
      "loss at Epoch  22   1210.608154296875\n",
      "loss at Epoch  23   1210.665771484375\n",
      "loss at Epoch  24   1210.67236328125\n",
      "loss at Epoch  25   1210.6982421875\n",
      "loss at Epoch  26   1210.70556640625\n",
      "loss at Epoch  27   1210.74609375\n",
      "loss at Epoch  28   1210.7266845703125\n",
      "loss at Epoch  29   1210.7371826171875\n",
      "loss at Epoch  30   1210.724365234375\n",
      "loss at Epoch  31   1210.7705078125\n",
      "loss at Epoch  32   1210.776123046875\n",
      "loss at Epoch  33   1210.770751953125\n",
      "loss at Epoch  34   1210.802001953125\n",
      "loss at Epoch  35   1210.8111572265625\n",
      "loss at Epoch  36   1210.8321533203125\n",
      "loss at Epoch  37   1210.8336181640625\n",
      "loss at Epoch  38   1210.830078125\n",
      "loss at Epoch  39   1210.83837890625\n",
      "loss at Epoch  0   1386.137939453125\n",
      "loss at Epoch  1   1386.4781494140625\n",
      "loss at Epoch  2   1386.846923828125\n",
      "loss at Epoch  3   1387.250732421875\n",
      "loss at Epoch  4   1387.607666015625\n",
      "loss at Epoch  5   1387.93359375\n",
      "loss at Epoch  6   1388.2177734375\n",
      "loss at Epoch  7   1388.47412109375\n",
      "loss at Epoch  8   1388.6842041015625\n",
      "loss at Epoch  9   1388.8896484375\n",
      "loss at Epoch  10   1389.0625\n",
      "loss at Epoch  11   1389.2137451171875\n",
      "loss at Epoch  12   1389.3477783203125\n",
      "loss at Epoch  13   1389.474365234375\n",
      "loss at Epoch  14   1389.58642578125\n",
      "loss at Epoch  15   1389.6798095703125\n",
      "loss at Epoch  16   1389.7666015625\n",
      "loss at Epoch  17   1389.84375\n",
      "loss at Epoch  18   1389.911865234375\n",
      "loss at Epoch  19   1389.97314453125\n",
      "loss at Epoch  20   1390.0302734375\n",
      "loss at Epoch  21   1390.07666015625\n",
      "loss at Epoch  22   1390.119873046875\n",
      "loss at Epoch  23   1390.16064453125\n",
      "loss at Epoch  24   1390.1964111328125\n",
      "loss at Epoch  25   1390.228271484375\n",
      "loss at Epoch  26   1390.256103515625\n",
      "loss at Epoch  27   1390.281494140625\n",
      "loss at Epoch  28   1390.30419921875\n",
      "loss at Epoch  29   1390.32421875\n",
      "loss at Epoch  30   1390.3436279296875\n",
      "loss at Epoch  31   1390.36083984375\n",
      "loss at Epoch  32   1390.37548828125\n",
      "loss at Epoch  33   1390.388427734375\n",
      "loss at Epoch  34   1390.400390625\n",
      "loss at Epoch  35   1390.41162109375\n",
      "loss at Epoch  36   1390.42138671875\n",
      "loss at Epoch  37   1390.430419921875\n",
      "loss at Epoch  38   1390.438232421875\n",
      "loss at Epoch  39   1390.445556640625\n",
      "loss at Epoch  0   1224.1810302734375\n",
      "loss at Epoch  1   1221.481689453125\n",
      "loss at Epoch  2   1222.09375\n",
      "loss at Epoch  3   1220.5435791015625\n",
      "loss at Epoch  4   1220.173095703125\n",
      "loss at Epoch  5   1220.0126953125\n",
      "loss at Epoch  6   1220.647705078125\n",
      "loss at Epoch  7   1220.587646484375\n",
      "loss at Epoch  8   1221.0068359375\n",
      "loss at Epoch  9   1221.360595703125\n",
      "loss at Epoch  10   1221.0740966796875\n",
      "loss at Epoch  11   1221.343994140625\n",
      "loss at Epoch  12   1221.281494140625\n",
      "loss at Epoch  13   1221.387939453125\n",
      "loss at Epoch  14   1221.511962890625\n",
      "loss at Epoch  15   1221.44775390625\n",
      "loss at Epoch  16   1221.5506591796875\n",
      "loss at Epoch  17   1221.64794921875\n",
      "loss at Epoch  18   1221.6953125\n",
      "loss at Epoch  19   1221.697998046875\n",
      "loss at Epoch  20   1221.802001953125\n",
      "loss at Epoch  21   1221.90771484375\n",
      "loss at Epoch  22   1221.89794921875\n",
      "loss at Epoch  23   1221.906005859375\n",
      "loss at Epoch  24   1221.92578125\n",
      "loss at Epoch  25   1221.9810791015625\n",
      "loss at Epoch  26   1221.9952392578125\n",
      "loss at Epoch  27   1222.0404052734375\n",
      "loss at Epoch  28   1222.0380859375\n",
      "loss at Epoch  29   1222.0323486328125\n",
      "loss at Epoch  30   1222.0667724609375\n",
      "loss at Epoch  31   1222.068115234375\n",
      "loss at Epoch  32   1222.0780029296875\n",
      "loss at Epoch  33   1222.096435546875\n",
      "loss at Epoch  34   1222.1160888671875\n",
      "loss at Epoch  35   1222.1107177734375\n",
      "loss at Epoch  36   1222.1080322265625\n",
      "loss at Epoch  37   1222.12451171875\n",
      "loss at Epoch  38   1222.137451171875\n",
      "loss at Epoch  39   1222.16015625\n",
      "loss at Epoch  0   1374.12939453125\n",
      "loss at Epoch  1   1374.326904296875\n",
      "loss at Epoch  2   1372.703857421875\n",
      "loss at Epoch  3   1373.25048828125\n",
      "loss at Epoch  4   1372.87841796875\n",
      "loss at Epoch  5   1372.8980712890625\n",
      "loss at Epoch  6   1373.1063232421875\n",
      "loss at Epoch  7   1372.8525390625\n",
      "loss at Epoch  8   1372.9306640625\n",
      "loss at Epoch  9   1373.182861328125\n",
      "loss at Epoch  10   1373.20166015625\n",
      "loss at Epoch  11   1372.8236083984375\n",
      "loss at Epoch  12   1373.21435546875\n",
      "loss at Epoch  13   1373.229248046875\n",
      "loss at Epoch  14   1373.3175048828125\n",
      "loss at Epoch  15   1373.3424072265625\n",
      "loss at Epoch  16   1373.3900146484375\n",
      "loss at Epoch  17   1373.3310546875\n",
      "loss at Epoch  18   1373.4390869140625\n",
      "loss at Epoch  19   1373.36328125\n",
      "loss at Epoch  20   1373.4776611328125\n",
      "loss at Epoch  21   1373.49267578125\n",
      "loss at Epoch  22   1373.5828857421875\n",
      "loss at Epoch  23   1373.5762939453125\n",
      "loss at Epoch  24   1373.57958984375\n",
      "loss at Epoch  25   1373.610595703125\n",
      "loss at Epoch  26   1373.7139892578125\n",
      "loss at Epoch  27   1373.6748046875\n",
      "loss at Epoch  28   1373.692626953125\n",
      "loss at Epoch  29   1373.6845703125\n",
      "loss at Epoch  30   1373.692626953125\n",
      "loss at Epoch  31   1373.720947265625\n",
      "loss at Epoch  32   1373.733154296875\n",
      "loss at Epoch  33   1373.731201171875\n",
      "loss at Epoch  34   1373.7435302734375\n",
      "loss at Epoch  35   1373.7607421875\n",
      "loss at Epoch  36   1373.76806640625\n",
      "loss at Epoch  37   1373.77587890625\n",
      "loss at Epoch  38   1373.775634765625\n",
      "loss at Epoch  39   1373.78515625\n",
      "loss at Epoch  0   1264.5047607421875\n",
      "loss at Epoch  1   1262.86767578125\n",
      "loss at Epoch  2   1262.9827880859375\n",
      "loss at Epoch  3   1261.0185546875\n",
      "loss at Epoch  4   1262.385498046875\n",
      "loss at Epoch  5   1262.40576171875\n",
      "loss at Epoch  6   1262.40625\n",
      "loss at Epoch  7   1261.917236328125\n",
      "loss at Epoch  8   1262.53662109375\n",
      "loss at Epoch  9   1263.172119140625\n",
      "loss at Epoch  10   1263.04638671875\n",
      "loss at Epoch  11   1263.0107421875\n",
      "loss at Epoch  12   1263.0087890625\n",
      "loss at Epoch  13   1263.0445556640625\n",
      "loss at Epoch  14   1263.3355712890625\n",
      "loss at Epoch  15   1263.419921875\n",
      "loss at Epoch  16   1263.4066162109375\n",
      "loss at Epoch  17   1263.4376220703125\n",
      "loss at Epoch  18   1263.418701171875\n",
      "loss at Epoch  19   1263.33837890625\n",
      "loss at Epoch  20   1263.471435546875\n",
      "loss at Epoch  21   1263.494384765625\n",
      "loss at Epoch  22   1263.468994140625\n",
      "loss at Epoch  23   1263.54931640625\n",
      "loss at Epoch  24   1263.565185546875\n",
      "loss at Epoch  25   1263.5987548828125\n",
      "loss at Epoch  26   1263.609375\n",
      "loss at Epoch  27   1263.6072998046875\n",
      "loss at Epoch  28   1263.607666015625\n",
      "loss at Epoch  29   1263.643798828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  30   1263.670166015625\n",
      "loss at Epoch  31   1263.6904296875\n",
      "loss at Epoch  32   1263.690673828125\n",
      "loss at Epoch  33   1263.6806640625\n",
      "loss at Epoch  34   1263.70556640625\n",
      "loss at Epoch  35   1263.7158203125\n",
      "loss at Epoch  36   1263.708740234375\n",
      "loss at Epoch  37   1263.70751953125\n",
      "loss at Epoch  38   1263.71533203125\n",
      "loss at Epoch  39   1263.731201171875\n",
      "loss at Epoch  0   1195.9232177734375\n",
      "loss at Epoch  1   1193.703857421875\n",
      "loss at Epoch  2   1194.5068359375\n",
      "loss at Epoch  3   1193.9451904296875\n",
      "loss at Epoch  4   1193.6380615234375\n",
      "loss at Epoch  5   1193.744384765625\n",
      "loss at Epoch  6   1193.6885986328125\n",
      "loss at Epoch  7   1194.039794921875\n",
      "loss at Epoch  8   1194.0751953125\n",
      "loss at Epoch  9   1194.1494140625\n",
      "loss at Epoch  10   1194.192138671875\n",
      "loss at Epoch  11   1194.0772705078125\n",
      "loss at Epoch  12   1194.27783203125\n",
      "loss at Epoch  13   1194.440673828125\n",
      "loss at Epoch  14   1194.364501953125\n",
      "loss at Epoch  15   1194.530029296875\n",
      "loss at Epoch  16   1194.527099609375\n",
      "loss at Epoch  17   1194.5849609375\n",
      "loss at Epoch  18   1194.69189453125\n",
      "loss at Epoch  19   1194.695556640625\n",
      "loss at Epoch  20   1194.71630859375\n",
      "loss at Epoch  21   1194.70556640625\n",
      "loss at Epoch  22   1194.720458984375\n",
      "loss at Epoch  23   1194.798095703125\n",
      "loss at Epoch  24   1194.79443359375\n",
      "loss at Epoch  25   1194.79296875\n",
      "loss at Epoch  26   1194.8228759765625\n",
      "loss at Epoch  27   1194.8427734375\n",
      "loss at Epoch  28   1194.8568115234375\n",
      "loss at Epoch  29   1194.878173828125\n",
      "loss at Epoch  30   1194.90478515625\n",
      "loss at Epoch  31   1194.90185546875\n",
      "loss at Epoch  32   1194.91796875\n",
      "loss at Epoch  33   1194.9390869140625\n",
      "loss at Epoch  34   1194.950927734375\n",
      "loss at Epoch  35   1194.9642333984375\n",
      "loss at Epoch  36   1194.9578857421875\n",
      "loss at Epoch  37   1194.9737548828125\n",
      "loss at Epoch  38   1194.96484375\n",
      "loss at Epoch  39   1194.963134765625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1208.9368896484375\n",
      "loss at Epoch  1   1209.514892578125\n",
      "loss at Epoch  2   1209.28759765625\n",
      "loss at Epoch  3   1210.2431640625\n",
      "loss at Epoch  4   1210.087646484375\n",
      "loss at Epoch  5   1210.16845703125\n",
      "loss at Epoch  6   1210.81591796875\n",
      "loss at Epoch  7   1211.17578125\n",
      "loss at Epoch  8   1210.7431640625\n",
      "loss at Epoch  9   1211.038818359375\n",
      "loss at Epoch  10   1211.18896484375\n",
      "loss at Epoch  11   1211.1817626953125\n",
      "loss at Epoch  12   1211.55322265625\n",
      "loss at Epoch  13   1211.270751953125\n",
      "loss at Epoch  14   1211.54443359375\n",
      "loss at Epoch  15   1211.663818359375\n",
      "loss at Epoch  16   1211.873779296875\n",
      "loss at Epoch  17   1211.70654296875\n",
      "loss at Epoch  18   1211.68896484375\n",
      "loss at Epoch  19   1211.7310791015625\n",
      "loss at Epoch  20   1211.8656005859375\n",
      "loss at Epoch  21   1211.910400390625\n",
      "loss at Epoch  22   1211.91259765625\n",
      "loss at Epoch  23   1211.951416015625\n",
      "loss at Epoch  24   1211.9434814453125\n",
      "loss at Epoch  25   1211.9580078125\n",
      "loss at Epoch  26   1211.9990234375\n",
      "loss at Epoch  27   1212.02587890625\n",
      "loss at Epoch  28   1212.023681640625\n",
      "loss at Epoch  29   1212.028076171875\n",
      "loss at Epoch  30   1212.038330078125\n",
      "loss at Epoch  31   1212.0404052734375\n",
      "loss at Epoch  32   1212.074951171875\n",
      "loss at Epoch  33   1212.099365234375\n",
      "loss at Epoch  34   1212.10107421875\n",
      "loss at Epoch  35   1212.096923828125\n",
      "loss at Epoch  36   1212.1114501953125\n",
      "loss at Epoch  37   1212.11669921875\n",
      "loss at Epoch  38   1212.1143798828125\n",
      "loss at Epoch  39   1212.1212158203125\n",
      "loss at Epoch  0   1329.612548828125\n",
      "loss at Epoch  1   1329.0087890625\n",
      "loss at Epoch  2   1327.18017578125\n",
      "loss at Epoch  3   1328.3748779296875\n",
      "loss at Epoch  4   1328.116943359375\n",
      "loss at Epoch  5   1326.545166015625\n",
      "loss at Epoch  6   1327.53955078125\n",
      "loss at Epoch  7   1327.051025390625\n",
      "loss at Epoch  8   1326.4976806640625\n",
      "loss at Epoch  9   1327.669189453125\n",
      "loss at Epoch  10   1327.396728515625\n",
      "loss at Epoch  11   1327.4327392578125\n",
      "loss at Epoch  12   1327.4022216796875\n",
      "loss at Epoch  13   1327.408447265625\n",
      "loss at Epoch  14   1327.57373046875\n",
      "loss at Epoch  15   1327.706787109375\n",
      "loss at Epoch  16   1327.5689697265625\n",
      "loss at Epoch  17   1327.63671875\n",
      "loss at Epoch  18   1327.7291259765625\n",
      "loss at Epoch  19   1327.810546875\n",
      "loss at Epoch  20   1327.921142578125\n",
      "loss at Epoch  21   1328.161865234375\n",
      "loss at Epoch  22   1328.10546875\n",
      "loss at Epoch  23   1328.019775390625\n",
      "loss at Epoch  24   1328.012939453125\n",
      "loss at Epoch  25   1327.998291015625\n",
      "loss at Epoch  26   1327.97314453125\n",
      "loss at Epoch  27   1328.042724609375\n",
      "loss at Epoch  28   1328.1158447265625\n",
      "loss at Epoch  29   1328.114990234375\n",
      "loss at Epoch  30   1328.0858154296875\n",
      "loss at Epoch  31   1328.07763671875\n",
      "loss at Epoch  32   1328.09912109375\n",
      "loss at Epoch  33   1328.1275634765625\n",
      "loss at Epoch  34   1328.12841796875\n",
      "loss at Epoch  35   1328.11572265625\n",
      "loss at Epoch  36   1328.14501953125\n",
      "loss at Epoch  37   1328.150146484375\n",
      "loss at Epoch  38   1328.1474609375\n",
      "loss at Epoch  39   1328.148193359375\n",
      "loss at Epoch  0   1282.256591796875\n",
      "loss at Epoch  1   1282.648681640625\n",
      "loss at Epoch  2   1282.8212890625\n",
      "loss at Epoch  3   1281.632568359375\n",
      "loss at Epoch  4   1281.2982177734375\n",
      "loss at Epoch  5   1280.894775390625\n",
      "loss at Epoch  6   1280.5684814453125\n",
      "loss at Epoch  7   1281.314697265625\n",
      "loss at Epoch  8   1281.28271484375\n",
      "loss at Epoch  9   1281.593994140625\n",
      "loss at Epoch  10   1281.4013671875\n",
      "loss at Epoch  11   1281.8070068359375\n",
      "loss at Epoch  12   1281.7596435546875\n",
      "loss at Epoch  13   1281.8408203125\n",
      "loss at Epoch  14   1281.8499755859375\n",
      "loss at Epoch  15   1281.7696533203125\n",
      "loss at Epoch  16   1281.80322265625\n",
      "loss at Epoch  17   1281.9334716796875\n",
      "loss at Epoch  18   1281.997314453125\n",
      "loss at Epoch  19   1281.96923828125\n",
      "loss at Epoch  20   1282.0244140625\n",
      "loss at Epoch  21   1282.085693359375\n",
      "loss at Epoch  22   1282.1153564453125\n",
      "loss at Epoch  23   1282.164794921875\n",
      "loss at Epoch  24   1282.1483154296875\n",
      "loss at Epoch  25   1282.151123046875\n",
      "loss at Epoch  26   1282.18310546875\n",
      "loss at Epoch  27   1282.21337890625\n",
      "loss at Epoch  28   1282.219482421875\n",
      "loss at Epoch  29   1282.230224609375\n",
      "loss at Epoch  30   1282.251953125\n",
      "loss at Epoch  31   1282.2666015625\n",
      "loss at Epoch  32   1282.260498046875\n",
      "loss at Epoch  33   1282.25390625\n",
      "loss at Epoch  34   1282.258056640625\n",
      "loss at Epoch  35   1282.26318359375\n",
      "loss at Epoch  36   1282.28369140625\n",
      "loss at Epoch  37   1282.296630859375\n",
      "loss at Epoch  38   1282.29833984375\n",
      "loss at Epoch  39   1282.304443359375\n",
      "loss at Epoch  0   1216.3927001953125\n",
      "loss at Epoch  1   1214.760009765625\n",
      "loss at Epoch  2   1215.03662109375\n",
      "loss at Epoch  3   1215.030517578125\n",
      "loss at Epoch  4   1214.528076171875\n",
      "loss at Epoch  5   1214.540771484375\n",
      "loss at Epoch  6   1214.2412109375\n",
      "loss at Epoch  7   1214.849365234375\n",
      "loss at Epoch  8   1214.7578125\n",
      "loss at Epoch  9   1214.9404296875\n",
      "loss at Epoch  10   1215.2197265625\n",
      "loss at Epoch  11   1214.9532470703125\n",
      "loss at Epoch  12   1215.30224609375\n",
      "loss at Epoch  13   1215.591552734375\n",
      "loss at Epoch  14   1215.5931396484375\n",
      "loss at Epoch  15   1215.56298828125\n",
      "loss at Epoch  16   1215.5625\n",
      "loss at Epoch  17   1215.669677734375\n",
      "loss at Epoch  18   1215.731201171875\n",
      "loss at Epoch  19   1215.7476806640625\n",
      "loss at Epoch  20   1215.8531494140625\n",
      "loss at Epoch  21   1215.8702392578125\n",
      "loss at Epoch  22   1215.8851318359375\n",
      "loss at Epoch  23   1215.947509765625\n",
      "loss at Epoch  24   1215.97802734375\n",
      "loss at Epoch  25   1215.9869384765625\n",
      "loss at Epoch  26   1216.0380859375\n",
      "loss at Epoch  27   1216.0465087890625\n",
      "loss at Epoch  28   1216.0703125\n",
      "loss at Epoch  29   1216.06689453125\n",
      "loss at Epoch  30   1216.1124267578125\n",
      "loss at Epoch  31   1216.120849609375\n",
      "loss at Epoch  32   1216.1317138671875\n",
      "loss at Epoch  33   1216.142822265625\n",
      "loss at Epoch  34   1216.161865234375\n",
      "loss at Epoch  35   1216.1697998046875\n",
      "loss at Epoch  36   1216.156982421875\n",
      "loss at Epoch  37   1216.1617431640625\n",
      "loss at Epoch  38   1216.174560546875\n",
      "loss at Epoch  39   1216.1768798828125\n",
      "loss at Epoch  0   1530.51708984375\n",
      "loss at Epoch  1   1530.9891357421875\n",
      "loss at Epoch  2   1528.00390625\n",
      "loss at Epoch  3   1529.3114013671875\n",
      "loss at Epoch  4   1528.4356689453125\n",
      "loss at Epoch  5   1528.181396484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  6   1528.5433349609375\n",
      "loss at Epoch  7   1528.6744384765625\n",
      "loss at Epoch  8   1528.682861328125\n",
      "loss at Epoch  9   1529.043212890625\n",
      "loss at Epoch  10   1529.15576171875\n",
      "loss at Epoch  11   1528.964599609375\n",
      "loss at Epoch  12   1529.159912109375\n",
      "loss at Epoch  13   1529.2017822265625\n",
      "loss at Epoch  14   1529.29150390625\n",
      "loss at Epoch  15   1529.214599609375\n",
      "loss at Epoch  16   1529.297119140625\n",
      "loss at Epoch  17   1529.25244140625\n",
      "loss at Epoch  18   1529.3323974609375\n",
      "loss at Epoch  19   1529.4063720703125\n",
      "loss at Epoch  20   1529.5218505859375\n",
      "loss at Epoch  21   1529.5440673828125\n",
      "loss at Epoch  22   1529.4239501953125\n",
      "loss at Epoch  23   1529.50830078125\n",
      "loss at Epoch  24   1529.5469970703125\n",
      "loss at Epoch  25   1529.622314453125\n",
      "loss at Epoch  26   1529.54833984375\n",
      "loss at Epoch  27   1529.6011962890625\n",
      "loss at Epoch  28   1529.636962890625\n",
      "loss at Epoch  29   1529.67724609375\n",
      "loss at Epoch  30   1529.6546630859375\n",
      "loss at Epoch  31   1529.630615234375\n",
      "loss at Epoch  32   1529.643798828125\n",
      "loss at Epoch  33   1529.7132568359375\n",
      "loss at Epoch  34   1529.695068359375\n",
      "loss at Epoch  35   1529.6806640625\n",
      "loss at Epoch  36   1529.7008056640625\n",
      "loss at Epoch  37   1529.6934814453125\n",
      "loss at Epoch  38   1529.7020263671875\n",
      "loss at Epoch  39   1529.7071533203125\n",
      "loss at Epoch  0   1323.27294921875\n",
      "loss at Epoch  1   1321.286376953125\n",
      "loss at Epoch  2   1322.33349609375\n",
      "loss at Epoch  3   1321.5677490234375\n",
      "loss at Epoch  4   1320.7572021484375\n",
      "loss at Epoch  5   1320.4696044921875\n",
      "loss at Epoch  6   1320.9544677734375\n",
      "loss at Epoch  7   1320.873291015625\n",
      "loss at Epoch  8   1321.4061279296875\n",
      "loss at Epoch  9   1321.296142578125\n",
      "loss at Epoch  10   1321.607666015625\n",
      "loss at Epoch  11   1321.3729248046875\n",
      "loss at Epoch  12   1321.140869140625\n",
      "loss at Epoch  13   1321.6119384765625\n",
      "loss at Epoch  14   1321.57421875\n",
      "loss at Epoch  15   1321.40283203125\n",
      "loss at Epoch  16   1321.49755859375\n",
      "loss at Epoch  17   1321.501708984375\n",
      "loss at Epoch  18   1321.66845703125\n",
      "loss at Epoch  19   1321.60400390625\n",
      "loss at Epoch  20   1321.5634765625\n",
      "loss at Epoch  21   1321.538818359375\n",
      "loss at Epoch  22   1321.577392578125\n",
      "loss at Epoch  23   1321.601806640625\n",
      "loss at Epoch  24   1321.6142578125\n",
      "loss at Epoch  25   1321.6396484375\n",
      "loss at Epoch  26   1321.6473388671875\n",
      "loss at Epoch  27   1321.6346435546875\n",
      "loss at Epoch  28   1321.6512451171875\n",
      "loss at Epoch  29   1321.6708984375\n",
      "loss at Epoch  30   1321.678466796875\n",
      "loss at Epoch  31   1321.6796875\n",
      "loss at Epoch  32   1321.673583984375\n",
      "loss at Epoch  33   1321.7021484375\n",
      "loss at Epoch  34   1321.720947265625\n",
      "loss at Epoch  35   1321.72021484375\n",
      "loss at Epoch  36   1321.716064453125\n",
      "loss at Epoch  37   1321.73388671875\n",
      "loss at Epoch  38   1321.734130859375\n",
      "loss at Epoch  39   1321.72998046875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1237.6925048828125\n",
      "loss at Epoch  1   1237.7794189453125\n",
      "loss at Epoch  2   1237.6064453125\n",
      "loss at Epoch  3   1239.13037109375\n",
      "loss at Epoch  4   1238.099609375\n",
      "loss at Epoch  5   1238.03955078125\n",
      "loss at Epoch  6   1239.26806640625\n",
      "loss at Epoch  7   1239.123046875\n",
      "loss at Epoch  8   1239.101318359375\n",
      "loss at Epoch  9   1239.2734375\n",
      "loss at Epoch  10   1239.458251953125\n",
      "loss at Epoch  11   1239.82275390625\n",
      "loss at Epoch  12   1239.5008544921875\n",
      "loss at Epoch  13   1239.7264404296875\n",
      "loss at Epoch  14   1239.8785400390625\n",
      "loss at Epoch  15   1239.777587890625\n",
      "loss at Epoch  16   1239.83056640625\n",
      "loss at Epoch  17   1239.83154296875\n",
      "loss at Epoch  18   1239.9761962890625\n",
      "loss at Epoch  19   1239.986328125\n",
      "loss at Epoch  20   1240.0814208984375\n",
      "loss at Epoch  21   1240.118408203125\n",
      "loss at Epoch  22   1240.09716796875\n",
      "loss at Epoch  23   1240.174560546875\n",
      "loss at Epoch  24   1240.1971435546875\n",
      "loss at Epoch  25   1240.22705078125\n",
      "loss at Epoch  26   1240.25048828125\n",
      "loss at Epoch  27   1240.2613525390625\n",
      "loss at Epoch  28   1240.265380859375\n",
      "loss at Epoch  29   1240.290283203125\n",
      "loss at Epoch  30   1240.32421875\n",
      "loss at Epoch  31   1240.347412109375\n",
      "loss at Epoch  32   1240.35888671875\n",
      "loss at Epoch  33   1240.3604736328125\n",
      "loss at Epoch  34   1240.357666015625\n",
      "loss at Epoch  35   1240.3680419921875\n",
      "loss at Epoch  36   1240.3697509765625\n",
      "loss at Epoch  37   1240.3828125\n",
      "loss at Epoch  38   1240.3846435546875\n",
      "loss at Epoch  39   1240.3978271484375\n",
      "loss at Epoch  0   1261.5185546875\n",
      "loss at Epoch  1   1261.7763671875\n",
      "loss at Epoch  2   1260.1600341796875\n",
      "loss at Epoch  3   1259.64306640625\n",
      "loss at Epoch  4   1259.6649169921875\n",
      "loss at Epoch  5   1260.036865234375\n",
      "loss at Epoch  6   1260.184814453125\n",
      "loss at Epoch  7   1260.38037109375\n",
      "loss at Epoch  8   1260.337158203125\n",
      "loss at Epoch  9   1260.603759765625\n",
      "loss at Epoch  10   1260.758056640625\n",
      "loss at Epoch  11   1260.982666015625\n",
      "loss at Epoch  12   1260.9649658203125\n",
      "loss at Epoch  13   1261.0133056640625\n",
      "loss at Epoch  14   1261.125732421875\n",
      "loss at Epoch  15   1261.195068359375\n",
      "loss at Epoch  16   1261.24755859375\n",
      "loss at Epoch  17   1261.2978515625\n",
      "loss at Epoch  18   1261.361328125\n",
      "loss at Epoch  19   1261.416748046875\n",
      "loss at Epoch  20   1261.4647216796875\n",
      "loss at Epoch  21   1261.472412109375\n",
      "loss at Epoch  22   1261.55908203125\n",
      "loss at Epoch  23   1261.540771484375\n",
      "loss at Epoch  24   1261.547119140625\n",
      "loss at Epoch  25   1261.62060546875\n",
      "loss at Epoch  26   1261.6298828125\n",
      "loss at Epoch  27   1261.695068359375\n",
      "loss at Epoch  28   1261.6748046875\n",
      "loss at Epoch  29   1261.68359375\n",
      "loss at Epoch  30   1261.707275390625\n",
      "loss at Epoch  31   1261.7191162109375\n",
      "loss at Epoch  32   1261.712890625\n",
      "loss at Epoch  33   1261.7294921875\n",
      "loss at Epoch  34   1261.7255859375\n",
      "loss at Epoch  35   1261.744384765625\n",
      "loss at Epoch  36   1261.75634765625\n",
      "loss at Epoch  37   1261.7635498046875\n",
      "loss at Epoch  38   1261.770263671875\n",
      "loss at Epoch  39   1261.785888671875\n",
      "loss at Epoch  0   1378.18701171875\n",
      "loss at Epoch  1   1378.853271484375\n",
      "loss at Epoch  2   1377.7100830078125\n",
      "loss at Epoch  3   1376.79052734375\n",
      "loss at Epoch  4   1375.81787109375\n",
      "loss at Epoch  5   1376.2156982421875\n",
      "loss at Epoch  6   1375.919921875\n",
      "loss at Epoch  7   1376.33056640625\n",
      "loss at Epoch  8   1376.64111328125\n",
      "loss at Epoch  9   1376.9197998046875\n",
      "loss at Epoch  10   1377.0169677734375\n",
      "loss at Epoch  11   1377.057861328125\n",
      "loss at Epoch  12   1376.91552734375\n",
      "loss at Epoch  13   1377.064208984375\n",
      "loss at Epoch  14   1377.0140380859375\n",
      "loss at Epoch  15   1377.009033203125\n",
      "loss at Epoch  16   1377.094482421875\n",
      "loss at Epoch  17   1377.305419921875\n",
      "loss at Epoch  18   1377.259765625\n",
      "loss at Epoch  19   1377.26513671875\n",
      "loss at Epoch  20   1377.3173828125\n",
      "loss at Epoch  21   1377.336181640625\n",
      "loss at Epoch  22   1377.345947265625\n",
      "loss at Epoch  23   1377.418701171875\n",
      "loss at Epoch  24   1377.37646484375\n",
      "loss at Epoch  25   1377.431396484375\n",
      "loss at Epoch  26   1377.4554443359375\n",
      "loss at Epoch  27   1377.506591796875\n",
      "loss at Epoch  28   1377.49658203125\n",
      "loss at Epoch  29   1377.55322265625\n",
      "loss at Epoch  30   1377.5233154296875\n",
      "loss at Epoch  31   1377.514404296875\n",
      "loss at Epoch  32   1377.5240478515625\n",
      "loss at Epoch  33   1377.54736328125\n",
      "loss at Epoch  34   1377.528564453125\n",
      "loss at Epoch  35   1377.55029296875\n",
      "loss at Epoch  36   1377.54736328125\n",
      "loss at Epoch  37   1377.5537109375\n",
      "loss at Epoch  38   1377.5611572265625\n",
      "loss at Epoch  39   1377.569091796875\n",
      "loss at Epoch  0   1221.874267578125\n",
      "loss at Epoch  1   1222.9412841796875\n",
      "loss at Epoch  2   1221.87646484375\n",
      "loss at Epoch  3   1220.35888671875\n",
      "loss at Epoch  4   1221.007080078125\n",
      "loss at Epoch  5   1220.620361328125\n",
      "loss at Epoch  6   1220.6695556640625\n",
      "loss at Epoch  7   1220.84228515625\n",
      "loss at Epoch  8   1221.15673828125\n",
      "loss at Epoch  9   1221.12109375\n",
      "loss at Epoch  10   1221.22802734375\n",
      "loss at Epoch  11   1221.411376953125\n",
      "loss at Epoch  12   1221.50927734375\n",
      "loss at Epoch  13   1221.6630859375\n",
      "loss at Epoch  14   1221.7672119140625\n",
      "loss at Epoch  15   1221.736572265625\n",
      "loss at Epoch  16   1221.7406005859375\n",
      "loss at Epoch  17   1221.72900390625\n",
      "loss at Epoch  18   1221.76220703125\n",
      "loss at Epoch  19   1221.84521484375\n",
      "loss at Epoch  20   1221.8826904296875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  21   1221.966796875\n",
      "loss at Epoch  22   1222.0390625\n",
      "loss at Epoch  23   1222.0487060546875\n",
      "loss at Epoch  24   1222.1219482421875\n",
      "loss at Epoch  25   1222.1180419921875\n",
      "loss at Epoch  26   1222.1593017578125\n",
      "loss at Epoch  27   1222.170166015625\n",
      "loss at Epoch  28   1222.177978515625\n",
      "loss at Epoch  29   1222.294677734375\n",
      "loss at Epoch  30   1222.277587890625\n",
      "loss at Epoch  31   1222.227294921875\n",
      "loss at Epoch  32   1222.250244140625\n",
      "loss at Epoch  33   1222.249267578125\n",
      "loss at Epoch  34   1222.2548828125\n",
      "loss at Epoch  35   1222.2724609375\n",
      "loss at Epoch  36   1222.2762451171875\n",
      "loss at Epoch  37   1222.287841796875\n",
      "loss at Epoch  38   1222.2900390625\n",
      "loss at Epoch  39   1222.290283203125\n",
      "loss at Epoch  0   1121.92724609375\n",
      "loss at Epoch  1   1120.1265869140625\n",
      "loss at Epoch  2   1120.749267578125\n",
      "loss at Epoch  3   1119.953125\n",
      "loss at Epoch  4   1120.1558837890625\n",
      "loss at Epoch  5   1119.928955078125\n",
      "loss at Epoch  6   1120.4423828125\n",
      "loss at Epoch  7   1119.9586181640625\n",
      "loss at Epoch  8   1120.1923828125\n",
      "loss at Epoch  9   1120.138427734375\n",
      "loss at Epoch  10   1120.281494140625\n",
      "loss at Epoch  11   1120.403564453125\n",
      "loss at Epoch  12   1120.4718017578125\n",
      "loss at Epoch  13   1120.539794921875\n",
      "loss at Epoch  14   1120.7490234375\n",
      "loss at Epoch  15   1120.850830078125\n",
      "loss at Epoch  16   1120.941650390625\n",
      "loss at Epoch  17   1120.812744140625\n",
      "loss at Epoch  18   1120.8712158203125\n",
      "loss at Epoch  19   1120.897705078125\n",
      "loss at Epoch  20   1120.9595947265625\n",
      "loss at Epoch  21   1121.03515625\n",
      "loss at Epoch  22   1121.03173828125\n",
      "loss at Epoch  23   1121.06201171875\n",
      "loss at Epoch  24   1121.040283203125\n",
      "loss at Epoch  25   1121.13330078125\n",
      "loss at Epoch  26   1121.1458740234375\n",
      "loss at Epoch  27   1121.1748046875\n",
      "loss at Epoch  28   1121.1851806640625\n",
      "loss at Epoch  29   1121.2000732421875\n",
      "loss at Epoch  30   1121.206298828125\n",
      "loss at Epoch  31   1121.238037109375\n",
      "loss at Epoch  32   1121.231689453125\n",
      "loss at Epoch  33   1121.233642578125\n",
      "loss at Epoch  34   1121.26708984375\n",
      "loss at Epoch  35   1121.2620849609375\n",
      "loss at Epoch  36   1121.257080078125\n",
      "loss at Epoch  37   1121.264892578125\n",
      "loss at Epoch  38   1121.26806640625\n",
      "loss at Epoch  39   1121.2769775390625\n",
      "loss at Epoch  0   2019.245361328125\n",
      "loss at Epoch  1   2017.7010498046875\n",
      "loss at Epoch  2   2018.8773193359375\n",
      "loss at Epoch  3   2016.332275390625\n",
      "loss at Epoch  4   2016.767822265625\n",
      "loss at Epoch  5   2017.6180419921875\n",
      "loss at Epoch  6   2017.774658203125\n",
      "loss at Epoch  7   2018.5552978515625\n",
      "loss at Epoch  8   2017.475830078125\n",
      "loss at Epoch  9   2017.32080078125\n",
      "loss at Epoch  10   2017.6885986328125\n",
      "loss at Epoch  11   2017.906494140625\n",
      "loss at Epoch  12   2017.8974609375\n",
      "loss at Epoch  13   2018.015869140625\n",
      "loss at Epoch  14   2017.91748046875\n",
      "loss at Epoch  15   2018.0791015625\n",
      "loss at Epoch  16   2018.1109619140625\n",
      "loss at Epoch  17   2018.310791015625\n",
      "loss at Epoch  18   2018.2347412109375\n",
      "loss at Epoch  19   2018.0965576171875\n",
      "loss at Epoch  20   2018.2587890625\n",
      "loss at Epoch  21   2018.2891845703125\n",
      "loss at Epoch  22   2018.3487548828125\n",
      "loss at Epoch  23   2018.3626708984375\n",
      "loss at Epoch  24   2018.434814453125\n",
      "loss at Epoch  25   2018.4095458984375\n",
      "loss at Epoch  26   2018.50732421875\n",
      "loss at Epoch  27   2018.504638671875\n",
      "loss at Epoch  28   2018.54638671875\n",
      "loss at Epoch  29   2018.5419921875\n",
      "loss at Epoch  30   2018.5477294921875\n",
      "loss at Epoch  31   2018.5150146484375\n",
      "loss at Epoch  32   2018.5341796875\n",
      "loss at Epoch  33   2018.56591796875\n",
      "loss at Epoch  34   2018.585693359375\n",
      "loss at Epoch  35   2018.591552734375\n",
      "loss at Epoch  36   2018.5975341796875\n",
      "loss at Epoch  37   2018.5972900390625\n",
      "loss at Epoch  38   2018.612060546875\n",
      "loss at Epoch  39   2018.610595703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1230.273681640625\n",
      "loss at Epoch  1   1230.9296875\n",
      "loss at Epoch  2   1230.95751953125\n",
      "loss at Epoch  3   1231.7047119140625\n",
      "loss at Epoch  4   1231.14599609375\n",
      "loss at Epoch  5   1232.016357421875\n",
      "loss at Epoch  6   1232.6107177734375\n",
      "loss at Epoch  7   1232.4156494140625\n",
      "loss at Epoch  8   1232.540771484375\n",
      "loss at Epoch  9   1232.6429443359375\n",
      "loss at Epoch  10   1232.8233642578125\n",
      "loss at Epoch  11   1232.823974609375\n",
      "loss at Epoch  12   1232.9146728515625\n",
      "loss at Epoch  13   1233.021484375\n",
      "loss at Epoch  14   1232.98095703125\n",
      "loss at Epoch  15   1233.1241455078125\n",
      "loss at Epoch  16   1233.0892333984375\n",
      "loss at Epoch  17   1233.22265625\n",
      "loss at Epoch  18   1233.2919921875\n",
      "loss at Epoch  19   1233.284423828125\n",
      "loss at Epoch  20   1233.334716796875\n",
      "loss at Epoch  21   1233.353759765625\n",
      "loss at Epoch  22   1233.396728515625\n",
      "loss at Epoch  23   1233.444580078125\n",
      "loss at Epoch  24   1233.490966796875\n",
      "loss at Epoch  25   1233.528564453125\n",
      "loss at Epoch  26   1233.5731201171875\n",
      "loss at Epoch  27   1233.5595703125\n",
      "loss at Epoch  28   1233.5810546875\n",
      "loss at Epoch  29   1233.588134765625\n",
      "loss at Epoch  30   1233.60888671875\n",
      "loss at Epoch  31   1233.616943359375\n",
      "loss at Epoch  32   1233.6517333984375\n",
      "loss at Epoch  33   1233.651123046875\n",
      "loss at Epoch  34   1233.6575927734375\n",
      "loss at Epoch  35   1233.6492919921875\n",
      "loss at Epoch  36   1233.6695556640625\n",
      "loss at Epoch  37   1233.6715087890625\n",
      "loss at Epoch  38   1233.676513671875\n",
      "loss at Epoch  39   1233.6829833984375\n",
      "loss at Epoch  0   1374.180908203125\n",
      "loss at Epoch  1   1375.343017578125\n",
      "loss at Epoch  2   1372.8193359375\n",
      "loss at Epoch  3   1372.858642578125\n",
      "loss at Epoch  4   1372.2122802734375\n",
      "loss at Epoch  5   1372.705322265625\n",
      "loss at Epoch  6   1372.599609375\n",
      "loss at Epoch  7   1372.6123046875\n",
      "loss at Epoch  8   1372.81298828125\n",
      "loss at Epoch  9   1373.0811767578125\n",
      "loss at Epoch  10   1372.9757080078125\n",
      "loss at Epoch  11   1372.9793701171875\n",
      "loss at Epoch  12   1373.01123046875\n",
      "loss at Epoch  13   1373.15087890625\n",
      "loss at Epoch  14   1373.234619140625\n",
      "loss at Epoch  15   1373.4034423828125\n",
      "loss at Epoch  16   1373.302978515625\n",
      "loss at Epoch  17   1373.3656005859375\n",
      "loss at Epoch  18   1373.366455078125\n",
      "loss at Epoch  19   1373.420166015625\n",
      "loss at Epoch  20   1373.392822265625\n",
      "loss at Epoch  21   1373.5364990234375\n",
      "loss at Epoch  22   1373.5325927734375\n",
      "loss at Epoch  23   1373.554931640625\n",
      "loss at Epoch  24   1373.5528564453125\n",
      "loss at Epoch  25   1373.5626220703125\n",
      "loss at Epoch  26   1373.57275390625\n",
      "loss at Epoch  27   1373.600341796875\n",
      "loss at Epoch  28   1373.6220703125\n",
      "loss at Epoch  29   1373.61328125\n",
      "loss at Epoch  30   1373.6302490234375\n",
      "loss at Epoch  31   1373.63916015625\n",
      "loss at Epoch  32   1373.6409912109375\n",
      "loss at Epoch  33   1373.66650390625\n",
      "loss at Epoch  34   1373.67333984375\n",
      "loss at Epoch  35   1373.6787109375\n",
      "loss at Epoch  36   1373.6708984375\n",
      "loss at Epoch  37   1373.675537109375\n",
      "loss at Epoch  38   1373.6810302734375\n",
      "loss at Epoch  39   1373.683837890625\n",
      "loss at Epoch  0   1215.19189453125\n",
      "loss at Epoch  1   1212.64306640625\n",
      "loss at Epoch  2   1213.0400390625\n",
      "loss at Epoch  3   1213.857666015625\n",
      "loss at Epoch  4   1212.344970703125\n",
      "loss at Epoch  5   1213.094482421875\n",
      "loss at Epoch  6   1213.066650390625\n",
      "loss at Epoch  7   1213.474853515625\n",
      "loss at Epoch  8   1213.330078125\n",
      "loss at Epoch  9   1213.231201171875\n",
      "loss at Epoch  10   1213.2880859375\n",
      "loss at Epoch  11   1213.1141357421875\n",
      "loss at Epoch  12   1213.52880859375\n",
      "loss at Epoch  13   1213.522216796875\n",
      "loss at Epoch  14   1213.674560546875\n",
      "loss at Epoch  15   1213.7193603515625\n",
      "loss at Epoch  16   1213.661376953125\n",
      "loss at Epoch  17   1213.639404296875\n",
      "loss at Epoch  18   1213.8441162109375\n",
      "loss at Epoch  19   1213.737548828125\n",
      "loss at Epoch  20   1213.759033203125\n",
      "loss at Epoch  21   1213.83056640625\n",
      "loss at Epoch  22   1213.8900146484375\n",
      "loss at Epoch  23   1213.96484375\n",
      "loss at Epoch  24   1213.93359375\n",
      "loss at Epoch  25   1213.947998046875\n",
      "loss at Epoch  26   1213.9765625\n",
      "loss at Epoch  27   1213.9765625\n",
      "loss at Epoch  28   1213.996337890625\n",
      "loss at Epoch  29   1214.007568359375\n",
      "loss at Epoch  30   1214.0196533203125\n",
      "loss at Epoch  31   1214.02783203125\n",
      "loss at Epoch  32   1214.0234375\n",
      "loss at Epoch  33   1214.041015625\n",
      "loss at Epoch  34   1214.075439453125\n",
      "loss at Epoch  35   1214.07421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  36   1214.07470703125\n",
      "loss at Epoch  37   1214.080810546875\n",
      "loss at Epoch  38   1214.0849609375\n",
      "loss at Epoch  39   1214.0906982421875\n",
      "loss at Epoch  0   1211.79150390625\n",
      "loss at Epoch  1   1209.495361328125\n",
      "loss at Epoch  2   1210.7392578125\n",
      "loss at Epoch  3   1209.020263671875\n",
      "loss at Epoch  4   1209.685546875\n",
      "loss at Epoch  5   1210.3973388671875\n",
      "loss at Epoch  6   1210.346923828125\n",
      "loss at Epoch  7   1210.1676025390625\n",
      "loss at Epoch  8   1210.1181640625\n",
      "loss at Epoch  9   1210.21484375\n",
      "loss at Epoch  10   1210.258544921875\n",
      "loss at Epoch  11   1210.0498046875\n",
      "loss at Epoch  12   1210.1649169921875\n",
      "loss at Epoch  13   1210.152099609375\n",
      "loss at Epoch  14   1210.32177734375\n",
      "loss at Epoch  15   1210.3763427734375\n",
      "loss at Epoch  16   1210.3892822265625\n",
      "loss at Epoch  17   1210.566162109375\n",
      "loss at Epoch  18   1210.499755859375\n",
      "loss at Epoch  19   1210.50634765625\n",
      "loss at Epoch  20   1210.5673828125\n",
      "loss at Epoch  21   1210.6705322265625\n",
      "loss at Epoch  22   1210.7412109375\n",
      "loss at Epoch  23   1210.7242431640625\n",
      "loss at Epoch  24   1210.73779296875\n",
      "loss at Epoch  25   1210.7010498046875\n",
      "loss at Epoch  26   1210.757568359375\n",
      "loss at Epoch  27   1210.7568359375\n",
      "loss at Epoch  28   1210.795166015625\n",
      "loss at Epoch  29   1210.825439453125\n",
      "loss at Epoch  30   1210.8330078125\n",
      "loss at Epoch  31   1210.847900390625\n",
      "loss at Epoch  32   1210.833251953125\n",
      "loss at Epoch  33   1210.8505859375\n",
      "loss at Epoch  34   1210.8616943359375\n",
      "loss at Epoch  35   1210.8643798828125\n",
      "loss at Epoch  36   1210.88916015625\n",
      "loss at Epoch  37   1210.8905029296875\n",
      "loss at Epoch  38   1210.899658203125\n",
      "loss at Epoch  39   1210.9111328125\n",
      "loss at Epoch  0   1333.353759765625\n",
      "loss at Epoch  1   1333.649169921875\n",
      "loss at Epoch  2   1333.40234375\n",
      "loss at Epoch  3   1332.325927734375\n",
      "loss at Epoch  4   1332.10986328125\n",
      "loss at Epoch  5   1331.89697265625\n",
      "loss at Epoch  6   1332.396484375\n",
      "loss at Epoch  7   1332.396728515625\n",
      "loss at Epoch  8   1332.384765625\n",
      "loss at Epoch  9   1332.2242431640625\n",
      "loss at Epoch  10   1332.083251953125\n",
      "loss at Epoch  11   1332.18212890625\n",
      "loss at Epoch  12   1332.3243408203125\n",
      "loss at Epoch  13   1332.572509765625\n",
      "loss at Epoch  14   1332.7181396484375\n",
      "loss at Epoch  15   1332.602783203125\n",
      "loss at Epoch  16   1332.693115234375\n",
      "loss at Epoch  17   1332.7235107421875\n",
      "loss at Epoch  18   1332.765625\n",
      "loss at Epoch  19   1332.9068603515625\n",
      "loss at Epoch  20   1332.8814697265625\n",
      "loss at Epoch  21   1332.84814453125\n",
      "loss at Epoch  22   1332.9185791015625\n",
      "loss at Epoch  23   1332.953857421875\n",
      "loss at Epoch  24   1332.9842529296875\n",
      "loss at Epoch  25   1332.9483642578125\n",
      "loss at Epoch  26   1332.9852294921875\n",
      "loss at Epoch  27   1333.001953125\n",
      "loss at Epoch  28   1333.0103759765625\n",
      "loss at Epoch  29   1333.0289306640625\n",
      "loss at Epoch  30   1333.031494140625\n",
      "loss at Epoch  31   1333.048828125\n",
      "loss at Epoch  32   1333.0543212890625\n",
      "loss at Epoch  33   1333.084228515625\n",
      "loss at Epoch  34   1333.07177734375\n",
      "loss at Epoch  35   1333.088134765625\n",
      "loss at Epoch  36   1333.09423828125\n",
      "loss at Epoch  37   1333.09814453125\n",
      "loss at Epoch  38   1333.100830078125\n",
      "loss at Epoch  39   1333.111572265625\n",
      "loss at Epoch  0   1194.86279296875\n",
      "loss at Epoch  1   1193.376708984375\n",
      "loss at Epoch  2   1194.14697265625\n",
      "loss at Epoch  3   1193.7210693359375\n",
      "loss at Epoch  4   1192.802978515625\n",
      "loss at Epoch  5   1193.6956787109375\n",
      "loss at Epoch  6   1193.094970703125\n",
      "loss at Epoch  7   1193.166748046875\n",
      "loss at Epoch  8   1193.262939453125\n",
      "loss at Epoch  9   1193.4967041015625\n",
      "loss at Epoch  10   1193.499267578125\n",
      "loss at Epoch  11   1193.7314453125\n",
      "loss at Epoch  12   1193.742919921875\n",
      "loss at Epoch  13   1193.85693359375\n",
      "loss at Epoch  14   1194.052001953125\n",
      "loss at Epoch  15   1193.9254150390625\n",
      "loss at Epoch  16   1194.077392578125\n",
      "loss at Epoch  17   1194.197265625\n",
      "loss at Epoch  18   1194.152099609375\n",
      "loss at Epoch  19   1194.15185546875\n",
      "loss at Epoch  20   1194.276123046875\n",
      "loss at Epoch  21   1194.2889404296875\n",
      "loss at Epoch  22   1194.299072265625\n",
      "loss at Epoch  23   1194.26953125\n",
      "loss at Epoch  24   1194.3275146484375\n",
      "loss at Epoch  25   1194.374755859375\n",
      "loss at Epoch  26   1194.3826904296875\n",
      "loss at Epoch  27   1194.405517578125\n",
      "loss at Epoch  28   1194.4237060546875\n",
      "loss at Epoch  29   1194.442138671875\n",
      "loss at Epoch  30   1194.450927734375\n",
      "loss at Epoch  31   1194.447265625\n",
      "loss at Epoch  32   1194.4736328125\n",
      "loss at Epoch  33   1194.48681640625\n",
      "loss at Epoch  34   1194.502197265625\n",
      "loss at Epoch  35   1194.502685546875\n",
      "loss at Epoch  36   1194.516845703125\n",
      "loss at Epoch  37   1194.513427734375\n",
      "loss at Epoch  38   1194.51708984375\n",
      "loss at Epoch  39   1194.5360107421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1230.8275146484375\n",
      "loss at Epoch  1   1231.5711669921875\n",
      "loss at Epoch  2   1231.7662353515625\n",
      "loss at Epoch  3   1231.6767578125\n",
      "loss at Epoch  4   1232.421142578125\n",
      "loss at Epoch  5   1232.579345703125\n",
      "loss at Epoch  6   1233.25390625\n",
      "loss at Epoch  7   1233.4456787109375\n",
      "loss at Epoch  8   1232.92529296875\n",
      "loss at Epoch  9   1233.336181640625\n",
      "loss at Epoch  10   1233.505615234375\n",
      "loss at Epoch  11   1233.77685546875\n",
      "loss at Epoch  12   1233.90869140625\n",
      "loss at Epoch  13   1234.01025390625\n",
      "loss at Epoch  14   1233.8798828125\n",
      "loss at Epoch  15   1233.9432373046875\n",
      "loss at Epoch  16   1234.2088623046875\n",
      "loss at Epoch  17   1234.15478515625\n",
      "loss at Epoch  18   1234.146728515625\n",
      "loss at Epoch  19   1234.203369140625\n",
      "loss at Epoch  20   1234.205322265625\n",
      "loss at Epoch  21   1234.2552490234375\n",
      "loss at Epoch  22   1234.324462890625\n",
      "loss at Epoch  23   1234.3636474609375\n",
      "loss at Epoch  24   1234.375\n",
      "loss at Epoch  25   1234.374755859375\n",
      "loss at Epoch  26   1234.3990478515625\n",
      "loss at Epoch  27   1234.407958984375\n",
      "loss at Epoch  28   1234.437255859375\n",
      "loss at Epoch  29   1234.458740234375\n",
      "loss at Epoch  30   1234.4521484375\n",
      "loss at Epoch  31   1234.4615478515625\n",
      "loss at Epoch  32   1234.4755859375\n",
      "loss at Epoch  33   1234.473876953125\n",
      "loss at Epoch  34   1234.483642578125\n",
      "loss at Epoch  35   1234.48388671875\n",
      "loss at Epoch  36   1234.5\n",
      "loss at Epoch  37   1234.50146484375\n",
      "loss at Epoch  38   1234.51025390625\n",
      "loss at Epoch  39   1234.513916015625\n",
      "loss at Epoch  0   1323.385986328125\n",
      "loss at Epoch  1   1322.453125\n",
      "loss at Epoch  2   1320.8240966796875\n",
      "loss at Epoch  3   1320.58056640625\n",
      "loss at Epoch  4   1319.830322265625\n",
      "loss at Epoch  5   1320.4007568359375\n",
      "loss at Epoch  6   1320.322509765625\n",
      "loss at Epoch  7   1320.092529296875\n",
      "loss at Epoch  8   1320.179931640625\n",
      "loss at Epoch  9   1320.33349609375\n",
      "loss at Epoch  10   1320.221435546875\n",
      "loss at Epoch  11   1320.415283203125\n",
      "loss at Epoch  12   1320.679443359375\n",
      "loss at Epoch  13   1320.734619140625\n",
      "loss at Epoch  14   1320.736083984375\n",
      "loss at Epoch  15   1320.6983642578125\n",
      "loss at Epoch  16   1320.6895751953125\n",
      "loss at Epoch  17   1320.8544921875\n",
      "loss at Epoch  18   1320.837646484375\n",
      "loss at Epoch  19   1320.95068359375\n",
      "loss at Epoch  20   1320.9716796875\n",
      "loss at Epoch  21   1320.974365234375\n",
      "loss at Epoch  22   1320.9306640625\n",
      "loss at Epoch  23   1321.03125\n",
      "loss at Epoch  24   1321.0626220703125\n",
      "loss at Epoch  25   1321.11279296875\n",
      "loss at Epoch  26   1321.083740234375\n",
      "loss at Epoch  27   1321.04052734375\n",
      "loss at Epoch  28   1321.071533203125\n",
      "loss at Epoch  29   1321.0902099609375\n",
      "loss at Epoch  30   1321.1123046875\n",
      "loss at Epoch  31   1321.0965576171875\n",
      "loss at Epoch  32   1321.112548828125\n",
      "loss at Epoch  33   1321.11669921875\n",
      "loss at Epoch  34   1321.1409912109375\n",
      "loss at Epoch  35   1321.13916015625\n",
      "loss at Epoch  36   1321.1490478515625\n",
      "loss at Epoch  37   1321.1563720703125\n",
      "loss at Epoch  38   1321.1591796875\n",
      "loss at Epoch  39   1321.17041015625\n",
      "loss at Epoch  0   1344.6737060546875\n",
      "loss at Epoch  1   1343.4569091796875\n",
      "loss at Epoch  2   1342.3310546875\n",
      "loss at Epoch  3   1343.0931396484375\n",
      "loss at Epoch  4   1342.2840576171875\n",
      "loss at Epoch  5   1342.7860107421875\n",
      "loss at Epoch  6   1342.978271484375\n",
      "loss at Epoch  7   1342.88720703125\n",
      "loss at Epoch  8   1342.92333984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  9   1343.1851806640625\n",
      "loss at Epoch  10   1343.1494140625\n",
      "loss at Epoch  11   1343.27734375\n",
      "loss at Epoch  12   1343.36328125\n",
      "loss at Epoch  13   1343.49658203125\n",
      "loss at Epoch  14   1343.598388671875\n",
      "loss at Epoch  15   1343.6280517578125\n",
      "loss at Epoch  16   1343.68701171875\n",
      "loss at Epoch  17   1343.576171875\n",
      "loss at Epoch  18   1343.74072265625\n",
      "loss at Epoch  19   1343.703857421875\n",
      "loss at Epoch  20   1343.7745361328125\n",
      "loss at Epoch  21   1343.8009033203125\n",
      "loss at Epoch  22   1343.801025390625\n",
      "loss at Epoch  23   1343.819580078125\n",
      "loss at Epoch  24   1343.8184814453125\n",
      "loss at Epoch  25   1343.850341796875\n",
      "loss at Epoch  26   1343.878173828125\n",
      "loss at Epoch  27   1343.8720703125\n",
      "loss at Epoch  28   1343.873779296875\n",
      "loss at Epoch  29   1343.896728515625\n",
      "loss at Epoch  30   1343.9058837890625\n",
      "loss at Epoch  31   1343.914794921875\n",
      "loss at Epoch  32   1343.9091796875\n",
      "loss at Epoch  33   1343.939453125\n",
      "loss at Epoch  34   1343.956298828125\n",
      "loss at Epoch  35   1343.9453125\n",
      "loss at Epoch  36   1343.95361328125\n",
      "loss at Epoch  37   1343.953857421875\n",
      "loss at Epoch  38   1343.964111328125\n",
      "loss at Epoch  39   1343.97265625\n",
      "loss at Epoch  0   1216.716796875\n",
      "loss at Epoch  1   1215.8125\n",
      "loss at Epoch  2   1214.71875\n",
      "loss at Epoch  3   1214.48876953125\n",
      "loss at Epoch  4   1214.736328125\n",
      "loss at Epoch  5   1215.260498046875\n",
      "loss at Epoch  6   1215.235107421875\n",
      "loss at Epoch  7   1214.890869140625\n",
      "loss at Epoch  8   1215.445068359375\n",
      "loss at Epoch  9   1215.25390625\n",
      "loss at Epoch  10   1215.663330078125\n",
      "loss at Epoch  11   1215.706787109375\n",
      "loss at Epoch  12   1215.909423828125\n",
      "loss at Epoch  13   1215.78759765625\n",
      "loss at Epoch  14   1215.9637451171875\n",
      "loss at Epoch  15   1216.001220703125\n",
      "loss at Epoch  16   1215.90625\n",
      "loss at Epoch  17   1216.002685546875\n",
      "loss at Epoch  18   1216.11279296875\n",
      "loss at Epoch  19   1216.1514892578125\n",
      "loss at Epoch  20   1216.08935546875\n",
      "loss at Epoch  21   1216.0853271484375\n",
      "loss at Epoch  22   1216.179443359375\n",
      "loss at Epoch  23   1216.23388671875\n",
      "loss at Epoch  24   1216.229736328125\n",
      "loss at Epoch  25   1216.28125\n",
      "loss at Epoch  26   1216.3104248046875\n",
      "loss at Epoch  27   1216.327392578125\n",
      "loss at Epoch  28   1216.380126953125\n",
      "loss at Epoch  29   1216.404541015625\n",
      "loss at Epoch  30   1216.4049072265625\n",
      "loss at Epoch  31   1216.397216796875\n",
      "loss at Epoch  32   1216.4053955078125\n",
      "loss at Epoch  33   1216.43017578125\n",
      "loss at Epoch  34   1216.4425048828125\n",
      "loss at Epoch  35   1216.444091796875\n",
      "loss at Epoch  36   1216.44189453125\n",
      "loss at Epoch  37   1216.4552001953125\n",
      "loss at Epoch  38   1216.460693359375\n",
      "loss at Epoch  39   1216.4710693359375\n",
      "loss at Epoch  0   1281.9312744140625\n",
      "loss at Epoch  1   1281.83935546875\n",
      "loss at Epoch  2   1281.154541015625\n",
      "loss at Epoch  3   1281.66796875\n",
      "loss at Epoch  4   1280.938232421875\n",
      "loss at Epoch  5   1280.724609375\n",
      "loss at Epoch  6   1281.61669921875\n",
      "loss at Epoch  7   1281.693603515625\n",
      "loss at Epoch  8   1281.09765625\n",
      "loss at Epoch  9   1281.244873046875\n",
      "loss at Epoch  10   1281.495849609375\n",
      "loss at Epoch  11   1281.779052734375\n",
      "loss at Epoch  12   1281.666259765625\n",
      "loss at Epoch  13   1281.6357421875\n",
      "loss at Epoch  14   1281.5828857421875\n",
      "loss at Epoch  15   1281.6533203125\n",
      "loss at Epoch  16   1281.828369140625\n",
      "loss at Epoch  17   1281.8145751953125\n",
      "loss at Epoch  18   1281.9031982421875\n",
      "loss at Epoch  19   1281.871826171875\n",
      "loss at Epoch  20   1281.906494140625\n",
      "loss at Epoch  21   1281.989013671875\n",
      "loss at Epoch  22   1282.056396484375\n",
      "loss at Epoch  23   1282.0096435546875\n",
      "loss at Epoch  24   1282.022216796875\n",
      "loss at Epoch  25   1282.057373046875\n",
      "loss at Epoch  26   1282.0849609375\n",
      "loss at Epoch  27   1282.1131591796875\n",
      "loss at Epoch  28   1282.0933837890625\n",
      "loss at Epoch  29   1282.123779296875\n",
      "loss at Epoch  30   1282.1148681640625\n",
      "loss at Epoch  31   1282.13330078125\n",
      "loss at Epoch  32   1282.1510009765625\n",
      "loss at Epoch  33   1282.169921875\n",
      "loss at Epoch  34   1282.154541015625\n",
      "loss at Epoch  35   1282.1739501953125\n",
      "loss at Epoch  36   1282.165771484375\n",
      "loss at Epoch  37   1282.17724609375\n",
      "loss at Epoch  38   1282.1800537109375\n",
      "loss at Epoch  39   1282.187744140625\n",
      "loss at Epoch  0   1333.4898681640625\n",
      "loss at Epoch  1   1331.623779296875\n",
      "loss at Epoch  2   1331.5535888671875\n",
      "loss at Epoch  3   1331.947021484375\n",
      "loss at Epoch  4   1331.60107421875\n",
      "loss at Epoch  5   1331.9075927734375\n",
      "loss at Epoch  6   1332.46337890625\n",
      "loss at Epoch  7   1331.840087890625\n",
      "loss at Epoch  8   1332.424072265625\n",
      "loss at Epoch  9   1332.31591796875\n",
      "loss at Epoch  10   1332.295654296875\n",
      "loss at Epoch  11   1332.1121826171875\n",
      "loss at Epoch  12   1332.4912109375\n",
      "loss at Epoch  13   1332.52294921875\n",
      "loss at Epoch  14   1332.453125\n",
      "loss at Epoch  15   1332.553466796875\n",
      "loss at Epoch  16   1332.612060546875\n",
      "loss at Epoch  17   1332.673828125\n",
      "loss at Epoch  18   1332.768798828125\n",
      "loss at Epoch  19   1332.790283203125\n",
      "loss at Epoch  20   1332.816162109375\n",
      "loss at Epoch  21   1332.8480224609375\n",
      "loss at Epoch  22   1332.8675537109375\n",
      "loss at Epoch  23   1332.904541015625\n",
      "loss at Epoch  24   1332.9000244140625\n",
      "loss at Epoch  25   1332.9287109375\n",
      "loss at Epoch  26   1332.970703125\n",
      "loss at Epoch  27   1332.9765625\n",
      "loss at Epoch  28   1333.008544921875\n",
      "loss at Epoch  29   1332.9990234375\n",
      "loss at Epoch  30   1333.0279541015625\n",
      "loss at Epoch  31   1333.015869140625\n",
      "loss at Epoch  32   1333.011962890625\n",
      "loss at Epoch  33   1333.0477294921875\n",
      "loss at Epoch  34   1333.048828125\n",
      "loss at Epoch  35   1333.0394287109375\n",
      "loss at Epoch  36   1333.0556640625\n",
      "loss at Epoch  37   1333.0576171875\n",
      "loss at Epoch  38   1333.056884765625\n",
      "loss at Epoch  39   1333.074462890625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1279.0751953125\n",
      "loss at Epoch  1   1280.471923828125\n",
      "loss at Epoch  2   1281.62841796875\n",
      "loss at Epoch  3   1281.13134765625\n",
      "loss at Epoch  4   1281.056884765625\n",
      "loss at Epoch  5   1281.5931396484375\n",
      "loss at Epoch  6   1281.219970703125\n",
      "loss at Epoch  7   1281.4835205078125\n",
      "loss at Epoch  8   1281.65576171875\n",
      "loss at Epoch  9   1281.7757568359375\n",
      "loss at Epoch  10   1281.921142578125\n",
      "loss at Epoch  11   1281.960693359375\n",
      "loss at Epoch  12   1282.0069580078125\n",
      "loss at Epoch  13   1282.455322265625\n",
      "loss at Epoch  14   1282.263427734375\n",
      "loss at Epoch  15   1282.474609375\n",
      "loss at Epoch  16   1282.443115234375\n",
      "loss at Epoch  17   1282.486083984375\n",
      "loss at Epoch  18   1282.501220703125\n",
      "loss at Epoch  19   1282.72021484375\n",
      "loss at Epoch  20   1282.59423828125\n",
      "loss at Epoch  21   1282.66552734375\n",
      "loss at Epoch  22   1282.7205810546875\n",
      "loss at Epoch  23   1282.74267578125\n",
      "loss at Epoch  24   1282.74658203125\n",
      "loss at Epoch  25   1282.7430419921875\n",
      "loss at Epoch  26   1282.7989501953125\n",
      "loss at Epoch  27   1282.813720703125\n",
      "loss at Epoch  28   1282.83642578125\n",
      "loss at Epoch  29   1282.832275390625\n",
      "loss at Epoch  30   1282.85546875\n",
      "loss at Epoch  31   1282.8673095703125\n",
      "loss at Epoch  32   1282.85888671875\n",
      "loss at Epoch  33   1282.87744140625\n",
      "loss at Epoch  34   1282.88232421875\n",
      "loss at Epoch  35   1282.88134765625\n",
      "loss at Epoch  36   1282.8798828125\n",
      "loss at Epoch  37   1282.8829345703125\n",
      "loss at Epoch  38   1282.893310546875\n",
      "loss at Epoch  39   1282.9072265625\n",
      "loss at Epoch  0   1323.162109375\n",
      "loss at Epoch  1   1323.62744140625\n",
      "loss at Epoch  2   1322.5037841796875\n",
      "loss at Epoch  3   1320.90673828125\n",
      "loss at Epoch  4   1320.63037109375\n",
      "loss at Epoch  5   1320.5291748046875\n",
      "loss at Epoch  6   1320.812255859375\n",
      "loss at Epoch  7   1320.367431640625\n",
      "loss at Epoch  8   1320.730712890625\n",
      "loss at Epoch  9   1320.8311767578125\n",
      "loss at Epoch  10   1320.6982421875\n",
      "loss at Epoch  11   1321.156005859375\n",
      "loss at Epoch  12   1320.701904296875\n",
      "loss at Epoch  13   1321.0684814453125\n",
      "loss at Epoch  14   1321.05126953125\n",
      "loss at Epoch  15   1321.07080078125\n",
      "loss at Epoch  16   1321.009033203125\n",
      "loss at Epoch  17   1321.105224609375\n",
      "loss at Epoch  18   1321.2242431640625\n",
      "loss at Epoch  19   1321.300048828125\n",
      "loss at Epoch  20   1321.2626953125\n",
      "loss at Epoch  21   1321.2567138671875\n",
      "loss at Epoch  22   1321.287841796875\n",
      "loss at Epoch  23   1321.3353271484375\n",
      "loss at Epoch  24   1321.3033447265625\n",
      "loss at Epoch  25   1321.36962890625\n",
      "loss at Epoch  26   1321.34716796875\n",
      "loss at Epoch  27   1321.3818359375\n",
      "loss at Epoch  28   1321.42822265625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  29   1321.398193359375\n",
      "loss at Epoch  30   1321.44482421875\n",
      "loss at Epoch  31   1321.44677734375\n",
      "loss at Epoch  32   1321.4410400390625\n",
      "loss at Epoch  33   1321.4464111328125\n",
      "loss at Epoch  34   1321.473388671875\n",
      "loss at Epoch  35   1321.47412109375\n",
      "loss at Epoch  36   1321.475830078125\n",
      "loss at Epoch  37   1321.473876953125\n",
      "loss at Epoch  38   1321.4761962890625\n",
      "loss at Epoch  39   1321.482177734375\n",
      "loss at Epoch  0   1344.953125\n",
      "loss at Epoch  1   1343.60498046875\n",
      "loss at Epoch  2   1342.9952392578125\n",
      "loss at Epoch  3   1342.1107177734375\n",
      "loss at Epoch  4   1343.5574951171875\n",
      "loss at Epoch  5   1342.6488037109375\n",
      "loss at Epoch  6   1343.0526123046875\n",
      "loss at Epoch  7   1342.7884521484375\n",
      "loss at Epoch  8   1343.21240234375\n",
      "loss at Epoch  9   1343.165283203125\n",
      "loss at Epoch  10   1343.21875\n",
      "loss at Epoch  11   1343.40869140625\n",
      "loss at Epoch  12   1343.6839599609375\n",
      "loss at Epoch  13   1343.60498046875\n",
      "loss at Epoch  14   1343.705322265625\n",
      "loss at Epoch  15   1343.665771484375\n",
      "loss at Epoch  16   1343.646240234375\n",
      "loss at Epoch  17   1343.793701171875\n",
      "loss at Epoch  18   1343.764404296875\n",
      "loss at Epoch  19   1343.826416015625\n",
      "loss at Epoch  20   1343.8670654296875\n",
      "loss at Epoch  21   1343.87744140625\n",
      "loss at Epoch  22   1343.845458984375\n",
      "loss at Epoch  23   1343.8773193359375\n",
      "loss at Epoch  24   1343.904052734375\n",
      "loss at Epoch  25   1343.944091796875\n",
      "loss at Epoch  26   1343.951171875\n",
      "loss at Epoch  27   1343.984130859375\n",
      "loss at Epoch  28   1344.01708984375\n",
      "loss at Epoch  29   1343.98388671875\n",
      "loss at Epoch  30   1344.00048828125\n",
      "loss at Epoch  31   1344.0159912109375\n",
      "loss at Epoch  32   1344.0411376953125\n",
      "loss at Epoch  33   1344.037841796875\n",
      "loss at Epoch  34   1344.075927734375\n",
      "loss at Epoch  35   1344.060546875\n",
      "loss at Epoch  36   1344.0576171875\n",
      "loss at Epoch  37   1344.0667724609375\n",
      "loss at Epoch  38   1344.082275390625\n",
      "loss at Epoch  39   1344.083984375\n",
      "loss at Epoch  0   1491.5355224609375\n",
      "loss at Epoch  1   1491.9420166015625\n",
      "loss at Epoch  2   1491.2052001953125\n",
      "loss at Epoch  3   1490.519775390625\n",
      "loss at Epoch  4   1489.761962890625\n",
      "loss at Epoch  5   1489.855712890625\n",
      "loss at Epoch  6   1490.169921875\n",
      "loss at Epoch  7   1489.98974609375\n",
      "loss at Epoch  8   1489.739990234375\n",
      "loss at Epoch  9   1490.08447265625\n",
      "loss at Epoch  10   1490.169677734375\n",
      "loss at Epoch  11   1490.173583984375\n",
      "loss at Epoch  12   1490.5037841796875\n",
      "loss at Epoch  13   1490.40283203125\n",
      "loss at Epoch  14   1490.5047607421875\n",
      "loss at Epoch  15   1490.44091796875\n",
      "loss at Epoch  16   1490.555419921875\n",
      "loss at Epoch  17   1490.6676025390625\n",
      "loss at Epoch  18   1490.65869140625\n",
      "loss at Epoch  19   1490.838623046875\n",
      "loss at Epoch  20   1490.858642578125\n",
      "loss at Epoch  21   1490.7685546875\n",
      "loss at Epoch  22   1490.8768310546875\n",
      "loss at Epoch  23   1490.91015625\n",
      "loss at Epoch  24   1490.86669921875\n",
      "loss at Epoch  25   1490.893310546875\n",
      "loss at Epoch  26   1490.92529296875\n",
      "loss at Epoch  27   1490.909423828125\n",
      "loss at Epoch  28   1490.972900390625\n",
      "loss at Epoch  29   1490.9981689453125\n",
      "loss at Epoch  30   1490.9984130859375\n",
      "loss at Epoch  31   1490.9937744140625\n",
      "loss at Epoch  32   1491.0108642578125\n",
      "loss at Epoch  33   1491.019287109375\n",
      "loss at Epoch  34   1491.012451171875\n",
      "loss at Epoch  35   1491.010009765625\n",
      "loss at Epoch  36   1491.02099609375\n",
      "loss at Epoch  37   1491.0274658203125\n",
      "loss at Epoch  38   1491.033935546875\n",
      "loss at Epoch  39   1491.028564453125\n",
      "loss at Epoch  0   1265.019287109375\n",
      "loss at Epoch  1   1263.7867431640625\n",
      "loss at Epoch  2   1262.283203125\n",
      "loss at Epoch  3   1263.564208984375\n",
      "loss at Epoch  4   1262.294921875\n",
      "loss at Epoch  5   1262.6175537109375\n",
      "loss at Epoch  6   1262.555419921875\n",
      "loss at Epoch  7   1262.6737060546875\n",
      "loss at Epoch  8   1263.0836181640625\n",
      "loss at Epoch  9   1262.961669921875\n",
      "loss at Epoch  10   1263.151123046875\n",
      "loss at Epoch  11   1263.2044677734375\n",
      "loss at Epoch  12   1263.0833740234375\n",
      "loss at Epoch  13   1263.4654541015625\n",
      "loss at Epoch  14   1263.5025634765625\n",
      "loss at Epoch  15   1263.325439453125\n",
      "loss at Epoch  16   1263.50341796875\n",
      "loss at Epoch  17   1263.4912109375\n",
      "loss at Epoch  18   1263.6214599609375\n",
      "loss at Epoch  19   1263.66015625\n",
      "loss at Epoch  20   1263.66064453125\n",
      "loss at Epoch  21   1263.7520751953125\n",
      "loss at Epoch  22   1263.7938232421875\n",
      "loss at Epoch  23   1263.7655029296875\n",
      "loss at Epoch  24   1263.803955078125\n",
      "loss at Epoch  25   1263.840576171875\n",
      "loss at Epoch  26   1263.8248291015625\n",
      "loss at Epoch  27   1263.852783203125\n",
      "loss at Epoch  28   1263.8646240234375\n",
      "loss at Epoch  29   1263.8720703125\n",
      "loss at Epoch  30   1263.859619140625\n",
      "loss at Epoch  31   1263.891845703125\n",
      "loss at Epoch  32   1263.893310546875\n",
      "loss at Epoch  33   1263.90478515625\n",
      "loss at Epoch  34   1263.9156494140625\n",
      "loss at Epoch  35   1263.9173583984375\n",
      "loss at Epoch  36   1263.93701171875\n",
      "loss at Epoch  37   1263.9378662109375\n",
      "loss at Epoch  38   1263.951416015625\n",
      "loss at Epoch  39   1263.948974609375\n",
      "loss at Epoch  0   1374.1337890625\n",
      "loss at Epoch  1   1374.83251953125\n",
      "loss at Epoch  2   1373.0186767578125\n",
      "loss at Epoch  3   1373.1317138671875\n",
      "loss at Epoch  4   1373.5706787109375\n",
      "loss at Epoch  5   1372.9852294921875\n",
      "loss at Epoch  6   1372.5166015625\n",
      "loss at Epoch  7   1372.7822265625\n",
      "loss at Epoch  8   1372.9388427734375\n",
      "loss at Epoch  9   1372.88134765625\n",
      "loss at Epoch  10   1373.119384765625\n",
      "loss at Epoch  11   1373.327392578125\n",
      "loss at Epoch  12   1373.288330078125\n",
      "loss at Epoch  13   1373.261962890625\n",
      "loss at Epoch  14   1373.336181640625\n",
      "loss at Epoch  15   1373.461181640625\n",
      "loss at Epoch  16   1373.3984375\n",
      "loss at Epoch  17   1373.41552734375\n",
      "loss at Epoch  18   1373.517822265625\n",
      "loss at Epoch  19   1373.4754638671875\n",
      "loss at Epoch  20   1373.5517578125\n",
      "loss at Epoch  21   1373.6131591796875\n",
      "loss at Epoch  22   1373.6715087890625\n",
      "loss at Epoch  23   1373.658203125\n",
      "loss at Epoch  24   1373.6455078125\n",
      "loss at Epoch  25   1373.6949462890625\n",
      "loss at Epoch  26   1373.68212890625\n",
      "loss at Epoch  27   1373.7308349609375\n",
      "loss at Epoch  28   1373.74462890625\n",
      "loss at Epoch  29   1373.744140625\n",
      "loss at Epoch  30   1373.7391357421875\n",
      "loss at Epoch  31   1373.7373046875\n",
      "loss at Epoch  32   1373.749267578125\n",
      "loss at Epoch  33   1373.755126953125\n",
      "loss at Epoch  34   1373.765869140625\n",
      "loss at Epoch  35   1373.7755126953125\n",
      "loss at Epoch  36   1373.7767333984375\n",
      "loss at Epoch  37   1373.7825927734375\n",
      "loss at Epoch  38   1373.786376953125\n",
      "loss at Epoch  39   1373.797607421875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n",
      "loss at Epoch  0   1230.3099365234375\n",
      "loss at Epoch  1   1231.9736328125\n",
      "loss at Epoch  2   1231.335205078125\n",
      "loss at Epoch  3   1231.49609375\n",
      "loss at Epoch  4   1231.1019287109375\n",
      "loss at Epoch  5   1232.0469970703125\n",
      "loss at Epoch  6   1232.342041015625\n",
      "loss at Epoch  7   1232.27587890625\n",
      "loss at Epoch  8   1232.5029296875\n",
      "loss at Epoch  9   1232.5361328125\n",
      "loss at Epoch  10   1233.07275390625\n",
      "loss at Epoch  11   1232.9603271484375\n",
      "loss at Epoch  12   1232.96044921875\n",
      "loss at Epoch  13   1233.054931640625\n",
      "loss at Epoch  14   1233.0054931640625\n",
      "loss at Epoch  15   1233.1519775390625\n",
      "loss at Epoch  16   1233.068359375\n",
      "loss at Epoch  17   1233.2330322265625\n",
      "loss at Epoch  18   1233.3153076171875\n",
      "loss at Epoch  19   1233.4150390625\n",
      "loss at Epoch  20   1233.3511962890625\n",
      "loss at Epoch  21   1233.44384765625\n",
      "loss at Epoch  22   1233.51220703125\n",
      "loss at Epoch  23   1233.5450439453125\n",
      "loss at Epoch  24   1233.51025390625\n",
      "loss at Epoch  25   1233.561767578125\n",
      "loss at Epoch  26   1233.548095703125\n",
      "loss at Epoch  27   1233.588134765625\n",
      "loss at Epoch  28   1233.642578125\n",
      "loss at Epoch  29   1233.627197265625\n",
      "loss at Epoch  30   1233.636474609375\n",
      "loss at Epoch  31   1233.653564453125\n",
      "loss at Epoch  32   1233.667724609375\n",
      "loss at Epoch  33   1233.66796875\n",
      "loss at Epoch  34   1233.669921875\n",
      "loss at Epoch  35   1233.684326171875\n",
      "loss at Epoch  36   1233.707275390625\n",
      "loss at Epoch  37   1233.718505859375\n",
      "loss at Epoch  38   1233.718994140625\n",
      "loss at Epoch  39   1233.721435546875\n",
      "loss at Epoch  0   1309.5665283203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  1   1309.031494140625\n",
      "loss at Epoch  2   1307.8341064453125\n",
      "loss at Epoch  3   1307.2545166015625\n",
      "loss at Epoch  4   1307.508544921875\n",
      "loss at Epoch  5   1308.569091796875\n",
      "loss at Epoch  6   1308.060546875\n",
      "loss at Epoch  7   1307.825927734375\n",
      "loss at Epoch  8   1307.8369140625\n",
      "loss at Epoch  9   1307.594482421875\n",
      "loss at Epoch  10   1307.743896484375\n",
      "loss at Epoch  11   1307.749267578125\n",
      "loss at Epoch  12   1307.662353515625\n",
      "loss at Epoch  13   1308.0562744140625\n",
      "loss at Epoch  14   1307.81298828125\n",
      "loss at Epoch  15   1308.013671875\n",
      "loss at Epoch  16   1308.013916015625\n",
      "loss at Epoch  17   1308.071044921875\n",
      "loss at Epoch  18   1308.1865234375\n",
      "loss at Epoch  19   1308.13671875\n",
      "loss at Epoch  20   1308.1595458984375\n",
      "loss at Epoch  21   1308.206787109375\n",
      "loss at Epoch  22   1308.143798828125\n",
      "loss at Epoch  23   1308.239013671875\n",
      "loss at Epoch  24   1308.2933349609375\n",
      "loss at Epoch  25   1308.313720703125\n",
      "loss at Epoch  26   1308.3232421875\n",
      "loss at Epoch  27   1308.319091796875\n",
      "loss at Epoch  28   1308.3291015625\n",
      "loss at Epoch  29   1308.3531494140625\n",
      "loss at Epoch  30   1308.35595703125\n",
      "loss at Epoch  31   1308.366455078125\n",
      "loss at Epoch  32   1308.3748779296875\n",
      "loss at Epoch  33   1308.4022216796875\n",
      "loss at Epoch  34   1308.4132080078125\n",
      "loss at Epoch  35   1308.4078369140625\n",
      "loss at Epoch  36   1308.430908203125\n",
      "loss at Epoch  37   1308.421630859375\n",
      "loss at Epoch  38   1308.427490234375\n",
      "loss at Epoch  39   1308.4385986328125\n",
      "loss at Epoch  0   1222.1328125\n",
      "loss at Epoch  1   1221.108642578125\n",
      "loss at Epoch  2   1222.0709228515625\n",
      "loss at Epoch  3   1220.17041015625\n",
      "loss at Epoch  4   1222.179443359375\n",
      "loss at Epoch  5   1221.2435302734375\n",
      "loss at Epoch  6   1220.85009765625\n",
      "loss at Epoch  7   1220.8819580078125\n",
      "loss at Epoch  8   1221.0631103515625\n",
      "loss at Epoch  9   1221.120849609375\n",
      "loss at Epoch  10   1220.98291015625\n",
      "loss at Epoch  11   1221.179931640625\n",
      "loss at Epoch  12   1221.456787109375\n",
      "loss at Epoch  13   1221.6429443359375\n",
      "loss at Epoch  14   1221.701171875\n",
      "loss at Epoch  15   1221.888671875\n",
      "loss at Epoch  16   1221.76123046875\n",
      "loss at Epoch  17   1221.84521484375\n",
      "loss at Epoch  18   1221.90380859375\n",
      "loss at Epoch  19   1221.8782958984375\n",
      "loss at Epoch  20   1221.8609619140625\n",
      "loss at Epoch  21   1221.903076171875\n",
      "loss at Epoch  22   1221.91162109375\n",
      "loss at Epoch  23   1221.99853515625\n",
      "loss at Epoch  24   1221.9913330078125\n",
      "loss at Epoch  25   1222.008056640625\n",
      "loss at Epoch  26   1222.054931640625\n",
      "loss at Epoch  27   1222.1339111328125\n",
      "loss at Epoch  28   1222.09716796875\n",
      "loss at Epoch  29   1222.09228515625\n",
      "loss at Epoch  30   1222.08447265625\n",
      "loss at Epoch  31   1222.1136474609375\n",
      "loss at Epoch  32   1222.12841796875\n",
      "loss at Epoch  33   1222.1195068359375\n",
      "loss at Epoch  34   1222.166259765625\n",
      "loss at Epoch  35   1222.167236328125\n",
      "loss at Epoch  36   1222.163818359375\n",
      "loss at Epoch  37   1222.166259765625\n",
      "loss at Epoch  38   1222.1787109375\n",
      "loss at Epoch  39   1222.1763916015625\n",
      "loss at Epoch  0   1212.2509765625\n",
      "loss at Epoch  1   1211.185546875\n",
      "loss at Epoch  2   1209.89892578125\n",
      "loss at Epoch  3   1210.2591552734375\n",
      "loss at Epoch  4   1210.353759765625\n",
      "loss at Epoch  5   1210.415771484375\n",
      "loss at Epoch  6   1209.89794921875\n",
      "loss at Epoch  7   1210.21435546875\n",
      "loss at Epoch  8   1210.182373046875\n",
      "loss at Epoch  9   1210.369384765625\n",
      "loss at Epoch  10   1210.325927734375\n",
      "loss at Epoch  11   1210.453125\n",
      "loss at Epoch  12   1210.6190185546875\n",
      "loss at Epoch  13   1210.580810546875\n",
      "loss at Epoch  14   1210.6461181640625\n",
      "loss at Epoch  15   1210.576171875\n",
      "loss at Epoch  16   1210.72021484375\n",
      "loss at Epoch  17   1210.6806640625\n",
      "loss at Epoch  18   1210.766845703125\n",
      "loss at Epoch  19   1210.83984375\n",
      "loss at Epoch  20   1210.831787109375\n",
      "loss at Epoch  21   1210.788330078125\n",
      "loss at Epoch  22   1210.91064453125\n",
      "loss at Epoch  23   1210.8740234375\n",
      "loss at Epoch  24   1210.9287109375\n",
      "loss at Epoch  25   1210.937744140625\n",
      "loss at Epoch  26   1210.9871826171875\n",
      "loss at Epoch  27   1211.0361328125\n",
      "loss at Epoch  28   1211.02685546875\n",
      "loss at Epoch  29   1211.0352783203125\n",
      "loss at Epoch  30   1211.034912109375\n",
      "loss at Epoch  31   1211.04150390625\n",
      "loss at Epoch  32   1211.0750732421875\n",
      "loss at Epoch  33   1211.083251953125\n",
      "loss at Epoch  34   1211.0914306640625\n",
      "loss at Epoch  35   1211.105224609375\n",
      "loss at Epoch  36   1211.1065673828125\n",
      "loss at Epoch  37   1211.10302734375\n",
      "loss at Epoch  38   1211.119873046875\n",
      "loss at Epoch  39   1211.11767578125\n",
      "loss at Epoch  0   1264.20166015625\n",
      "loss at Epoch  1   1264.740966796875\n",
      "loss at Epoch  2   1262.06640625\n",
      "loss at Epoch  3   1263.8138427734375\n",
      "loss at Epoch  4   1261.8084716796875\n",
      "loss at Epoch  5   1262.40966796875\n",
      "loss at Epoch  6   1262.308837890625\n",
      "loss at Epoch  7   1262.6861572265625\n",
      "loss at Epoch  8   1262.93701171875\n",
      "loss at Epoch  9   1263.025390625\n",
      "loss at Epoch  10   1262.86376953125\n",
      "loss at Epoch  11   1262.8446044921875\n",
      "loss at Epoch  12   1262.986083984375\n",
      "loss at Epoch  13   1263.111328125\n",
      "loss at Epoch  14   1263.2415771484375\n",
      "loss at Epoch  15   1263.376953125\n",
      "loss at Epoch  16   1263.294921875\n",
      "loss at Epoch  17   1263.30859375\n",
      "loss at Epoch  18   1263.39599609375\n",
      "loss at Epoch  19   1263.37646484375\n",
      "loss at Epoch  20   1263.483642578125\n",
      "loss at Epoch  21   1263.5535888671875\n",
      "loss at Epoch  22   1263.550537109375\n",
      "loss at Epoch  23   1263.606689453125\n",
      "loss at Epoch  24   1263.6353759765625\n",
      "loss at Epoch  25   1263.62890625\n",
      "loss at Epoch  26   1263.6207275390625\n",
      "loss at Epoch  27   1263.67724609375\n",
      "loss at Epoch  28   1263.699462890625\n",
      "loss at Epoch  29   1263.6795654296875\n",
      "loss at Epoch  30   1263.671142578125\n",
      "loss at Epoch  31   1263.7001953125\n",
      "loss at Epoch  32   1263.710693359375\n",
      "loss at Epoch  33   1263.724365234375\n",
      "loss at Epoch  34   1263.741943359375\n",
      "loss at Epoch  35   1263.73193359375\n",
      "loss at Epoch  36   1263.747802734375\n",
      "loss at Epoch  37   1263.7620849609375\n",
      "loss at Epoch  38   1263.758056640625\n",
      "loss at Epoch  39   1263.7835693359375\n",
      "loss at Epoch  0   1329.6656494140625\n",
      "loss at Epoch  1   1329.0252685546875\n",
      "loss at Epoch  2   1327.3084716796875\n",
      "loss at Epoch  3   1327.1280517578125\n",
      "loss at Epoch  4   1327.4610595703125\n",
      "loss at Epoch  5   1327.7337646484375\n",
      "loss at Epoch  6   1327.981201171875\n",
      "loss at Epoch  7   1327.6033935546875\n",
      "loss at Epoch  8   1327.9505615234375\n",
      "loss at Epoch  9   1327.683837890625\n",
      "loss at Epoch  10   1328.1632080078125\n",
      "loss at Epoch  11   1328.008544921875\n",
      "loss at Epoch  12   1328.12939453125\n",
      "loss at Epoch  13   1328.058349609375\n",
      "loss at Epoch  14   1328.1705322265625\n",
      "loss at Epoch  15   1328.318115234375\n",
      "loss at Epoch  16   1328.3077392578125\n",
      "loss at Epoch  17   1328.4285888671875\n",
      "loss at Epoch  18   1328.36572265625\n",
      "loss at Epoch  19   1328.4759521484375\n",
      "loss at Epoch  20   1328.4349365234375\n",
      "loss at Epoch  21   1328.43212890625\n",
      "loss at Epoch  22   1328.541015625\n",
      "loss at Epoch  23   1328.575927734375\n",
      "loss at Epoch  24   1328.61279296875\n",
      "loss at Epoch  25   1328.629638671875\n",
      "loss at Epoch  26   1328.6025390625\n",
      "loss at Epoch  27   1328.647705078125\n",
      "loss at Epoch  28   1328.66943359375\n",
      "loss at Epoch  29   1328.7021484375\n",
      "loss at Epoch  30   1328.705810546875\n",
      "loss at Epoch  31   1328.699462890625\n",
      "loss at Epoch  32   1328.72216796875\n",
      "loss at Epoch  33   1328.716064453125\n",
      "loss at Epoch  34   1328.7423095703125\n",
      "loss at Epoch  35   1328.7335205078125\n",
      "loss at Epoch  36   1328.735107421875\n",
      "loss at Epoch  37   1328.735595703125\n",
      "loss at Epoch  38   1328.7564697265625\n",
      "loss at Epoch  39   1328.75341796875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.524     0.649       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.484     0.467     0.415      1115\n",
      "weighted avg      0.754     0.509     0.586      1115\n",
      "\n",
      "loss at Epoch  0   1208.80615234375\n",
      "loss at Epoch  1   1209.0880126953125\n",
      "loss at Epoch  2   1209.8984375\n",
      "loss at Epoch  3   1209.20263671875\n",
      "loss at Epoch  4   1209.5960693359375\n",
      "loss at Epoch  5   1210.2734375\n",
      "loss at Epoch  6   1209.94091796875\n",
      "loss at Epoch  7   1210.4825439453125\n",
      "loss at Epoch  8   1210.59228515625\n",
      "loss at Epoch  9   1210.88720703125\n",
      "loss at Epoch  10   1211.4383544921875\n",
      "loss at Epoch  11   1211.306640625\n",
      "loss at Epoch  12   1211.2470703125\n",
      "loss at Epoch  13   1211.291259765625\n",
      "loss at Epoch  14   1211.525390625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  15   1211.45556640625\n",
      "loss at Epoch  16   1211.6282958984375\n",
      "loss at Epoch  17   1211.697509765625\n",
      "loss at Epoch  18   1211.63037109375\n",
      "loss at Epoch  19   1211.734619140625\n",
      "loss at Epoch  20   1211.79736328125\n",
      "loss at Epoch  21   1211.875244140625\n",
      "loss at Epoch  22   1211.845947265625\n",
      "loss at Epoch  23   1211.87548828125\n",
      "loss at Epoch  24   1211.877685546875\n",
      "loss at Epoch  25   1211.8681640625\n",
      "loss at Epoch  26   1211.859375\n",
      "loss at Epoch  27   1211.878173828125\n",
      "loss at Epoch  28   1211.8941650390625\n",
      "loss at Epoch  29   1211.9095458984375\n",
      "loss at Epoch  30   1211.9114990234375\n",
      "loss at Epoch  31   1211.953125\n",
      "loss at Epoch  32   1212.006591796875\n",
      "loss at Epoch  33   1211.981689453125\n",
      "loss at Epoch  34   1211.9716796875\n",
      "loss at Epoch  35   1212.0107421875\n",
      "loss at Epoch  36   1212.0040283203125\n",
      "loss at Epoch  37   1211.9957275390625\n",
      "loss at Epoch  38   1211.9993896484375\n",
      "loss at Epoch  39   1212.0068359375\n",
      "loss at Epoch  0   1242.062255859375\n",
      "loss at Epoch  1   1242.2681884765625\n",
      "loss at Epoch  2   1239.7869873046875\n",
      "loss at Epoch  3   1239.66748046875\n",
      "loss at Epoch  4   1239.374755859375\n",
      "loss at Epoch  5   1238.8834228515625\n",
      "loss at Epoch  6   1239.221435546875\n",
      "loss at Epoch  7   1239.58984375\n",
      "loss at Epoch  8   1239.345703125\n",
      "loss at Epoch  9   1239.63720703125\n",
      "loss at Epoch  10   1239.64404296875\n",
      "loss at Epoch  11   1239.48779296875\n",
      "loss at Epoch  12   1239.494873046875\n",
      "loss at Epoch  13   1239.6123046875\n",
      "loss at Epoch  14   1239.671142578125\n",
      "loss at Epoch  15   1239.7081298828125\n",
      "loss at Epoch  16   1239.763916015625\n",
      "loss at Epoch  17   1239.940185546875\n",
      "loss at Epoch  18   1239.959716796875\n",
      "loss at Epoch  19   1239.974365234375\n",
      "loss at Epoch  20   1239.940673828125\n",
      "loss at Epoch  21   1240.042236328125\n",
      "loss at Epoch  22   1240.0108642578125\n",
      "loss at Epoch  23   1240.048828125\n",
      "loss at Epoch  24   1240.077392578125\n",
      "loss at Epoch  25   1240.1063232421875\n",
      "loss at Epoch  26   1240.11181640625\n",
      "loss at Epoch  27   1240.131103515625\n",
      "loss at Epoch  28   1240.15869140625\n",
      "loss at Epoch  29   1240.1575927734375\n",
      "loss at Epoch  30   1240.196044921875\n",
      "loss at Epoch  31   1240.204345703125\n",
      "loss at Epoch  32   1240.2158203125\n",
      "loss at Epoch  33   1240.22607421875\n",
      "loss at Epoch  34   1240.22412109375\n",
      "loss at Epoch  35   1240.221435546875\n",
      "loss at Epoch  36   1240.2347412109375\n",
      "loss at Epoch  37   1240.232666015625\n",
      "loss at Epoch  38   1240.23486328125\n",
      "loss at Epoch  39   1240.238037109375\n",
      "loss at Epoch  0   1333.5428466796875\n",
      "loss at Epoch  1   1332.3046875\n",
      "loss at Epoch  2   1332.42529296875\n",
      "loss at Epoch  3   1332.21875\n",
      "loss at Epoch  4   1332.2669677734375\n",
      "loss at Epoch  5   1331.9091796875\n",
      "loss at Epoch  6   1331.7435302734375\n",
      "loss at Epoch  7   1331.673583984375\n",
      "loss at Epoch  8   1332.035888671875\n",
      "loss at Epoch  9   1331.877197265625\n",
      "loss at Epoch  10   1332.234619140625\n",
      "loss at Epoch  11   1332.280029296875\n",
      "loss at Epoch  12   1332.63671875\n",
      "loss at Epoch  13   1332.343017578125\n",
      "loss at Epoch  14   1332.458984375\n",
      "loss at Epoch  15   1332.749267578125\n",
      "loss at Epoch  16   1332.769287109375\n",
      "loss at Epoch  17   1332.767333984375\n",
      "loss at Epoch  18   1332.7314453125\n",
      "loss at Epoch  19   1332.730224609375\n",
      "loss at Epoch  20   1332.876220703125\n",
      "loss at Epoch  21   1332.8848876953125\n",
      "loss at Epoch  22   1332.9306640625\n",
      "loss at Epoch  23   1332.9293212890625\n",
      "loss at Epoch  24   1333.019287109375\n",
      "loss at Epoch  25   1332.9583740234375\n",
      "loss at Epoch  26   1333.0228271484375\n",
      "loss at Epoch  27   1333.058837890625\n",
      "loss at Epoch  28   1333.046875\n",
      "loss at Epoch  29   1333.05859375\n",
      "loss at Epoch  30   1333.056884765625\n",
      "loss at Epoch  31   1333.06005859375\n",
      "loss at Epoch  32   1333.0789794921875\n",
      "loss at Epoch  33   1333.08837890625\n",
      "loss at Epoch  34   1333.07861328125\n",
      "loss at Epoch  35   1333.0938720703125\n",
      "loss at Epoch  36   1333.09619140625\n",
      "loss at Epoch  37   1333.11669921875\n",
      "loss at Epoch  38   1333.120361328125\n",
      "loss at Epoch  39   1333.1199951171875\n",
      "loss at Epoch  0   1234.27001953125\n",
      "loss at Epoch  1   1233.322265625\n",
      "loss at Epoch  2   1233.2879638671875\n",
      "loss at Epoch  3   1232.7786865234375\n",
      "loss at Epoch  4   1232.5205078125\n",
      "loss at Epoch  5   1232.255126953125\n",
      "loss at Epoch  6   1232.224365234375\n",
      "loss at Epoch  7   1233.0338134765625\n",
      "loss at Epoch  8   1233.176025390625\n",
      "loss at Epoch  9   1233.1611328125\n",
      "loss at Epoch  10   1233.090576171875\n",
      "loss at Epoch  11   1232.86962890625\n",
      "loss at Epoch  12   1233.178955078125\n",
      "loss at Epoch  13   1233.129638671875\n",
      "loss at Epoch  14   1233.1492919921875\n",
      "loss at Epoch  15   1233.11767578125\n",
      "loss at Epoch  16   1233.234130859375\n",
      "loss at Epoch  17   1233.3507080078125\n",
      "loss at Epoch  18   1233.360595703125\n",
      "loss at Epoch  19   1233.35791015625\n",
      "loss at Epoch  20   1233.4072265625\n",
      "loss at Epoch  21   1233.486572265625\n",
      "loss at Epoch  22   1233.54638671875\n",
      "loss at Epoch  23   1233.5966796875\n",
      "loss at Epoch  24   1233.619384765625\n",
      "loss at Epoch  25   1233.578125\n",
      "loss at Epoch  26   1233.5843505859375\n",
      "loss at Epoch  27   1233.596923828125\n",
      "loss at Epoch  28   1233.6259765625\n",
      "loss at Epoch  29   1233.624755859375\n",
      "loss at Epoch  30   1233.6639404296875\n",
      "loss at Epoch  31   1233.6683349609375\n",
      "loss at Epoch  32   1233.6929931640625\n",
      "loss at Epoch  33   1233.709228515625\n",
      "loss at Epoch  34   1233.6962890625\n",
      "loss at Epoch  35   1233.69091796875\n",
      "loss at Epoch  36   1233.701416015625\n",
      "loss at Epoch  37   1233.7288818359375\n",
      "loss at Epoch  38   1233.730224609375\n",
      "loss at Epoch  39   1233.7353515625\n",
      "loss at Epoch  0   1195.041748046875\n",
      "loss at Epoch  1   1193.684326171875\n",
      "loss at Epoch  2   1193.95263671875\n",
      "loss at Epoch  3   1192.24169921875\n",
      "loss at Epoch  4   1193.14111328125\n",
      "loss at Epoch  5   1193.093994140625\n",
      "loss at Epoch  6   1193.1148681640625\n",
      "loss at Epoch  7   1193.109619140625\n",
      "loss at Epoch  8   1193.1439208984375\n",
      "loss at Epoch  9   1193.156494140625\n",
      "loss at Epoch  10   1193.29638671875\n",
      "loss at Epoch  11   1193.610107421875\n",
      "loss at Epoch  12   1193.8134765625\n",
      "loss at Epoch  13   1193.772705078125\n",
      "loss at Epoch  14   1193.8681640625\n",
      "loss at Epoch  15   1193.904541015625\n",
      "loss at Epoch  16   1193.875\n",
      "loss at Epoch  17   1194.04541015625\n",
      "loss at Epoch  18   1194.10693359375\n",
      "loss at Epoch  19   1194.072021484375\n",
      "loss at Epoch  20   1194.09912109375\n",
      "loss at Epoch  21   1194.1136474609375\n",
      "loss at Epoch  22   1194.121337890625\n",
      "loss at Epoch  23   1194.199462890625\n",
      "loss at Epoch  24   1194.1959228515625\n",
      "loss at Epoch  25   1194.2060546875\n",
      "loss at Epoch  26   1194.26806640625\n",
      "loss at Epoch  27   1194.28271484375\n",
      "loss at Epoch  28   1194.279541015625\n",
      "loss at Epoch  29   1194.302734375\n",
      "loss at Epoch  30   1194.299072265625\n",
      "loss at Epoch  31   1194.310546875\n",
      "loss at Epoch  32   1194.330078125\n",
      "loss at Epoch  33   1194.350341796875\n",
      "loss at Epoch  34   1194.36376953125\n",
      "loss at Epoch  35   1194.361328125\n",
      "loss at Epoch  36   1194.368896484375\n",
      "loss at Epoch  37   1194.366943359375\n",
      "loss at Epoch  38   1194.376708984375\n",
      "loss at Epoch  39   1194.3720703125\n",
      "loss at Epoch  0   1324.172607421875\n",
      "loss at Epoch  1   1323.9345703125\n",
      "loss at Epoch  2   1322.721435546875\n",
      "loss at Epoch  3   1321.73974609375\n",
      "loss at Epoch  4   1322.6444091796875\n",
      "loss at Epoch  5   1321.86767578125\n",
      "loss at Epoch  6   1322.24267578125\n",
      "loss at Epoch  7   1322.3797607421875\n",
      "loss at Epoch  8   1322.5042724609375\n",
      "loss at Epoch  9   1322.83349609375\n",
      "loss at Epoch  10   1322.6181640625\n",
      "loss at Epoch  11   1322.817138671875\n",
      "loss at Epoch  12   1322.619140625\n",
      "loss at Epoch  13   1322.777099609375\n",
      "loss at Epoch  14   1322.818603515625\n",
      "loss at Epoch  15   1322.889892578125\n",
      "loss at Epoch  16   1322.90771484375\n",
      "loss at Epoch  17   1323.003173828125\n",
      "loss at Epoch  18   1322.99365234375\n",
      "loss at Epoch  19   1323.1412353515625\n",
      "loss at Epoch  20   1323.137939453125\n",
      "loss at Epoch  21   1323.2235107421875\n",
      "loss at Epoch  22   1323.196533203125\n",
      "loss at Epoch  23   1323.1600341796875\n",
      "loss at Epoch  24   1323.238525390625\n",
      "loss at Epoch  25   1323.2557373046875\n",
      "loss at Epoch  26   1323.2452392578125\n",
      "loss at Epoch  27   1323.244384765625\n",
      "loss at Epoch  28   1323.23583984375\n",
      "loss at Epoch  29   1323.2554931640625\n",
      "loss at Epoch  30   1323.2860107421875\n",
      "loss at Epoch  31   1323.2958984375\n",
      "loss at Epoch  32   1323.287841796875\n",
      "loss at Epoch  33   1323.293701171875\n",
      "loss at Epoch  34   1323.307373046875\n",
      "loss at Epoch  35   1323.3193359375\n",
      "loss at Epoch  36   1323.354248046875\n",
      "loss at Epoch  37   1323.344482421875\n",
      "loss at Epoch  38   1323.34521484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at Epoch  39   1323.360595703125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim1,dim2=A.shape[0],100\n",
    "mi_params = dict(estimator='smile',critic='separable', baseline='unnormalized')\n",
    "\n",
    "data_params = {\n",
    "    'dim': dim1,\n",
    "    'batch_size': 64,\n",
    "    'cubic': None\n",
    "}\n",
    "\n",
    "critic_params = {\n",
    "     \n",
    "    'dim1': dim1,\n",
    "    'dim2': dim2,\n",
    "    'layers': 2,\n",
    "    'embed_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "critic = SeparableCritic(**critic_params)#.cuda()\n",
    "\n",
    "#A = torch.randn(count_mat.shape[0],count_mat.shape[1])\n",
    "critic_params = {\n",
    "    'layers': 2,\n",
    "    'embed_dim': 32,\n",
    "    'hidden_dim': 256,\n",
    "    'activation': 'relu',\n",
    "}\n",
    "\n",
    "\n",
    "W, H = nmf1.server_train(labels, iters =20, C=0.2, epoch=40, batch_size=128, lr=2.e-4, xi=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5572])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method sample in module random:\n",
      "\n",
      "sample(population, k) method of random.Random instance\n",
      "    Chooses k unique random elements from a population sequence or set.\n",
      "    \n",
      "    Returns a new list containing elements from the population while\n",
      "    leaving the original population unchanged.  The resulting list is\n",
      "    in selection order so that all sub-slices will also be valid random\n",
      "    samples.  This allows raffle winners (the sample) to be partitioned\n",
      "    into grand prize and second place winners (the subslices).\n",
      "    \n",
      "    Members of the population need not be hashable or unique.  If the\n",
      "    population contains repeats, then each occurrence is a possible\n",
      "    selection in the sample.\n",
      "    \n",
      "    To choose a sample in a range of integers, use range as an argument.\n",
      "    This is especially fast and space efficient for sampling from a\n",
      "    large population:   sample(range(10000000), 60)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "help(random.sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_glove' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-38d6258ff708>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcoherence_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcoherence_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_glove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoherence_vec\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m## the mean coherence score of all topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_glove' is not defined"
     ]
    }
   ],
   "source": [
    "dic0 = top_keywords(W, features, num=20)\n",
    "\n",
    "\n",
    "## compute the coherence score for each topic\n",
    "coherence_vec = []\n",
    "for i in range(W.shape[1]):  \n",
    "    coherence_vec.append(coherence(dic0[i], model_glove))\n",
    "\n",
    "np.mean(coherence_vec)   ## the mean coherence score of all topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.852     0.525     0.650       966\n",
      "           1      0.117     0.409     0.182       149\n",
      "\n",
      "    accuracy                          0.509      1115\n",
      "   macro avg      0.485     0.467     0.416      1115\n",
      "weighted avg      0.754     0.509     0.587      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    indices = list(range(len(labels)))   ## indices of documents\n",
    "    \n",
    "    ## split data into train and test\n",
    "    ind_train, ind_test, y_train, y_test = train_test_split(\n",
    "        indices, labels, test_size=0.2, random_state=2021, stratify=labels)\n",
    "    H_new = H.detach().numpy()\n",
    "    x_train, x_test = H_new[:, ind_train],H_new[:, ind_test]\n",
    "    \n",
    "    ## encode labels to integers\n",
    "    Encoder = LabelEncoder()\n",
    "    Y_train = Encoder.fit_transform(y_train)\n",
    "    Y_test = Encoder.fit_transform(y_test)\n",
    "\n",
    "\n",
    "    # Classifier - Algorithm - SVM -- linear kernel\n",
    "    # fit the training dataset on the classifier\n",
    "    SVM = svm.SVC(C=1., kernel='linear', degree=3, gamma='auto', random_state=82, class_weight='balanced')\n",
    "    SVM.fit(x_train.T, Y_train)# predict the labels on validation dataset\n",
    "    predictions_SVM = SVM.predict(x_test.T) # make predictions\n",
    "    print(classification_report(Y_test, predictions_SVM, digits=3))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, torch.Size([1, 500]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.random import dirichlet\n",
    "k=10\n",
    "client_dis=dirichlet([1,1],k)\n",
    "spam=[]\n",
    "ham=[]\n",
    "num=500\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]=='ham':\n",
    "        ham.append(i)\n",
    "    elif labels[i]=='spam':\n",
    "        spam.append(i)\n",
    "client_sample=[]\n",
    "for i in client_dis: \n",
    "    spam_sample=np.random.choice(spam, size=int(i[0]*num),replace=False)\n",
    "    ham_sample=np.random.choice(ham, size=num-int(i[0]*num))\n",
    "    client_sample.append(np.concatenate((spam_sample,ham_sample)))\n",
    "n=torch.from_numpy(np.array(client_sample))\n",
    "B=torch.chunk(n,k)\n",
    "(len(B),B[0].shape)\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function choice:\n",
      "\n",
      "choice(...) method of numpy.random.mtrand.RandomState instance\n",
      "    choice(a, size=None, replace=True, p=None)\n",
      "    \n",
      "    Generates a random sample from a given 1-D array\n",
      "    \n",
      "    .. versionadded:: 1.7.0\n",
      "    \n",
      "    .. note::\n",
      "        New code should use the ``choice`` method of a ``default_rng()``\n",
      "        instance instead; please see the :ref:`random-quick-start`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : 1-D array-like or int\n",
      "        If an ndarray, a random sample is generated from its elements.\n",
      "        If an int, the random sample is generated as if a were np.arange(a)\n",
      "    size : int or tuple of ints, optional\n",
      "        Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
      "        ``m * n * k`` samples are drawn.  Default is None, in which case a\n",
      "        single value is returned.\n",
      "    replace : boolean, optional\n",
      "        Whether the sample is with or without replacement\n",
      "    p : 1-D array-like, optional\n",
      "        The probabilities associated with each entry in a.\n",
      "        If not given the sample assumes a uniform distribution over all\n",
      "        entries in a.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    samples : single item or ndarray\n",
      "        The generated random samples\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If a is an int and less than zero, if a or p are not 1-dimensional,\n",
      "        if a is an array-like of size 0, if p is not a vector of\n",
      "        probabilities, if a and p have different lengths, or if\n",
      "        replace=False and the sample size is greater than the population\n",
      "        size\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    randint, shuffle, permutation\n",
      "    Generator.choice: which should be used in new code\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Sampling random rows from a 2-D array is not possible with this function,\n",
      "    but is possible with `Generator.choice` through its ``axis`` keyword.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Generate a uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3)\n",
      "    array([0, 3, 4]) # random\n",
      "    >>> #This is equivalent to np.random.randint(0,5,3)\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size 3:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([3, 3, 0]) # random\n",
      "    \n",
      "    Generate a uniform random sample from np.arange(5) of size 3 without\n",
      "    replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False)\n",
      "    array([3,1,0]) # random\n",
      "    >>> #This is equivalent to np.random.permutation(np.arange(5))[:3]\n",
      "    \n",
      "    Generate a non-uniform random sample from np.arange(5) of size\n",
      "    3 without replacement:\n",
      "    \n",
      "    >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])\n",
      "    array([2, 3, 0]) # random\n",
      "    \n",
      "    Any of the above can be repeated with an arbitrary array-like\n",
      "    instead of just integers. For instance:\n",
      "    \n",
      "    >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']\n",
      "    >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])\n",
      "    array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], # random\n",
      "          dtype='<U11')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.random.choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array([[ 0, 1, 2, 0, 2, 4],\n",
    "        [ 3, 4, 5, 6, 8, 10],\n",
    "        [ 3, 4, 5, 6, 8, 10], \n",
    "        [ 6, 7, 8, 12, 14, 16]])\n",
    "n=torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0,  1,  2,  0,  2,  4],\n",
       "         [ 3,  4,  5,  6,  8, 10]]),\n",
       " tensor([[ 3,  4,  5,  6,  8, 10],\n",
       "         [ 6,  7,  8, 12, 14, 16]]))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=torch.from_numpy(a)\n",
    "torch.chunk(n,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dirichlet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-baf86d919e9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient_dis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dirichlet' is not defined"
     ]
    }
   ],
   "source": [
    "client_dis=dirichlet([1,1], 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
